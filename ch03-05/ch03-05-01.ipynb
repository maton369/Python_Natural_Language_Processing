{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b159da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3-5-1 Wikipediaの日本百名湯記事で類似文書検索（説明コメント付き）\n",
    "# --------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・Wikipedia の日本百名湯（記事あり）の各ページ本文を収集し、後続のベクトル化（TF-IDF など）や\n",
    "#     類似文書検索（コサイン類似度など）の入力とするためのデータ構造（data_list）を構築する。\n",
    "#\n",
    "# 収集方針（理論/設計）:\n",
    "#   ・タイトルは Wikipedia のページ見出しに「完全一致」させる（曖昧さ回避を避けるため）。\n",
    "#   ・wikipedia パッケージの `page(title, auto_suggest=False)` を用いることで、\n",
    "#     自動補完による誤ジャンプを抑止し、表記ゆれに強すぎる検索を避ける。\n",
    "#   ・得られる `.content` は本文テキスト（マークアップはある程度除去済み）で、節見出し等は含まれる。\n",
    "#   ・結果は辞書の配列 data_list に格納する（id/title/text の 3 キー）。`app_id` は 1 始まりの連番。\n",
    "#\n",
    "# 実務上の注意:\n",
    "#   ・大量アクセスは先方に負荷となるため、必要に応じて `time.sleep` を入れる（礼儀的なレート制御）。\n",
    "#   ・曖昧さ回避/ページ欠落などの例外（DisambiguationError, PageError）を適切にハンドリングする。\n",
    "#   ・後続処理（形態素解析・ベクトル化）での再現性確保のため、成功/失敗リストをロギングするのが望ましい。\n",
    "#   ・本文の前処理（見出し/注記の除去、正規化、句読点処理など）は別段階で行うと責務分離が明確。\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# 日本百名湯の記事タイトル一覧（Wikipedia のページ見出しに合わせた表記）\n",
    "title_list = [\n",
    "    \"菅野温泉\",\n",
    "    \"養老牛温泉\",\n",
    "    \"定山渓温泉\",\n",
    "    \"登別温泉\",\n",
    "    \"洞爺湖温泉\",\n",
    "    \"ニセコ温泉郷\",\n",
    "    \"朝日温泉 (北海道)\",\n",
    "    \"酸ヶ湯温泉\",\n",
    "    \"蔦温泉\",\n",
    "    \"花巻南温泉峡\",\n",
    "    \"夏油温泉\",\n",
    "    \"須川高原温泉\",\n",
    "    \"鳴子温泉郷\",\n",
    "    \"遠刈田温泉\",\n",
    "    \"峩々温泉\",\n",
    "    \"乳頭温泉郷\",\n",
    "    \"後生掛温泉\",\n",
    "    \"玉川温泉 (秋田県)\",\n",
    "    \"秋ノ宮温泉郷\",\n",
    "    \"銀山温泉\",\n",
    "    \"瀬見温泉\",\n",
    "    \"赤倉温泉 (山形県)\",\n",
    "    \"東山温泉\",\n",
    "    \"飯坂温泉\",\n",
    "    \"二岐温泉\",\n",
    "    \"那須温泉郷\",\n",
    "    \"塩原温泉郷\",\n",
    "    \"鬼怒川温泉\",\n",
    "    \"奥鬼怒温泉郷\",\n",
    "    \"草津温泉\",\n",
    "    \"伊香保温泉\",\n",
    "    \"四万温泉\",\n",
    "    \"法師温泉\",\n",
    "    \"箱根温泉\",\n",
    "    \"湯河原温泉\",\n",
    "    \"越後湯沢温泉\",\n",
    "    \"松之山温泉\",\n",
    "    \"大牧温泉\",\n",
    "    \"山中温泉\",\n",
    "    \"山代温泉\",\n",
    "    \"粟津温泉\",\n",
    "    \"奈良田温泉\",\n",
    "    \"西山温泉 (山梨県)\",\n",
    "    \"野沢温泉\",\n",
    "    \"湯田中温泉\",\n",
    "    \"別所温泉\",\n",
    "    \"中房温泉\",\n",
    "    \"白骨温泉\",\n",
    "    \"小谷温泉\",\n",
    "    \"下呂温泉\",\n",
    "    \"福地温泉\",\n",
    "    \"熱海温泉\",\n",
    "    \"伊東温泉\",\n",
    "    \"修善寺温泉\",\n",
    "    \"湯谷温泉 (愛知県)\",\n",
    "    \"榊原温泉\",\n",
    "    \"木津温泉\",\n",
    "    \"有馬温泉\",\n",
    "    \"城崎温泉\",\n",
    "    \"湯村温泉 (兵庫県)\",\n",
    "    \"十津川温泉\",\n",
    "    \"南紀白浜温泉\",\n",
    "    \"南紀勝浦温泉\",\n",
    "    \"湯の峰温泉\",\n",
    "    \"龍神温泉\",\n",
    "    \"奥津温泉\",\n",
    "    \"湯原温泉\",\n",
    "    \"三朝温泉\",\n",
    "    \"岩井温泉\",\n",
    "    \"関金温泉\",\n",
    "    \"玉造温泉\",\n",
    "    \"有福温泉\",\n",
    "    \"温泉津温泉\",\n",
    "    \"湯田温泉\",\n",
    "    \"長門湯本温泉\",\n",
    "    \"祖谷温泉\",\n",
    "    \"道後温泉\",\n",
    "    \"二日市温泉 (筑紫野市)\",\n",
    "    \"嬉野温泉\",\n",
    "    \"武雄温泉\",\n",
    "    \"雲仙温泉\",\n",
    "    \"小浜温泉\",\n",
    "    \"黒川温泉\",\n",
    "    \"地獄温泉\",\n",
    "    \"垂玉温泉\",\n",
    "    \"杖立温泉\",\n",
    "    \"日奈久温泉\",\n",
    "    \"鉄輪温泉\",\n",
    "    \"明礬温泉\",\n",
    "    \"由布院温泉\",\n",
    "    \"川底温泉\",\n",
    "    \"長湯温泉\",\n",
    "    \"京町温泉\",\n",
    "    \"指宿温泉\",\n",
    "    \"霧島温泉郷\",\n",
    "    \"新川渓谷温泉郷\",\n",
    "    \"栗野岳温泉\",\n",
    "]\n",
    "\n",
    "# Wikipedia から本文を読み取る\n",
    "# - `wikipedia` ライブラリは MediaWiki API を叩く簡易ラッパ。オンライン接続が必要。\n",
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"ja\")  # 日本語版 Wikipedia に固定（他言語へ飛ぶのを防ぐ）\n",
    "\n",
    "data_list = []  # 後続の類似検索・ベクトル化に渡すための原データを蓄積するリスト\n",
    "for index, title in enumerate(title_list):\n",
    "    # 進捗確認のため、1 始まりの連番とタイトルを表示（学習/デバッグ用途）\n",
    "    print(index + 1, title)\n",
    "\n",
    "    # auto_suggest=False:\n",
    "    #  - 与えたタイトルでの厳密取得を志向。誤補完により別ページへ飛ぶことを防ぐ。\n",
    "    #  - 表記が完全一致していないと例外（PageError 等）になる可能性がある点に留意。\n",
    "    text = wikipedia.page(title, auto_suggest=False).content\n",
    "\n",
    "    # 構造化: app_id（連番）, title（記事名）, text（本文）という素朴なレコード形式\n",
    "    item = {\n",
    "        \"app_id\": index\n",
    "        + 1,  # 1, 2, 3, ...（検索結果表示や ES の _id 等に流用しやすい）\n",
    "        \"title\": title,  # 記事タイトル（後続での見出し・ログ用）\n",
    "        \"text\": text,  # 記事本文（前処理→ベクトル化の入力）\n",
    "    }\n",
    "    data_list.append(item)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# （拡張: 例外処理 + レート制御の雛形。必要に応じて使用）\n",
    "# import time\n",
    "# from wikipedia.exceptions import DisambiguationError, PageError\n",
    "# failed = []\n",
    "# data_list = []\n",
    "# for index, title in enumerate(title_list):\n",
    "#     print(index + 1, title)\n",
    "#     try:\n",
    "#         page = wikipedia.page(title, auto_suggest=False)\n",
    "#         data_list.append({'app_id': index + 1, 'title': title, 'text': page.content})\n",
    "#         time.sleep(0.5)  # マナー的ウェイト。必要に応じて調整\n",
    "#     except DisambiguationError as e:\n",
    "#         failed.append({'title': title, 'reason': 'disambiguation', 'options': e.options[:5]})\n",
    "#     except PageError:\n",
    "#         failed.append({'title': title, 'reason': 'page_not_found'})\n",
    "# # 収集サマリを確認（学習用）\n",
    "# # print(f\"成功: {len(data_list)} / 失敗: {len(failed)}\"); print(failed)\n",
    "# --------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915bb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticsearchインスタンスの生成\n",
    "# 3.3節参照\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca2b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデックス作成用JSONの定義（詳細コメント付き）\n",
    "# 3.3節参照\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・Elasticsearch に日本語向けの analysis 設定（正規化・形態素解析・同義語等）を与え、\n",
    "#     検索時/索引用で異なるアナライザを使い分ける土台を作る。\n",
    "#\n",
    "# 理論ポイント:\n",
    "#   ・アナライザは「char_filter → tokenizer → filter」の順で適用される。\n",
    "#   ・索引時（document投入）: analyzer=\"jpn-index\" を適用し、語彙を“素直に”保存。\n",
    "#   ・検索時（クエリ解析） : search_analyzer=\"jpn-search\" を適用し、同義語などで想起を拡張。\n",
    "#     → 索引側に同義語を混ぜないことで観測性と保守性が高まるのが定石。\n",
    "#\n",
    "# 実務メモ:\n",
    "#   ・icu_normalizer を使うには analysis-icu プラグインが必要。\n",
    "#   ・kuromoji_* を使うには analysis-kuromoji プラグインが必要（バージョンにより同梱/外部）。\n",
    "#   ・\"user_dictionary\" のパスは **Elasticsearchノード上** の配置先（ローカル開発機ではない）。\n",
    "#   ・同義語タイプ \"synonym\" は等価展開。片方向正規化や位置情報重視には \"synonym_graph\" も検討。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "create_index = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            # ---------------------------\n",
    "            # Token Filter 群（語彙変換）\n",
    "            # ---------------------------\n",
    "            \"filter\": {\n",
    "                \"synonyms_filter\": {  # 同義語フィルタの定義（検索側で使用予定）\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [  # 等価展開ルール群（ここでは空/プレースホルダ）\n",
    "                        # 例: \"すし,スシ,鮨,寿司\"\n",
    "                    ],\n",
    "                }\n",
    "            },\n",
    "            # ---------------------------\n",
    "            # Tokenizer（形態素分割）\n",
    "            # ---------------------------\n",
    "            \"tokenizer\": {\n",
    "                \"kuromoji_w_dic\": {  # ユーザー辞書付きの kuromoji トークナイザ\n",
    "                    \"type\": \"kuromoji_tokenizer\",  # ※ 綴りは \"kuromoji\"\n",
    "                    \"user_dictionary\": \"my_jisho.dic\",  # ESノード側パス（*.dic=事前ビルド済み）\n",
    "                }\n",
    "            },\n",
    "            # ---------------------------\n",
    "            # Analyzer（前処理パイプライン）\n",
    "            # ---------------------------\n",
    "            \"analyzer\": {\n",
    "                # 検索用: 想起重視（同義語をここで展開）\n",
    "                \"jpn-search\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\n",
    "                        \"icu_normalizer\",  # 文字正規化（NFKC等）: 全半角/互換文字の吸収\n",
    "                        \"kuromoji_iteration_mark\",  # 繰り返し記号（々/ゝ等）の展開\n",
    "                    ],\n",
    "                    \"tokenizer\": \"kuromoji_w_dic\",  # 形態素解析（ユーザー辞書適用）\n",
    "                    \"filter\": [\n",
    "                        \"synonyms_filter\",  # ← 同義語展開（検索側のみ）\n",
    "                        \"kuromoji_baseform\",  # 活用の原形化（形容詞/動詞など）\n",
    "                        \"kuromoji_part_of_speech\",  # 不要品詞の除去（助詞・助動詞・記号 等）\n",
    "                        \"ja_stop\",  # 日本語ストップワード除去\n",
    "                        \"kuromoji_number\",  # 数字正規化（漢数字→算用 等）\n",
    "                        \"kuromoji_stemmer\",  # 長音正規化（コンピューター→コンピュータ）\n",
    "                    ],\n",
    "                },\n",
    "                # 索引用: 観測性重視（同義語は含めない）\n",
    "                \"jpn-index\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\"icu_normalizer\", \"kuromoji_iteration_mark\"],\n",
    "                    \"tokenizer\": \"kuromoji_w_dic\",\n",
    "                    \"filter\": [\n",
    "                        \"kuromoji_baseform\",\n",
    "                        \"kuromoji_part_of_speech\",\n",
    "                        \"ja_stop\",\n",
    "                        \"kuromoji_number\",\n",
    "                        \"kuromoji_stemmer\",\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 日本語用インデックス名の定義\n",
    "jp_index = \"jp_index\"\n",
    "\n",
    "# 既存インデックスがあれば削除（学習/検証用途）\n",
    "# 本番運用では: 新インデックス作成 → _reindex 移行 → エイリアス切替 が安全\n",
    "if es.indices.exists(index=jp_index):\n",
    "    es.indices.delete(index=jp_index)\n",
    "\n",
    "# インデックスの作成（settings のみ）\n",
    "# 注意: この段階ではフィールドへの analyzer 割当は未定義。\n",
    "#       → 後続で put_mapping し、\"content\" などの text フィールドに\n",
    "#          analyzer=\"jpn-index\" / search_analyzer=\"jpn-search\" を明示すること。\n",
    "es.indices.create(index=jp_index, body=create_index)\n",
    "\n",
    "# ----（参考: 動作確認の雛形。必要時のみコメント解除）-------------------------\n",
    "# print(es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-search\",\"text\":\"寿司\"}))\n",
    "# mapping = {\"properties\":{\"content\":{\"type\":\"text\",\"analyzer\":\"jpn-index\",\"search_analyzer\":\"jpn-search\"}}}\n",
    "# es.indices.put_mapping(index=jp_index, body=mapping)\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c1733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mappingの設定（詳細コメント付き）\n",
    "# 3.3節参照\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・インデックス `jp_index` に対して、全文検索対象フィールドのマッピングを設定する。\n",
    "#   ・索引時は「jpn-index」、検索時は「jpn-search」を適用して役割分離する\n",
    "#     （= 索引は素直に、検索で想起拡張：同義語など）。\n",
    "#\n",
    "# 理論メモ:\n",
    "#   ・Elasticsearch では analyzer は\n",
    "#       char_filter → tokenizer → filter\n",
    "#     の順で適用される。\n",
    "#   ・`analyzer` は索引時（ドキュメント投入時）に使用、\n",
    "#     `search_analyzer` は検索クエリ解析時に使用される。\n",
    "#   ・既存フィールドの analyzer は **後から変更できない**（型制約）。\n",
    "#     変更が必要な場合は「新インデックス作成 → _reindex → エイリアス切替」が必要。\n",
    "#\n",
    "# 実務メモ:\n",
    "#   ・title の完全一致・集計・ソート用に keyword のサブフィールド（raw）を併設するのが定石。\n",
    "#   ・`jpn-search` に同義語を入れている場合、索引側に同義語を焼き込まないことで\n",
    "#     可観測性と運用保守性が高くなる（デバッグ時に“生の語彙”が見える）。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            # 索引時に適用: 形態素解析・正規化は行うが、同義語はここでは展開しない\n",
    "            \"analyzer\": \"jpn-index\",\n",
    "            # 検索時に適用: 同義語や原形化などで表記ゆれを吸収\n",
    "            \"search_analyzer\": \"jpn-search\",\n",
    "        },\n",
    "        \"title\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"jpn-index\",\n",
    "            \"search_analyzer\": \"jpn-search\",\n",
    "            # 完全一致/集計/ソート向けの keyword サブフィールド\n",
    "            \"fields\": {\n",
    "                \"raw\": {\n",
    "                    \"type\": \"keyword\",\n",
    "                    \"ignore_above\": 256,\n",
    "                    # 正規化を厳密にしたい場合は settings 側に normalizer を定義して指定する\n",
    "                    # 例: \"normalizer\": \"lowercase_normalizer\"\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# マッピング適用\n",
    "es.indices.put_mapping(index=jp_index, body=mapping)\n",
    "\n",
    "# --- 任意: 適用結果の確認（観測性向上のための簡易ダンプ） --------------------\n",
    "# cur = es.indices.get_mapping(index=jp_index)\n",
    "# from pprint import pprint\n",
    "# pprint(cur[jp_index][\"mappings\"][\"properties\"])\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 参考（比較用・非推奨パターン）:\n",
    "#   analyzer=\"jpn-search\" を索引時にも使うと、同義語が索引側に焼き込まれ、\n",
    "#   後から同義語辞書を差し替えた際の検証が難しくなるため運用上は避けるのが無難。\n",
    "#   ただし “検索時と索引時を完全同一にしたい” という方針なら一貫性は高い（可観測性は低下）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a33341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文書の登録（詳細コメント付き）\n",
    "# 3.3節参照\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・前段で収集した Wikipedia 記事データ（data_list）を、インデックス jp_index に投入する。\n",
    "#   ・各レコードの 'app_id' を Elasticsearch の _id として用い、後続の類似検索時に\n",
    "#     クエリ元と結果の対応を取りやすくする（例: self-match 除外や ID ベースの参照に便利）。\n",
    "#\n",
    "# 前提:\n",
    "#   ・`jp_index` は settings（日本語アナライザ）と mappings（text/title フィールド）を設定済み。\n",
    "#   ・`data_list` は以下の辞書要素を持つ配列:\n",
    "#       {'app_id': int, 'title': str, 'text': str}\n",
    "#   ・アナライザ分離方針（索引: jpn-index / 検索: jpn-search）により、\n",
    "#     索引側は“素直な語彙”を保存、検索側で同義語等を展開する。\n",
    "#\n",
    "# 実務メモ:\n",
    "#   ・このループは 1 件ずつ HTTP リクエストを発行するため、大量投入時は遅くなる。\n",
    "#     → 大規模データでは helpers.bulk の利用を推奨（下に雛形）。\n",
    "#   ・直後に検索して可視化したい場合は refresh=\"wait_for\" を付けるか、事後に refresh API を呼ぶ。\n",
    "#   ・同一 _id の再投入は上書き（_version が上がる）。衝突で失敗させたい場合は op_type=\"create\" を指定。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "for body in data_list:\n",
    "    # id と app_id の値を同じにして、類似検索をやりやすくする\n",
    "    # - 後続で「ある文書に最も近い文書」を探す際、_id をキーに自己一致を除外/識別しやすい。\n",
    "    es.index(index=jp_index, id=body[\"app_id\"], body=body)\n",
    "    # 参考（8.x 推奨の引数名。上書き禁止・即時可視化の例）:\n",
    "    # es.index(index=jp_index, id=body['app_id'], document=body, op_type=\"create\", refresh=\"wait_for\")\n",
    "\n",
    "# 直後に検索評価を行う場合の明示リフレッシュ（refresh=\"wait_for\" を使わない場合の代替）\n",
    "# es.indices.refresh(index=jp_index)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 参考: bulk で高速投入する雛形（大量データ向け）\n",
    "# -----------------------------------------------------------------------------\n",
    "# from elasticsearch import helpers\n",
    "# actions = (\n",
    "#     {\n",
    "#         \"_op_type\": \"index\",      # \"create\" にすると既存IDはエラー\n",
    "#         \"_index\": jp_index,\n",
    "#         \"_id\": doc[\"app_id\"],\n",
    "#         \"_source\": doc\n",
    "#     }\n",
    "#     for doc in data_list\n",
    "# )\n",
    "# helpers.bulk(es, actions, refresh=\"wait_for\")  # 成功/失敗件数を返す\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6698e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.5.2 類似検索の実行（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・Elasticsearch の More Like This（MLT）クエリで、指定の文書（ここでは _id=3: 定山渓温泉）\n",
    "#     と“内容的に似ている”文書を検索する。\n",
    "#\n",
    "# 理論メモ（MLT の挙動）:\n",
    "#   ・MLT は「参照文書（like）」の本文から“代表語（terms）”を抽出し、クエリを自動生成する。\n",
    "#     抽出語はフィールドのアナライザで解析される（ここでは text フィールドの search_analyzer が既定）。\n",
    "#   ・スコアリングは基本的に BM25。共通語（df が高い語）は IDF が小さくなり寄与が下がる。\n",
    "#   ・`min_term_freq`, `min_doc_freq`, `max_query_terms`, `minimum_should_match` などで\n",
    "#     「どの程度の珍しさ/一致度」を要求するかを制御できる。\n",
    "#   ・デフォルトでは `include=false` で参照文書そのものは結果から除外される（自己一致の抑制）。\n",
    "#\n",
    "# 実務メモ:\n",
    "#   ・Elasticsearch 7 以降は type 廃止（_type を指定しないのが基本）。8 系では完全削除。\n",
    "#     → 互換性重視なら like に _type は含めない（下に修正例を併記）。\n",
    "#   ・直前に index したデータを即座に検索する場合は refresh を考慮（refresh=\"wait_for\" など）。\n",
    "#   ・日本語の場合、検索側アナライザ（ここでは jpn-search）で同義語/正規化を効かせる設計が有効。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 検索条件の設定\n",
    "# - 元コード（_type を含む; 旧バージョン互換）\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"more_like_this\": {\n",
    "            \"fields\": [\"text\"],  # 類似度の計算に使うフィールド\n",
    "            \"like\": [\n",
    "                {\n",
    "                    \"_index\": \"jp_index\",\n",
    "                    \"_type\": \"_doc\",  # ← 7+ では非推奨/8 では無効。可能なら削除を推奨\n",
    "                    \"_id\": \"3\",  # 参照文書（app_id と _id を一致させてある前提）\n",
    "                }\n",
    "            ],\n",
    "            # 代表的なチューニング項目（必要に応じて追加）\n",
    "            # ,\"min_term_freq\": 1          # 参照文書内での最低出現回数（文書内ストップ語化）\n",
    "            # ,\"min_doc_freq\": 1           # コーパス内での最低文書頻度（汎用語の除外）\n",
    "            # ,\"max_query_terms\": 50       # クエリ化する語の最大数（既定: 25）\n",
    "            # ,\"minimum_should_match\": \"30%\"  # 一致率をパーセンテージ指定で要求\n",
    "            # ,\"analyzer\": \"jpn-search\"    # 明示したい場合のみ（既定はフィールドの search_analyzer）\n",
    "            # ,\"stop_words\": [\"温泉\"]      # ドメイン特有の汎用語を明示的に無視\n",
    "            # ,\"include\": False            # 参照文書を結果に含めるか（既定は False）\n",
    "        }\n",
    "    },\n",
    "    \"track_total_hits\": True,  # 総件数を正確計上（評価時に便利）\n",
    "    # ,\"size\": 10             # 返す件数を制御（既定: 10）\n",
    "}\n",
    "\n",
    "# --- 推奨: ES 7/8 互換の like（_type を削除） -------------------------------\n",
    "# query = {\n",
    "#     \"query\": {\n",
    "#         \"more_like_this\": {\n",
    "#             \"fields\": [\"text\"],\n",
    "#             \"like\": [{ \"_index\": jp_index, \"_id\": \"3\" }],\n",
    "#             \"min_term_freq\": 1,\n",
    "#             \"min_doc_freq\": 1,\n",
    "#             \"max_query_terms\": 50,\n",
    "#             \"minimum_should_match\": \"30%\"\n",
    "#         }\n",
    "#     },\n",
    "#     \"track_total_hits\": True,\n",
    "#     \"size\": 10\n",
    "# }\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 検索実行\n",
    "res = es.search(index=jp_index, body=query)\n",
    "\n",
    "# 結果表示\n",
    "# - _score は BM25 による関連度。高いほど参照文書に“似ている”。\n",
    "# - 参照文書そのものは（通常は）除外されるが、設定や ES のバージョンによっては残ることがある。\n",
    "w1 = res[\"hits\"][\"hits\"]\n",
    "\n",
    "for item in w1:\n",
    "    score = item[\"_score\"]\n",
    "    source = item[\"_source\"]\n",
    "    app_id = source[\"app_id\"]\n",
    "    title = source[\"title\"]\n",
    "    print(app_id, title, score)\n",
    "\n",
    "# 参考: デバッグ/可観測性向上の補助（必要に応じて活用）\n",
    "# - 代表語の確認（MLT がどの語をクエリ化したかは API では直接取れないため、\n",
    "#   近似として参照文書 text を analyze API で可視化し、tf/df の高低を観察する）\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\": \"jpn-search\", \"text\": w1[0][\"_source\"][\"text\"][:2000]})\n",
    "# for t in toks[\"tokens\"][:50]:\n",
    "#     print(t[\"token\"], t[\"position\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
