{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1505a346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "一\n",
      "　うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている。このじいさんはたしかに前の前の駅から乗ったいなか者である。発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌をぬい\n",
      "\n",
      "\n",
      "評に取りかかる。与次郎だけが三四郎のそばへ来た。\n",
      "「どうだ森の女は」\n",
      "「森の女という題が悪い」\n",
      "「じゃ、なんとすればよいんだ」\n",
      "　三四郎はなんとも答えなかった。ただ口の中で迷羊、迷羊と繰り返した。\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# リスト5.2.1\n",
    "# Word2Vec 学習用テキストの取得と整形（青空文庫：夏目漱石『三四郎』）\n",
    "# 目的:\n",
    "#   - 後段の形態素解析・Word2Vec 学習に使えるよう、一次テキストをダウンロード→解凍→本文抽出→前処理する。\n",
    "# ポイント:\n",
    "#   - 青空文庫の本文はヘッダ/フッタやルビ・注記を含むため、正規表現で除去してクリーンなコーパスを得る。\n",
    "#   - 文字コードは Shift_JIS（sjis）で公開されることが多いので、明示的に指定して開く。\n",
    "#   - 最終的に先頭/末尾を一部表示して、整形結果を目視確認する。\n",
    "\n",
    "# =========================\n",
    "# 1) zipファイルのダウンロード\n",
    "# =========================\n",
    "# 対象は夏目漱石『三四郎』のルビ付きテキスト（_ruby_ を含むファイル）\n",
    "url = \"https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip\"\n",
    "zip = \"794_ruby_4237.zip\"  # ※組込み関数名 'zip' と同名だが、ここでは変数として使用（実務では別名推奨）\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# 指定URLからローカルへダウンロード（戻り値は保存先パス, ヘッダ情報のタプル）\n",
    "urllib.request.urlretrieve(url, zip)\n",
    "\n",
    "# =========================\n",
    "# 2) ダウンロードしたzipを解凍し、本文テキストを読み込み\n",
    "# =========================\n",
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(zip, \"r\") as myzip:\n",
    "    # アーカイブ内の全ファイルをカレントディレクトリへ展開\n",
    "    myzip.extractall()\n",
    "    # 解凍後に含まれるファイル一覧を走査し、テキスト本文を読み込む\n",
    "    # ルビ付きテキスト（.txt）が1つだけ入っている想定\n",
    "    for myfile in myzip.infolist():\n",
    "        filename = myfile.filename  # 例: 'sanshirou.txt' のようなファイル名\n",
    "        # 青空文庫のテキストは多くが Shift_JIS（ここでは 'sjis' 指定）で配布されている\n",
    "        # OS/環境差異で 'cp932' が必要になることもあるが、まずは 'sjis' で試す\n",
    "        with open(filename, encoding=\"sjis\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "# =========================\n",
    "# 3) 青空文庫特有の前処理（本文抽出・ノイズ除去）\n",
    "# =========================\n",
    "import re\n",
    "\n",
    "# 青空文庫のメタ情報は '-----' による区切りで囲まれることが多い。\n",
    "# re.split('\\-{5,}', text) で \"-----\" 以上の連続ハイフンを区切りに分割し、\n",
    "# 先頭から [0]=ヘッダ前半, [1]=ヘッダ, [2]=本文… のことが多いため [2] を採用。\n",
    "# （作品により構造が微妙に異なる可能性がある点に注意）\n",
    "text = re.split(\"\\-{5,}\", text)[2]\n",
    "\n",
    "# フッタ（底本情報以降）を除去。\n",
    "# '底本：' 以降は出典や注記なので学習には不要。\n",
    "text = re.split(\"底本：\", text)[0]\n",
    "\n",
    "# 青空文庫の縦書き由来の区切り記号 '|' を削除（行内のルビ位置などのために使われる）\n",
    "text = text.replace(\"|\", \"\")\n",
    "\n",
    "# ルビ（《…》で囲まれた読み仮名）を削除。学習用には表記ゆれの原因になるため除去。\n",
    "text = re.sub(\"《.+?》\", \"\", text)\n",
    "\n",
    "# 入力注（［＃…］で囲まれた編集注記）を削除。こちらも本文外情報のため学習から除外。\n",
    "text = re.sub(\"［＃.+?］\", \"\", text)\n",
    "\n",
    "# 連続する空行を1つに正規化（文区切りの過剰なノイズを抑える）\n",
    "text = re.sub(\"\\n\\n\", \"\\n\", text)\n",
    "\n",
    "# Windows系改行に混在しがちな '\\r' を除去し、行末記号を '\\n' に統一\n",
    "text = re.sub(\"\\r\", \"\", text)\n",
    "\n",
    "# =========================\n",
    "# 4) 整形結果の確認（サンプル表示）\n",
    "# =========================\n",
    "# 冒頭100文字を表示して、ヘッダが除去され本文が先頭から始まっているかを目視確認\n",
    "print(text[:100])\n",
    "\n",
    "# 見やすさのための空行\n",
    "print()\n",
    "print()\n",
    "\n",
    "# 末尾100文字を表示して、フッタが除去されているかを目視確認\n",
    "print(text[-100:])\n",
    "\n",
    "# 以上で、Word2Vec など分散表現学習のコーパスとして利用可能なプレーンテキストが得られる。\n",
    "# 後続ステップ例：\n",
    "#   - 形態素解析で分かち書き（名詞・動詞原形など抽出）\n",
    "#   - トークン列を Gensim の Word2Vec に投入し学習\n",
    "#   - 学習済みモデルで類似語検索/アナロジー推論を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dff27f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一', 'する', '目', 'さめる', '女', '隣', 'じいさん', '話', '始める', 'いる']\n",
      "['じいさん', '前', '前', '駅', '乗る', 'いなか者']\n"
     ]
    }
   ],
   "source": [
    "# リスト5.2.2\n",
    "# Word2Vec 学習用データの前処理\n",
    "# 目的：\n",
    "#   - 後段の Word2Vec 学習で入力する「文ごとのトークン列（list[list[str]]）」を作る。\n",
    "#   - Janome で形態素解析し、名詞・動詞・形容詞の「原形（base_form）」のみを抽出する。\n",
    "# 背景：\n",
    "#   - Word2Vec は連続語彙モデル（CBOW/Skip-gram）により、隣接語の共起を手掛かりにベクトルを学習する。\n",
    "#   - 品詞を絞ることでノイズ（助詞・記号など）を減らし、学習の安定性・計算効率を高める。\n",
    "#   - 動詞・形容詞は活用形が多く、そのままだと語彙がスパース化するため「原形化」して語彙を統合するのが定石。\n",
    "# 注意：\n",
    "#   - Janome の Token.base_form は未知語の場合 '*' を返すことがある（必要なら surface へのフォールバックを検討）。\n",
    "#   - 文分割を単純に '。' で行うと、引用・箇条書き・記号ゆれで分割精度が落ちる。厳密には文区切り器を使う。\n",
    "#   - 大規模コーパスでは解析コストが高いので、事前のキャッシュや並列化、バッチ処理を検討する。\n",
    "\n",
    "# Janomeのロード\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Tokenizer は内部で辞書をロードするため、毎回生成せずに再利用するのが高速\n",
    "t = Tokenizer()\n",
    "\n",
    "\n",
    "# テキストを引数として、形態素解析の結果から\n",
    "#   - 名詞・動詞・形容詞のみを対象\n",
    "#   - 原形（base_form）を取り出す\n",
    "# を行い、単語（文字列）リストを返す関数\n",
    "def extract_words(text):\n",
    "    # Tokenizer.tokenize は形態素ごとの Token オブジェクト（surface, base_form, part_of_speech 等を持つ）を返す\n",
    "    tokens = t.tokenize(text)\n",
    "    # part_of_speech は「品詞,品詞細分類1,品詞細分類2,品詞細分類3」のカンマ区切り文字列\n",
    "    # 先頭要素（大分類）が '名詞'・'動詞'・'形容詞' のものだけを残す\n",
    "    # base_form（原形）を返すことで、活用差による語彙の分断を回避して学習を安定させる\n",
    "    return [\n",
    "        token.base_form\n",
    "        for token in tokens\n",
    "        if token.part_of_speech.split(\",\")[0] in [\"名詞\", \"動詞\", \"形容詞\"]\n",
    "    ]\n",
    "    # 参考：\n",
    "    #   - 名詞は原形＝表層形であることが多いが、固有名詞・未知語の扱いに注意\n",
    "    #   - 動詞・形容詞は原形化の効果が大きい（例：「行く」「行った」「行き」→「行く」に正規化）\n",
    "\n",
    "\n",
    "#  関数テスト（任意）：\n",
    "# ret = extract_words('三四郎は京都でちょっと用があって降りたついでに。')\n",
    "# for word in ret:\n",
    "#     print(word)\n",
    "\n",
    "# 文単位で学習させるため、まず全文を \"。\" で分割して文配列にする\n",
    "# 重要：\n",
    "#   - 末尾が \"。\" で終わらない場合、最後の要素は空でない短文になる。\n",
    "#   - 「。」以外の終止記号（! ?、全角/半角の混在）や見出し行は別途正規化すると良い。\n",
    "sentences = text.split(\"。\")\n",
    "\n",
    "# 各文を extract_words でトークン化し、list[list[str]] 形式に変換する\n",
    "# 規模によっては数分～数十分かかるため、進捗表示（tqdm）や分割処理の導入が有効\n",
    "word_list = [extract_words(sentence) for sentence in sentences]\n",
    "\n",
    "# 結果の一部を確認（サンプル）：\n",
    "# - 先頭2文分のトークン列を出力し、想定どおり「名詞・動詞・形容詞の原形」のみになっているかを確認する\n",
    "# - インデックスエラー回避のため、最低限の要素数チェックを行ってもよい\n",
    "print(word_list[0])\n",
    "print(word_list[1])\n",
    "\n",
    "# これで、Gensim の Word2Vec にそのまま投入できる学習用データ（コーパス）が整った。\n",
    "# 次段例：\n",
    "#   from gensim.models import Word2Vec\n",
    "#   model = Word2Vec(sentences=word_list, vector_size=200, window=5, min_count=5, sg=1, workers=4)\n",
    "#   model.wv.most_similar('学生')  # 類似語の確認 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2b757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
