{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7069e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.1 初期処理と変数宣言（説明コメント付き）\n",
    "# ------------------------------------------------------------\n",
    "# 目的:\n",
    "# - Mac（特に Apple Silicon / MKL 由来の OpenMP ランタイム）環境で発生しやすい\n",
    "#   \"KMP_DUPLICATE_LIB_OK\" 警告クラッシュの回避設定\n",
    "# - ノートブック/学習ログの見通しを良くするための警告抑止（デモ用）\n",
    "# - Word2Vec 埋め込み次元数および LSTM に入力する最大系列長の基準値を宣言\n",
    "#\n",
    "# 注意:\n",
    "# - 以下の環境変数・警告抑止は「実験用の便宜措置」です。プロダクションでは\n",
    "#   依存ライブラリの統一や仮想環境管理（conda/venv/uv など）で根本対応するのが望ましい。\n",
    "# - EMBEDDING_DIM / MAX_LEN は下流のモデル定義・前処理（Tokenizer, padding 戦略）\n",
    "#   と一貫している必要があります（不一致だと shape エラーや性能劣化につながる）。\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Mac の OpenMP ランタイム多重ロード問題の暫定回避 ---------------------------------\n",
    "# Intel MKL / libomp 等が複数ロードされるとクラッシュするケースがあり、Mac で学習時に\n",
    "# \"KMP_DUPLICATE_LIB_OK\" を True にして回避することがある。\n",
    "# 本来は「ライブラリの競合解消（単一の OpenMP に統一）」が理想。恒久対策ではない点に注意。\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"  # デモ/学習用の一時対応\n",
    "\n",
    "# --- 警告抑止（デモの可読性向上のため） ---------------------------------------------------\n",
    "# 学習途中の FutureWarning / DeprecationWarning などで出力がノイズ化するのを防ぐ。\n",
    "# ただし、重要な警告まで隠すリスクがあるため、本番/検証では必要な種類だけを個別に filter すること。\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 埋め込み次元（Word2Vec の隠れ層ノード数 = ベクトル次元数） ---------------------------\n",
    "# 一般的な選択肢: 100〜300（コーパス規模が小〜中の場合）。大きくするほど表現力は上がるが、\n",
    "# 学習時間・メモリ（O(|V|×D)）が増加し過学習のリスクも高まる。\n",
    "# 既存の学習済みモデル（例: fastText 300 次元）と互換を取りたい場合は、その次元に合わせる。\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# --- LSTM に入力する最大系列長（トークン数） ----------------------------------------------\n",
    "# 前処理側（Tokenizer/分かち書き）で得られた系列を padding/truncation する長さ。\n",
    "# 値を大きくすると長距離依存を捉えやすくなるが、計算負荷とメモリ消費が増える。\n",
    "# 日本語文で 50 はやや短め。短文中心のデータ（ツイート等）には妥当だが、長文なら 128〜256 も検討。\n",
    "# 下流モデル（Embedding 層の input_length, LSTM の time_steps）や学習用 DataLoader と整合させること。\n",
    "MAX_LEN = 50\n",
    "\n",
    "# --- 参考: 再現性確保のための乱数シード（必要に応じて使用） ------------------------------\n",
    "# import random, numpy as np\n",
    "# import torch\n",
    "# SEED = 42\n",
    "# random.seed(SEED); np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "# torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "#\n",
    "# --- 参考: gensim 4.x で学習する場合のパラメータ整合 --------------------------------------\n",
    "# - Word2Vec(vector_size=EMBEDDING_DIM, ...)  # ※ gensim 3 系の size は 4 系では vector_size に名称変更\n",
    "# - 学習済みベクトルを使う場合は EMBEDDING_DIM と一致していないとロード時に shape 不整合となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00205787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト5.3.2 テキスト取得（説明コメント付き）\n",
    "# ------------------------------------------------------------\n",
    "# 目的:\n",
    "# - Wikipedia から「歴史」「地理」「テスト」カテゴリの見出し語に対する\n",
    "#   日本語サマリーを取得し、後続の前処理や学習（例: 文書分類/系列モデル）で使える\n",
    "#   文字列配列に整形する。\n",
    "#\n",
    "# 設計メモ:\n",
    "# - wikipedia ライブラリの summary/page 取得は、曖昧さ回避（Disambiguation）や\n",
    "#   ページ未存在（PageError）で例外を投げることがあるため、実運用では try/except を推奨。\n",
    "# - 取得結果は「文字列」の配列（list[str]）。後段でトークナイズ→語彙化→パディング等を行う。\n",
    "# - API 呼び出しは外部リソースに依存するため、ネットワーク断やレート制限も考慮が必要。\n",
    "#   デモでは単純な再試行やタイムスリープは省略している。\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import wikipedia\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "\n",
    "# --- Wikipedia を日本語に設定 -------------------------------------------------------------\n",
    "# これにより page/summary の検索対象が日本語版 Wikipedia になる。\n",
    "wikipedia.set_lang(\"ja\")\n",
    "\n",
    "# --- 学習データ(歴史)のタイトル ----------------------------------------------------------\n",
    "list1 = [\n",
    "    \"大和時代\",\n",
    "    \"奈良時代\",\n",
    "    \"平安時代\",\n",
    "    \"鎌倉時代\",\n",
    "    \"室町時代\",\n",
    "    \"安土桃山時代\",\n",
    "    \"江戸時代\",\n",
    "    \"藤原道長\",\n",
    "    \"平清盛\",\n",
    "    \"源頼朝\",\n",
    "    \"北条早雲\",\n",
    "    \"伊達政宗\",\n",
    "    \"徳川家康\",\n",
    "    \"武田信玄\",\n",
    "    \"上杉謙信\",\n",
    "    \"今川義元\",\n",
    "    \"毛利元就\",\n",
    "    \"足利尊氏\",\n",
    "    \"足利義満\",\n",
    "    \"北条泰時\",\n",
    "]\n",
    "\n",
    "# --- 学習データ(地理)のタイトル ----------------------------------------------------------\n",
    "list2 = [\n",
    "    \"東北地方\",\n",
    "    \"関東地方\",\n",
    "    \"中部地方\",\n",
    "    \"近畿地方\",\n",
    "    \"中国地方\",\n",
    "    \"四国地方\",\n",
    "    \"九州地方\",\n",
    "    \"北海道\",\n",
    "    \"秋田県\",\n",
    "    \"福島県\",\n",
    "    \"宮城県\",\n",
    "    \"新潟県\",\n",
    "    \"長野県\",\n",
    "    \"山梨県\",\n",
    "    \"静岡県\",\n",
    "    \"愛知県\",\n",
    "    \"栃木県\",\n",
    "    \"群馬県\",\n",
    "    \"千葉県\",\n",
    "    \"神奈川県\",\n",
    "]\n",
    "\n",
    "# --- テストデータのタイトル --------------------------------------------------------------\n",
    "list3 = [\"織田信長\", \"豊臣秀吉\", \"青森県\", \"北海道\"]\n",
    "\n",
    "\n",
    "# --- 取得用の安全ラッパ ---------------------------------------------------------------\n",
    "# Wikipedia API は以下の例外が発生しうる:\n",
    "# - DisambiguationError: 曖昧さ回避ページにぶつかった場合\n",
    "# - PageError: ページが存在しない場合\n",
    "# この関数では例外を握りつぶさず、最小限のフォールバック文字列を返す。\n",
    "def safe_summary(title: str, sentences: int | None = None) -> str:\n",
    "    \"\"\"\n",
    "    指定タイトルの Wikipedia サマリーを返す。\n",
    "    失敗時はエラー内容を含む短い代替テキストを返す。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # sentences を指定すると要約長を短くできる（None ならライブラリ既定）\n",
    "        return wikipedia.summary(title, sentences=sentences)\n",
    "    except DisambiguationError as e:\n",
    "        # 曖昧さ回避: 候補の一部を添えて知らせる（学習に使う場合は除外も選択肢）\n",
    "        few = \", \".join(e.options[:3])\n",
    "        return f\"[Disambiguation] '{title}' は曖昧です。例: {few} ...\"\n",
    "    except PageError:\n",
    "        return f\"[PageError] '{title}' に対応するページが見つかりません。\"\n",
    "    except Exception as ex:\n",
    "        # ネットワーク等のその他エラー\n",
    "        return f\"[Error] '{title}' の取得に失敗しました: {type(ex).__name__}: {ex}\"\n",
    "\n",
    "\n",
    "# --- 各リストのタイトルからサマリー文字列配列を作成 --------------------------------------\n",
    "# 大規模取得では API 呼び出し回数が多くなるため、必要に応じて\n",
    "# ・キャッシュ（ローカル保存）\n",
    "# ・バックオフ（time.sleep）\n",
    "# ・並列化（ただし Wikipedia API の規約/負荷に留意）\n",
    "# などを検討する。\n",
    "list1_w = [safe_summary(item) for item in list1]\n",
    "list2_w = [safe_summary(item) for item in list2]\n",
    "list3_w = [safe_summary(item) for item in list3]\n",
    "\n",
    "# --- すべての取得結果を 1 つのリストに集約 ------------------------------------------------\n",
    "# 下流で学習/評価に使う「コーパス」。メタ情報（ラベル: 歴史/地理/テスト）を\n",
    "# 併せて管理したい場合は、別途同長のラベル配列や (text, label) のタプル配列にする。\n",
    "list_all_w = list1_w + list2_w + list3_w\n",
    "\n",
    "# --- 動作確認（任意） -------------------------------------------------------------------\n",
    "# 先頭 1–2 件を軽く出力して取得できているかを確認したい場合:\n",
    "# print(list_all_w[0][:120], \"...\")\n",
    "# print(list_all_w[len(list1_w)][:120], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ba6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.3 テキストに対して単語毎にブランクを入れる（説明コメント付き）\n",
    "# ------------------------------------------------------------\n",
    "# 目的:\n",
    "# - 直前で収集した Wikipedia サマリー（list1_w, list2_w, list3_w）を\n",
    "#   形態素解析して「分かち書き」（語と語の間を半角スペースで区切る）に変換する。\n",
    "# - 後段の処理（例: Word2Vec / Doc2Vec / BoW / TF-IDF / RNN/LSTM など）で、\n",
    "#   トークナイザ不要のシンプルな「スペース区切りトークン列」を入力として扱えるようにする。\n",
    "#\n",
    "# 設計メモ:\n",
    "# - 日本語は英語と違い空白を単語境界に用いないため、多くのモデル/ベクトル化器に入力する前に\n",
    "#   形態素解析で単語境界を明示する必要がある。\n",
    "# - 本コードは Janome を採用（純 Python で導入容易・辞書同梱）。高速性やドメイン適合度が\n",
    "#   重要なら MeCab(+IPA/NEologd) 等の代替も検討する。\n",
    "# - wakati=True は「表層形の列」を返すため、品詞情報は捨てて単純な単語列にする運用。\n",
    "#   品詞に応じたフィルタ（名詞/動詞の原形のみ等）を行いたい場合は Token の属性を使う別関数を用意する。\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Tokenizer の生成は比較的コストがかかるため、関数外で 1 度だけインスタンス化して再利用する。\n",
    "# （大量文書を処理する場合のパフォーマンス観点）\n",
    "t = Tokenizer()\n",
    "\n",
    "\n",
    "def wakati(text: str) -> str:\n",
    "    \"\"\"\n",
    "    文字列 `text` を Janome で分かち書きし、半角スペース区切りで返す。\n",
    "\n",
    "    パラメータ:\n",
    "        text (str): 日本語テキスト（Wikipedia サマリーなど）\n",
    "    戻り値:\n",
    "        str: \"単語1 単語2 単語3 ...\" のようなスペース区切り列\n",
    "    備考:\n",
    "        - wakati=True を指定しているため、Token オブジェクトではなく表層形のイテラブルが得られる。\n",
    "        - 記号・数字・助詞なども出力に含まれる（Janome の標準挙動）。\n",
    "          下流で除去/正規化したい場合は別途前処理（例: 正規化、品詞フィルタ）を追加する。\n",
    "        - 未知語（固有名詞・新語）は辞書次第で分割精度が変わる点に留意。\n",
    "    \"\"\"\n",
    "    # Tokenizer.tokenize(..., wakati=True) は「表層形のイテレータ」を返す\n",
    "    # 例: \"徳川家康 は 江戸 幕府 を 開い た\" → ' ' で join して 1 本の文字列に\n",
    "    words_iter = t.tokenize(text, wakati=True)\n",
    "    return \" \".join(words_iter)\n",
    "\n",
    "\n",
    "# --- 分かち書きの実行 ---------------------------------------------------------\n",
    "# list*_w は直前ステップ（リスト 5.3.2）で取得した Wikipedia サマリー配列を想定\n",
    "# * _w: raw 文（sentence/string）のリスト\n",
    "# * _x: 分かち書き（tokenized）後の文のリスト\n",
    "list1_x = [wakati(w) for w in list1_w]  # 歴史カテゴリ\n",
    "list2_x = [wakati(w) for w in list2_w]  # 地理カテゴリ\n",
    "list3_x = [wakati(w) for w in list3_w]  # テスト用（評価/可視化など）\n",
    "\n",
    "# 学習・評価で一括処理したいケースに備えてマージ\n",
    "list_all_x = list1_x + list2_x + list3_x\n",
    "\n",
    "# --- 参考: 以降の一般的な流れ（ここでは実装しない） --------------------------\n",
    "# 1) 正規化（NFKC）、英数字の統一、記号除去など（必要に応じて）\n",
    "# 2) 語彙化（Vocabulary 構築）→ 単語ID列に変換\n",
    "# 3) 固定長化（MAX_LEN へのパディング/トランケーション）\n",
    "# 4) 埋め込み層 or 事前学習ベクトルの読み込み（Word2Vec/BERT トークナイザ等）\n",
    "# 5) 学習（分類/系列ラベリング/類似度計算など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4379d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "総単語数:  1465\n",
      "変換前テキスト:  織田   信長 （ おだ   のぶ な が ） は 、 日本 の 戦国 時代 から 安土 桃山 時代 にかけて の 武将 ・ 大名 。 戦国 の 三 英傑 の 一 人 。 \n",
      " 尾張 国 （ 現在 の 愛知 県 ） 出身 。 織田 信秀 の 嫡男 。 家督 争い の 混乱 を 収め た 後 に 、 桶 狭間 の 戦い で 今川 義元 を 討ち取り 、 勢力 を 拡大 し た 。 足利 義昭 を 奉じ て 上洛 し 、 後 に は 義昭 を 追放 する こと で 、 畿内 を 中心 に 独自 の 中央 政権 （ 「 織田 政権 」 ） を 確立 し て 天下 人 と なっ た 。 しかし 、 天正 10 年 6 月 2 日 （ 1582 年 6 月 21 日 ） 、 家臣 ・ 明智 光秀 に 謀反 を 起こさ れ 、 本能寺 で 自害 し た 。 \n",
      " これ まで 信長 の 政権 は 、 豊臣 秀吉 による 豊臣 政権 、 徳川 家康 が 開い た 江戸 幕府 へ の 流れ を つくっ た 画期的 な もの で 、 その 政治 手法 も 革新 的 な もの で ある と みなさ れ て き た 。 しかし 、 近年 の 歴史 学界 で は その 政策 の 前 時代 性 が 指摘 さ れる よう に なり 、 しばしば 「 中世 社会 の 最終 段階 」 と も 評さ れ 、 その 革新 性 を 否定 する 研究 が 主流 と なっ て いる 。\n",
      "変換後:  [62, 103, 8, 459, 333, 30, 14, 7, 4, 2, 23, 1, 51, 10, 33, 54, 32, 10, 553, 1, 76, 13, 56, 3, 51, 1, 105, 644, 1, 161, 95, 3, 356, 36, 8, 351, 1, 205, 15, 7, 1423, 3, 62, 1424, 1, 320, 3, 558, 1425, 1, 1426, 6, 1427, 11, 75, 5, 2, 564, 565, 1, 342, 9, 204, 353, 6, 1428, 2, 359, 6, 254, 19, 11, 3, 72, 645, 6, 1429, 16, 1430, 19, 2, 75, 5, 4, 645, 6, 517, 20, 37, 9, 2, 211, 6, 93, 5, 1431, 1, 164, 68, 8, 18, 62, 68, 17, 7, 6, 284, 19, 16, 398, 95, 12, 74, 11, 3, 646, 2, 1432, 374, 26, 108, 85, 67, 96, 8, 1433, 26, 108, 85, 627, 96, 7, 2, 1434, 13, 1435, 1436, 5, 1437, 6, 1438, 27, 2, 1439, 9, 1440, 19, 11, 3, 225, 98, 103, 1, 68, 4, 2, 73, 137, 152, 73, 68, 2, 140, 535, 14, 1441, 11, 119, 31, 153, 1, 1442, 6, 1443, 11, 1444, 30, 233, 9, 2, 86, 112, 1445, 25, 647, 43, 30, 233, 9, 22, 12, 1446, 27, 16, 273, 11, 3, 646, 2, 426, 1, 44, 305, 9, 4, 86, 1447, 1, 441, 10, 245, 14, 1448, 29, 50, 117, 5, 114, 2, 1449, 18, 113, 648, 1, 246, 1450, 17, 12, 25, 1451, 27, 2, 86, 647, 245, 6, 1452, 20, 212, 14, 1453, 12, 74, 16, 60, 3]\n",
      "パディング後:  [   3  646    2  426    1   44  305    9    4   86 1447    1  441   10\n",
      "  245   14 1448   29   50  117    5  114    2 1449   18  113  648    1\n",
      "  246 1450   17   12   25 1451   27    2   86  647  245    6 1452   20\n",
      "  212   14 1453   12   74   16   60    3]\n",
      "正解データ(学習用):  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "正解データ(検証用):  [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# リスト 5.3.4 学習データ作成（説明コメント付き）\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer:\n",
    "# - テキストを語彙辞書（単語→整数ID）に変換し、整数ID列（シーケンス）へ数値化するユーティリティ。\n",
    "# - 既定では「出現頻度の高い順」に 1, 2, 3, ... の ID が割り当てられる（0 はパディング用に予約しない設計）。\n",
    "# - 実務では oov_token（未知語トークン）を指定しておくと、学習時に未観測な語にも頑健になる。\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 学習・検証で使う全テキストを引数にして辞書を作成する\n",
    "# 注意（データリーク）:\n",
    "# - ここでは list_all_x（学習+検証+テスト相当）をまとめて fit しているため、\n",
    "#   厳密には検証・テスト側の語彙情報が学習側に漏れる（軽微だが情報リーク）。\n",
    "# - 再現実験や教材としては簡潔だが、厳密評価では「学習コーパスのみで fit」→\n",
    "#   「検証/テストは texts_to_sequences のみ」を推奨。\n",
    "tokenizer.fit_on_texts(list_all_x)\n",
    "\n",
    "# 単語一覧（語→ID の辞書）を取得\n",
    "# 例: {'江戸時代': 1, 'に': 2, 'は': 3, ...}\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 総単語数（ユニーク語彙数）を取得\n",
    "num_words = len(word_index)\n",
    "print(\"総単語数: \", num_words)\n",
    "\n",
    "# 変換前の検証用テキスト確認（分かち書き済みの文字列）\n",
    "# - list3_x はテスト用タイトルの Wikipedia サマリーを分かち書きした配列（前工程参照）。\n",
    "print(\"変換前テキスト: \", list3_x[0])\n",
    "\n",
    "# テキストの数値化:\n",
    "# - texts_to_sequences は、各テキストを語彙辞書に基づいて「整数IDのリスト」へ変換する。\n",
    "# - 未知語は既定では無視され、ID 化されない（→ シーケンスから脱落）。\n",
    "#   未知語を保持したい場合は Tokenizer(oov_token='[UNK]') などを利用。\n",
    "sequence_test = tokenizer.texts_to_sequences(list3_x)\n",
    "\n",
    "# 変換結果確認（例: [12, 345, 78, ...]）\n",
    "print(\"変換後: \", sequence_test[0])\n",
    "\n",
    "# 単語のパディング:\n",
    "# - ニューラルネットに固定長テンソルを与えるため、pad_sequences で\n",
    "#   「短い文は 0 で埋める」「長い文は途中で切り詰める」処理を行う。\n",
    "# - 既定では先頭側にパディング（padding='pre'）、長い場合は先頭側を切る（truncating='pre'）。\n",
    "#   LSTM の方向やモデル設計に応じて 'post' に変えることも多い。\n",
    "sequence_test = pad_sequences(sequence_test, maxlen=MAX_LEN)\n",
    "\n",
    "# パディング後の確認（長さが MAX_LEN の固定長ベクトルに）\n",
    "print(\"パディング後: \", sequence_test[0])\n",
    "\n",
    "# 学習データ（歴史: list1_x、地理: list2_x）に対しても同じ数値化→パディングを適用\n",
    "# - fit はすでに済んでいるため、ここでは texts_to_sequences + pad_sequences のみ。\n",
    "sequence_train = tokenizer.texts_to_sequences(list1_x + list2_x)\n",
    "sequence_train = pad_sequences(sequence_train, maxlen=MAX_LEN)\n",
    "\n",
    "# 正解ラベルの作成:\n",
    "# - 2 クラス分類を想定し、歴史カテゴリを 0、地理カテゴリを 1 として付与。\n",
    "# - 学習用 y は学習データの件数に合わせて 0/1 を連結。\n",
    "# - 検証/テスト用 y は list3_x の内容（例: ['織田信長','豊臣秀吉','青森県','北海道']）に合わせて\n",
    "#   先頭 2 件を歴史=0、後ろ 2 件を地理=1 として作成（デモ簡略化のための固定割当）。\n",
    "Y_train = np.array([0] * len(list1_x) + [1] * len(list2_x))\n",
    "Y_test = np.array([0] * 2 + [1] * 2)\n",
    "\n",
    "# ラベル配列の確認\n",
    "print(\"正解データ(学習用): \", Y_train)\n",
    "print(\"正解データ(検証用): \", Y_test)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 補足（実務上のベストプラクティス）:\n",
    "# - 語彙構築は学習データのみに限定（データリーク回避）。\n",
    "# - トークン頻度に基づく語彙上限 num_words を設定し、希少語をまとめて OOV へ。\n",
    "# - pad_sequences の padding/truncating はモデル特性に合わせて 'post' を検討。\n",
    "# - ラベルは One-Hot ではなく整数ラベルのままでも SparseCategoricalCrossentropy で学習可。\n",
    "# - クラス不均衡がある場合は class_weight やサンプリングで補正。\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e462e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 症状の本質\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Jupyter/Colab では sys.argv に「-f=…/kernel-xxxx.json」という引数が自動で\n",
    "# 混入するため、スクリプト側が「sys.argv[1] をモデルパス」と決め打ちすると\n",
    "# その -f をファイルパスと誤認して FileNotFoundError になります。\n",
    "#\n",
    "# 回避方針（gensim 4 系前提）:\n",
    "#  1) ノートブックでは引数を使わず、明示パス（例: \"ja.bin\"）で関数を直呼び出し。\n",
    "#  2) CLI 実行時は argparse(parse_known_args) で未知引数(-f)を無視。\n",
    "#  3) ついでに Git LFS ポインタや形式差（pickle/word2vec互換）を自動判別。\n",
    "#\n",
    "# 下は「ノートブック/CLI 両対応」の最小完全版です。\n",
    "#  - ch05-03-01.ipynb と同じディレクトリに ja.bin がある想定。\n",
    "#  - 形式は Word2Vec.save() 産物 or KeyedVectors or word2vec互換(.bin/.vec/.txt)のいずれでもOK。\n",
    "#  - LFS ポインタなら明示的にエラーを出して git lfs pull を促します。\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "LFS_SIGNATURE = b\"version https://git-lfs.github.com/spec\"\n",
    "\n",
    "\n",
    "def _is_lfs_pointer(path: Path, sniff=64) -> bool:\n",
    "    with path.open(\"rb\") as f:\n",
    "        return f.read(sniff).startswith(LFS_SIGNATURE)\n",
    "\n",
    "\n",
    "def _looks_like_pickle(path: Path) -> bool:\n",
    "    # Gensim の .save() 産物（pickle）は 0x80 で始まることが多い\n",
    "    with path.open(\"rb\") as f:\n",
    "        return f.read(1) == b\"\\x80\"\n",
    "\n",
    "\n",
    "def _looks_like_w2v_text_header(path: Path) -> bool:\n",
    "    # 先頭行が \"語彙数 ベクトル次元\" の ASCII 2整数なら word2vec テキスト互換とみなす\n",
    "    with path.open(\"rb\") as f:\n",
    "        line = f.readline()\n",
    "    try:\n",
    "        h = line.decode(\"ascii\").strip().split()\n",
    "        return len(h) >= 2 and int(h[0]) >= 1 and int(h[1]) >= 1\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _looks_like_w2v_binary_header(path: Path) -> bool:\n",
    "    # バイナリも先頭に \"語彙数 ベクトル次元\\n\" の ASCII ヘッダがある\n",
    "    with path.open(\"rb\") as f:\n",
    "        buf = []\n",
    "        while True:\n",
    "            b = f.read(1)\n",
    "            if not b or b == b\"\\n\":\n",
    "                break\n",
    "            buf.append(b)\n",
    "    try:\n",
    "        h = b\"\".join(buf).decode(\"ascii\").strip().split()\n",
    "        return len(h) >= 2 and int(h[0]) >= 1 and int(h[1]) >= 1\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def load_word_vectors_auto(path: Path) -> KeyedVectors:\n",
    "    \"\"\"ファイル種別を自動判別して KeyedVectors を返す（gensim 4 系）。\"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"埋め込みファイルが見つかりません: {path}\")\n",
    "\n",
    "    # LFS ポインタ検出\n",
    "    if _is_lfs_pointer(path):\n",
    "        raise RuntimeError(\n",
    "            f\"LFS ポインタを検出: {path}\\n\"\n",
    "            f\"実体が未取得です。`git lfs pull` を実行してからリトライしてください。\"\n",
    "        )\n",
    "\n",
    "    # Gensim の pickle 産物（Word2Vec/KeyedVectors の .save()）\n",
    "    if _looks_like_pickle(path):\n",
    "        try:\n",
    "            # 完全モデル\n",
    "            return Word2Vec.load(str(path)).wv\n",
    "        except Exception:\n",
    "            # KeyedVectors 直列化\n",
    "            return KeyedVectors.load(str(path))\n",
    "\n",
    "    # word2vec 互換（テキスト or バイナリ）\n",
    "    if _looks_like_w2v_text_header(path):\n",
    "        # まず UTF-8 で試行、駄目なら cp932 の緩和読み\n",
    "        try:\n",
    "            return KeyedVectors.load_word2vec_format(\n",
    "                str(path), binary=False, encoding=\"utf-8\"\n",
    "            )\n",
    "        except UnicodeDecodeError:\n",
    "            return KeyedVectors.load_word2vec_format(\n",
    "                str(path), binary=False, encoding=\"cp932\", unicode_errors=\"ignore\"\n",
    "            )\n",
    "\n",
    "    if _looks_like_w2v_binary_header(path):\n",
    "        return KeyedVectors.load_word2vec_format(str(path), binary=True)\n",
    "\n",
    "    # 拡張子ヒューリスティック（最後の砦）\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".kv\":\n",
    "        return KeyedVectors.load(str(path))\n",
    "    if suf == \".bin\":\n",
    "        # Gensim pickle or word2vec 互換の両面待ち\n",
    "        try:\n",
    "            return Word2Vec.load(str(path)).wv\n",
    "        except Exception:\n",
    "            return KeyedVectors.load_word2vec_format(str(path), binary=True)\n",
    "    if suf in {\".vec\", \".txt\"}:\n",
    "        try:\n",
    "            return KeyedVectors.load_word2vec_format(\n",
    "                str(path), binary=False, encoding=\"utf-8\"\n",
    "            )\n",
    "        except UnicodeDecodeError:\n",
    "            return KeyedVectors.load_word2vec_format(\n",
    "                str(path), binary=False, encoding=\"cp932\", unicode_errors=\"ignore\"\n",
    "            )\n",
    "\n",
    "    raise RuntimeError(\n",
    "        \"形式の自動判別に失敗しました。\\n\"\n",
    "        \"・Gensim .save() 産物は Word2Vec.load/KeyedVectors.load で読めます。\\n\"\n",
    "        \"・word2vec 互換は .bin→binary=True, .vec/.txt→binary=False で読めます。\\n\"\n",
    "        \"・LFS ポインタではないこと、圧縮ファイルを渡していないことを確認してください。\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _running_in_notebook() -> bool:\n",
    "    # Jupyter/ipykernel は -f=kernel-xxx.json を argv に差す\n",
    "    return any(a == \"-f\" or a.startswith(\"-f=\") for a in sys.argv)\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# ノートブック/CLI 両対応のエントリポイント\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    if _running_in_notebook():\n",
    "        # Notebook: 引数は使わず、作業ディレクトリ直下の 'ja.bin' を既定に。\n",
    "        model_path = Path(\"ja.bin\")\n",
    "    else:\n",
    "        # CLI: 未知引数(-f)は無視してモデルパスだけ拾う\n",
    "        import argparse\n",
    "\n",
    "        parser = argparse.ArgumentParser(add_help=False)\n",
    "        parser.add_argument(\"model\", nargs=\"?\", default=\"ja.bin\")  # 省略時デフォルト\n",
    "        args, _ = parser.parse_known_args()\n",
    "        model_path = Path(args.model)\n",
    "\n",
    "    print(f\"[INFO] CWD: {Path.cwd()}\")\n",
    "    print(f\"[INFO] Target model: {model_path}\")\n",
    "    if not model_path.exists():\n",
    "        # 補助: その場のファイル一覧を出して目視で確認しやすく\n",
    "        print(\"[HINT] カレント直下のファイル一覧:\")\n",
    "        for p in Path(\".\").glob(\"*\"):\n",
    "            print(\"  -\", p)\n",
    "        raise FileNotFoundError(f\"見つかりません: {model_path}\")\n",
    "\n",
    "    kv = load_word_vectors_auto(model_path)\n",
    "    print(\"[OK] ロード成功\")\n",
    "    print(f\"- 語彙数: {len(kv.index_to_key)}\")\n",
    "    print(f\"- ベクトル次元: {kv.vector_size}\")\n",
    "\n",
    "    # 簡易デモ（語があれば類似語を表示）\n",
    "    token = \"世間\"\n",
    "    if token in kv:\n",
    "        print(f\"\\n[DEMO] most_similar('{token}'):\")\n",
    "        for w, s in kv.most_similar(token, topn=10):\n",
    "            print(f\"  {w}\\t{s:.3f}\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"\\n[INFO] '{token}' は語彙に未登録です。先頭10語: {kv.index_to_key[:10]}\"\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30bafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.6 Embedding Matrix作成（説明コメント付き）\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 目的：\n",
    "# - Keras/TensorFlow の Embedding 層に初期重みとして与えるための行列（embedding_matrix）を構築する。\n",
    "# - 行列の各行 i は語彙 ID i に対応し、各行ベクトルは事前学習済み Word2Vec の埋め込みベクトルとなる。\n",
    "#   （ID=0 は padding 用の行としてゼロベクトルにしておくのが一般的）\n",
    "\n",
    "# 前提：\n",
    "# - `num_words` : Tokenizer.fit_on_texts() 後に得られる語彙サイズ（リスト 5.3.4 で作成）\n",
    "# - `EMBEDDING_DIM` : 事前学習 Word2Vec のベクトル次元（リスト 5.3.1 で定義）\n",
    "# - `word_index` : Keras Tokenizer が作った「単語 → 連番ID（1始まり）」の辞書\n",
    "# - `word_vectors` : 事前学習済み Word2Vec（あるいは KeyedVectors）オブジェクト\n",
    "#   例）gensim.models.Word2Vec.load(...) や KeyedVectors.load(...)\n",
    "\n",
    "# メモ：\n",
    "# - 行列サイズは (num_words + 1, EMBEDDING_DIM)。+1 は ID=0（PAD）分の余白。\n",
    "# - dtype は float32 として GPU 転送・計算の効率を確保し、メモリ使用量も抑える。\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM), dtype=np.float32)\n",
    "\n",
    "# Embedding Matrix に Word2Vec の重みベクトルをコピー\n",
    "# - Tokenizer の語彙（word_index）を走査し、各単語に割り当てられた ID（i）に対応する行へベクトルを貼る。\n",
    "# - OOV（事前学習に無かった単語）はゼロベクトルのまま残る（必要に応じて <UNK> を平均ベクトル等で初期化する設計もある）。\n",
    "for word, i in word_index.items():\n",
    "    # 【gensim 3 系の存在確認】：\n",
    "    #   Word2Vec モデルでは `word_vectors.wv.vocab` に語彙辞書（単語→Vocab）がある。\n",
    "    #   ここでは「単語が事前学習語彙に含まれているか」を確認してからコピーする。\n",
    "    #   ※ gensim 4 では `wv.vocab` は廃止。`key_to_index` を使う（下記参照）。\n",
    "    if word in word_vectors.wv.vocab:\n",
    "        # ベクトルの貼り付け：\n",
    "        # - `word_vectors[word]` は該当単語の埋め込みベクトル（長さ EMBEDDING_DIM）\n",
    "        # - i 行目にそのままコピーすることで、ID→ベクトルの写像を完成させる。\n",
    "        embedding_matrix[i] = word_vectors[word]\n",
    "\n",
    "        # （理論メモ）\n",
    "        # - ここで貼るベクトルは事前学習空間の点であり、下流モデルは線形変換や再学習により\n",
    "        #   タスク分布へ適応していく（fine-tune するか、固定するかは用途次第）。\n",
    "        # - 事前学習時と推論時で分かち書き器が異なると OOV が増え、ゼロ行が多くなるため性能低下に直結する。\n",
    "\n",
    "# 参考（コメントのみ・書き換え案）：\n",
    "# - gensim 4 系では `wv.vocab` が廃止され `key_to_index` に置換。\n",
    "#   その場合は下のように書く：\n",
    "#     if word in word_vectors.key_to_index:  # もしくは word_vectors.wv.key_to_index\n",
    "#         embedding_matrix[i] = word_vectors.get_vector(word)  # もしくは word_vectors[word]\n",
    "#\n",
    "# - また、`EMBEDDING_DIM` と `word_vectors.vector_size` が一致していることを\n",
    "#   事前に assert で確認しておくと安全（次元不一致は理論的にも計算的にも破綻の原因）。\n",
    "#\n",
    "# - OOV をすべてゼロにせず、Tokenizer(oov_token=\"<UNK>\") の ID を平均ベクトルで埋めると\n",
    "#   未知語に対しても語彙空間の中心近傍から学習を進められ、収束が安定する場合がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f9aef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.7 LSTMモデル作成（説明コメント付き）\n",
    "# 目的：\n",
    "# - 事前学習済み Word2Vec で初期化した埋め込みを入力とし、LSTM により\n",
    "#   文（固定長 MAX_LEN）を二値分類（歴史=0 / 地理=1）するシンプルなモデルを構築する。\n",
    "#\n",
    "# ポイント：\n",
    "# - Embedding 層は学習済みベクトル（embedding_matrix）で初期化し、PAD（ID=0）はゼロ行のまま。\n",
    "# - mask_zero=True により PAD=0 を LSTM へ伝播しない（系列長が短くても学習が安定）。\n",
    "# - trainable=False とすることで、事前学習ベクトルを固定（微調整したい場合は True）。\n",
    "# - 出力は Dense(1, sigmoid) で二値交差エントロピーの損失を採用。\n",
    "\n",
    "# 依存関係（tf.keras を推奨：環境差異に強い）\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "\n",
    "# 安全性チェック：埋め込み行列の形が想定どおりか検証\n",
    "# embedding_matrix.shape == (num_words + 1, EMBEDDING_DIM) を仮定\n",
    "assert embedding_matrix.shape == (\n",
    "    num_words + 1,\n",
    "    EMBEDDING_DIM,\n",
    "), f\"embedding_matrix の形が不一致です: {embedding_matrix.shape} ≠ ({num_words + 1}, {EMBEDDING_DIM})\"\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding 層\n",
    "# - input_dim=num_words+1 : 語彙サイズ（0 は PAD 用に確保）\n",
    "# - output_dim=EMBEDDING_DIM : ベクトル次元（Word2Vec と一致させる）\n",
    "# - weights=[embedding_matrix] : 事前学習済み重みで初期化\n",
    "# - input_length=MAX_LEN : 入力系列長（pad_sequences の長さ）\n",
    "# - mask_zero=True : トークンID=0 をマスク（PAD の影響を LSTM に渡さない）\n",
    "# - trainable=False : 埋め込みを固定（必要に応じて True で微調整）\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=num_words + 1,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_LEN,\n",
    "        mask_zero=True,\n",
    "        trainable=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# LSTM 層\n",
    "# - units=32 : 隠れ状態の次元\n",
    "# - dropout : 入力に対するドロップアウト（過学習抑制）\n",
    "# - recurrent_dropout : 再帰接続に対するドロップアウト（CPU 実装で有効。GPU/cuDNN では無視される場合あり）\n",
    "# - return_sequences=False : 最終時刻の隠れ状態のみ出力（分類タスクなので十分）\n",
    "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2, return_sequences=False))\n",
    "\n",
    "# 出力層\n",
    "# - 1 ユニット + sigmoid：二値分類の確率（[0,1]）を出力\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# コンパイル\n",
    "# - 損失：binary_crossentropy（二値分類に適合）\n",
    "# - 最適化器：Adam（学習率自動調整で収束が安定）\n",
    "# - 評価指標：accuracy（分類精度）\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# モデル概要の表示（入出力形状とパラメータ数を確認）\n",
    "model.summary()\n",
    "\n",
    "# 学習時の注意：\n",
    "# - sequence_train（形状: [N_train, MAX_LEN]）, Y_train（形状: [N_train]）で model.fit(...) を実行。\n",
    "# - 検証には sequence_test, Y_test を使用。\n",
    "# - 事前学習埋め込みを微調整したい場合は Embedding(trainable=True) に変更。\n",
    "# - データが小規模で過学習する場合は LSTM ユニット数を減らす / 早期終了 / 正則化を検討。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8b343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.8 学習（説明コメント付き）\n",
    "# 目的：\n",
    "# - 事前に構築した LSTM モデル（リスト 5.3.7）を、\n",
    "#   前処理済みデータ sequence_train / Y_train で学習し、\n",
    "#   sequence_test / Y_test で汎化性能を監視する。\n",
    "#\n",
    "# 重要ポイント（理論）：\n",
    "# - 早期終了（EarlyStopping）：\n",
    "#   * 検証損失 val_loss を監視し、改善が止まったら学習を打ち切ることで\n",
    "#     過学習（train が進むが val が悪化）を抑制し、最良エポックの重みを復元する。\n",
    "#   * 一般に「検証損失」は汎化誤差の proxy として扱われるため、これが最小となる点を\n",
    "#     モデル選択の基準とするのが理にかなっている（構成要素最小化によるバイアス）。\n",
    "# - 学習率自動調整（ReduceLROnPlateau）：\n",
    "#   * Plateu（改善停滞）検出時に learning rate を下げ、局所解周辺での\n",
    "#     微細な探索を促す。凸でない最適化（ニューラルネット）では LR スケジュールが\n",
    "#     収束安定性に与える影響が大きい。\n",
    "# - エポック数とバッチサイズ：\n",
    "#   * 大きな batch は勾配分散が小さく収束が速い一方、汎化が悪化することがある。\n",
    "#     リソースとデータ量に応じて調整（本例は 128）。エポック上限は早期終了で実質短縮される前提。\n",
    "# - 再現性：\n",
    "#   * 研究用途では乱数シード固定を推奨。ただし完全再現はスレッド/BLAS/非決定実装に依存して難しい点に注意。\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# （任意）乱数シード固定：結果のばらつきを抑える\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ハイパーパラメータ（ユーザー指定値を変数化）\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# コールバック設定\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"val_loss\",  # 汎化性能の代理指標\n",
    "    patience=7,  # 7エポック改善しなければ停止（データ量に応じて調整）\n",
    "    restore_best_weights=True,  # 最良（最小 val_loss）エポックの重みに巻き戻す\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",  # 改善停滞の検知対象\n",
    "    factor=0.5,  # 学習率を半減\n",
    "    patience=3,  # 3エポック停滞で LR 低減\n",
    "    min_lr=1e-6,  # 下限（過度な低下を防止）\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# 入力検証（安全性チェック）\n",
    "assert (\n",
    "    sequence_train.shape[1] == sequence_test.shape[1]\n",
    "), f\"パディング長（MAX_LEN）が一致していません: {sequence_train.shape[1]} vs {sequence_test.shape[1]}\"\n",
    "assert sequence_train.shape[0] == len(\n",
    "    Y_train\n",
    "), f\"学習入力と正解ラベルの件数が不一致: X={sequence_train.shape[0]} / y={len(Y_train)}\"\n",
    "assert sequence_test.shape[0] == len(\n",
    "    Y_test\n",
    "), f\"検証入力と正解ラベルの件数が不一致: X={sequence_test.shape[0]} / y={len(Y_test)}\"\n",
    "\n",
    "# （任意）クラス不均衡対策：\n",
    "# - Y_train の分布を見て偏りが大きい場合は class_weight を導入すると良い。\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# classes = np.unique(Y_train)\n",
    "# class_weights = compute_class_weight('balanced', classes=classes, y=Y_train)\n",
    "# class_weight = {int(c): w for c, w in zip(classes, class_weights)}\n",
    "class_weight = None  # ここでは未使用（均衡仮定）\n",
    "\n",
    "# モデル学習の実施\n",
    "# - validation_data を指定して汎化誤差を逐次評価\n",
    "# - コールバックで過学習を抑制しつつ、最良点を保持\n",
    "history = model.fit(\n",
    "    sequence_train,\n",
    "    Y_train,\n",
    "    validation_data=(sequence_test, Y_test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    class_weight=class_weight,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# （任意）学習履歴の最終値を表示（レポーティング用途）\n",
    "final_train_loss = history.history[\"loss\"][-1]\n",
    "final_val_loss = history.history[\"val_loss\"][-1]\n",
    "final_train_acc = history.history.get(\"accuracy\", [None])[-1]\n",
    "final_val_acc = history.history.get(\"val_accuracy\", [None])[-1]\n",
    "print(\n",
    "    f\"[Result] loss={final_train_loss:.4f} val_loss={final_val_loss:.4f} \"\n",
    "    f\"acc={final_train_acc if final_train_acc is None else f'{final_train_acc:.4f}'} \"\n",
    "    f\"val_acc={final_val_acc if final_val_acc is None else f'{final_val_acc:.4f}'}\"\n",
    ")\n",
    "\n",
    "# 備考：\n",
    "# - 追加で ModelCheckpoint を使うと、最良エポックの重みを自動保存できる。\n",
    "#   ただしリポジトリ直下に大きなファイルを作ると Git 運用が重くなるため、\n",
    "#   研究ログ用ディレクトリ（例：./artifacts/）や Git LFS を活用すること。\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# ckpt = ModelCheckpoint(\"./artifacts/best_lstm.keras\", monitor=\"val_loss\", save_best_only=True)\n",
    "# → callbacks=[early_stop, reduce_lr, ckpt]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
