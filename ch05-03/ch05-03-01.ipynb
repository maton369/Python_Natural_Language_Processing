{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7069e1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.1 初期処理と変数宣言（説明コメント付き）\n",
    "# ------------------------------------------------------------\n",
    "# 目的:\n",
    "# - Mac（特に Apple Silicon / MKL 由来の OpenMP ランタイム）環境で発生しやすい\n",
    "#   \"KMP_DUPLICATE_LIB_OK\" 警告クラッシュの回避設定\n",
    "# - ノートブック/学習ログの見通しを良くするための警告抑止（デモ用）\n",
    "# - Word2Vec 埋め込み次元数および LSTM に入力する最大系列長の基準値を宣言\n",
    "#\n",
    "# 注意:\n",
    "# - 以下の環境変数・警告抑止は「実験用の便宜措置」です。プロダクションでは\n",
    "#   依存ライブラリの統一や仮想環境管理（conda/venv/uv など）で根本対応するのが望ましい。\n",
    "# - EMBEDDING_DIM / MAX_LEN は下流のモデル定義・前処理（Tokenizer, padding 戦略）\n",
    "#   と一貫している必要があります（不一致だと shape エラーや性能劣化につながる）。\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- Mac の OpenMP ランタイム多重ロード問題の暫定回避 ---------------------------------\n",
    "# Intel MKL / libomp 等が複数ロードされるとクラッシュするケースがあり、Mac で学習時に\n",
    "# \"KMP_DUPLICATE_LIB_OK\" を True にして回避することがある。\n",
    "# 本来は「ライブラリの競合解消（単一の OpenMP に統一）」が理想。恒久対策ではない点に注意。\n",
    "import os\n",
    "import platform\n",
    "\n",
    "if platform.system() == \"Darwin\":\n",
    "    os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"  # デモ/学習用の一時対応\n",
    "\n",
    "# --- 警告抑止（デモの可読性向上のため） ---------------------------------------------------\n",
    "# 学習途中の FutureWarning / DeprecationWarning などで出力がノイズ化するのを防ぐ。\n",
    "# ただし、重要な警告まで隠すリスクがあるため、本番/検証では必要な種類だけを個別に filter すること。\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 埋め込み次元（Word2Vec の隠れ層ノード数 = ベクトル次元数） ---------------------------\n",
    "# 一般的な選択肢: 100〜300（コーパス規模が小〜中の場合）。大きくするほど表現力は上がるが、\n",
    "# 学習時間・メモリ（O(|V|×D)）が増加し過学習のリスクも高まる。\n",
    "# 既存の学習済みモデル（例: fastText 300 次元）と互換を取りたい場合は、その次元に合わせる。\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# --- LSTM に入力する最大系列長（トークン数） ----------------------------------------------\n",
    "# 前処理側（Tokenizer/分かち書き）で得られた系列を padding/truncation する長さ。\n",
    "# 値を大きくすると長距離依存を捉えやすくなるが、計算負荷とメモリ消費が増える。\n",
    "# 日本語文で 50 はやや短め。短文中心のデータ（ツイート等）には妥当だが、長文なら 128〜256 も検討。\n",
    "# 下流モデル（Embedding 層の input_length, LSTM の time_steps）や学習用 DataLoader と整合させること。\n",
    "MAX_LEN = 50\n",
    "\n",
    "# --- 参考: 再現性確保のための乱数シード（必要に応じて使用） ------------------------------\n",
    "# import random, numpy as np\n",
    "# import torch\n",
    "# SEED = 42\n",
    "# random.seed(SEED); np.random.seed(SEED)\n",
    "# torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "# torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "#\n",
    "# --- 参考: gensim 4.x で学習する場合のパラメータ整合 --------------------------------------\n",
    "# - Word2Vec(vector_size=EMBEDDING_DIM, ...)  # ※ gensim 3 系の size は 4 系では vector_size に名称変更\n",
    "# - 学習済みベクトルを使う場合は EMBEDDING_DIM と一致していないとロード時に shape 不整合となる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00205787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト5.3.2 テキスト取得（説明コメント付き）\n",
    "# ------------------------------------------------------------\n",
    "# 目的:\n",
    "# - Wikipedia から「歴史」「地理」「テスト」カテゴリの見出し語に対する\n",
    "#   日本語サマリーを取得し、後続の前処理や学習（例: 文書分類/系列モデル）で使える\n",
    "#   文字列配列に整形する。\n",
    "#\n",
    "# 設計メモ:\n",
    "# - wikipedia ライブラリの summary/page 取得は、曖昧さ回避（Disambiguation）や\n",
    "#   ページ未存在（PageError）で例外を投げることがあるため、実運用では try/except を推奨。\n",
    "# - 取得結果は「文字列」の配列（list[str]）。後段でトークナイズ→語彙化→パディング等を行う。\n",
    "# - API 呼び出しは外部リソースに依存するため、ネットワーク断やレート制限も考慮が必要。\n",
    "#   デモでは単純な再試行やタイムスリープは省略している。\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import wikipedia\n",
    "from wikipedia.exceptions import PageError, DisambiguationError\n",
    "\n",
    "# --- Wikipedia を日本語に設定 -------------------------------------------------------------\n",
    "# これにより page/summary の検索対象が日本語版 Wikipedia になる。\n",
    "wikipedia.set_lang(\"ja\")\n",
    "\n",
    "# --- 学習データ(歴史)のタイトル ----------------------------------------------------------\n",
    "list1 = [\n",
    "    \"大和時代\",\n",
    "    \"奈良時代\",\n",
    "    \"平安時代\",\n",
    "    \"鎌倉時代\",\n",
    "    \"室町時代\",\n",
    "    \"安土桃山時代\",\n",
    "    \"江戸時代\",\n",
    "    \"藤原道長\",\n",
    "    \"平清盛\",\n",
    "    \"源頼朝\",\n",
    "    \"北条早雲\",\n",
    "    \"伊達政宗\",\n",
    "    \"徳川家康\",\n",
    "    \"武田信玄\",\n",
    "    \"上杉謙信\",\n",
    "    \"今川義元\",\n",
    "    \"毛利元就\",\n",
    "    \"足利尊氏\",\n",
    "    \"足利義満\",\n",
    "    \"北条泰時\",\n",
    "]\n",
    "\n",
    "# --- 学習データ(地理)のタイトル ----------------------------------------------------------\n",
    "list2 = [\n",
    "    \"東北地方\",\n",
    "    \"関東地方\",\n",
    "    \"中部地方\",\n",
    "    \"近畿地方\",\n",
    "    \"中国地方\",\n",
    "    \"四国地方\",\n",
    "    \"九州地方\",\n",
    "    \"北海道\",\n",
    "    \"秋田県\",\n",
    "    \"福島県\",\n",
    "    \"宮城県\",\n",
    "    \"新潟県\",\n",
    "    \"長野県\",\n",
    "    \"山梨県\",\n",
    "    \"静岡県\",\n",
    "    \"愛知県\",\n",
    "    \"栃木県\",\n",
    "    \"群馬県\",\n",
    "    \"千葉県\",\n",
    "    \"神奈川県\",\n",
    "]\n",
    "\n",
    "# --- テストデータのタイトル --------------------------------------------------------------\n",
    "list3 = [\"織田信長\", \"豊臣秀吉\", \"青森県\", \"北海道\"]\n",
    "\n",
    "\n",
    "# --- 取得用の安全ラッパ ---------------------------------------------------------------\n",
    "# Wikipedia API は以下の例外が発生しうる:\n",
    "# - DisambiguationError: 曖昧さ回避ページにぶつかった場合\n",
    "# - PageError: ページが存在しない場合\n",
    "# この関数では例外を握りつぶさず、最小限のフォールバック文字列を返す。\n",
    "def safe_summary(title: str, sentences: int | None = None) -> str:\n",
    "    \"\"\"\n",
    "    指定タイトルの Wikipedia サマリーを返す。\n",
    "    失敗時はエラー内容を含む短い代替テキストを返す。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # sentences を指定すると要約長を短くできる（None ならライブラリ既定）\n",
    "        return wikipedia.summary(title, sentences=sentences)\n",
    "    except DisambiguationError as e:\n",
    "        # 曖昧さ回避: 候補の一部を添えて知らせる（学習に使う場合は除外も選択肢）\n",
    "        few = \", \".join(e.options[:3])\n",
    "        return f\"[Disambiguation] '{title}' は曖昧です。例: {few} ...\"\n",
    "    except PageError:\n",
    "        return f\"[PageError] '{title}' に対応するページが見つかりません。\"\n",
    "    except Exception as ex:\n",
    "        # ネットワーク等のその他エラー\n",
    "        return f\"[Error] '{title}' の取得に失敗しました: {type(ex).__name__}: {ex}\"\n",
    "\n",
    "\n",
    "# --- 各リストのタイトルからサマリー文字列配列を作成 --------------------------------------\n",
    "# 大規模取得では API 呼び出し回数が多くなるため、必要に応じて\n",
    "# ・キャッシュ（ローカル保存）\n",
    "# ・バックオフ（time.sleep）\n",
    "# ・並列化（ただし Wikipedia API の規約/負荷に留意）\n",
    "# などを検討する。\n",
    "list1_w = [safe_summary(item) for item in list1]\n",
    "list2_w = [safe_summary(item) for item in list2]\n",
    "list3_w = [safe_summary(item) for item in list3]\n",
    "\n",
    "# --- すべての取得結果を 1 つのリストに集約 ------------------------------------------------\n",
    "# 下流で学習/評価に使う「コーパス」。メタ情報（ラベル: 歴史/地理/テスト）を\n",
    "# 併せて管理したい場合は、別途同長のラベル配列や (text, label) のタプル配列にする。\n",
    "list_all_w = list1_w + list2_w + list3_w\n",
    "\n",
    "# --- 動作確認（任意） -------------------------------------------------------------------\n",
    "# 先頭 1–2 件を軽く出力して取得できているかを確認したい場合:\n",
    "# print(list_all_w[0][:120], \"...\")\n",
    "# print(list_all_w[len(list1_w)][:120], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ba6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 5.3.3 テキストに対して単語毎にブランクを入れる（説明コメント付き）\n",
    "# ------------------------------------------------------------\n",
    "# 目的:\n",
    "# - 直前で収集した Wikipedia サマリー（list1_w, list2_w, list3_w）を\n",
    "#   形態素解析して「分かち書き」（語と語の間を半角スペースで区切る）に変換する。\n",
    "# - 後段の処理（例: Word2Vec / Doc2Vec / BoW / TF-IDF / RNN/LSTM など）で、\n",
    "#   トークナイザ不要のシンプルな「スペース区切りトークン列」を入力として扱えるようにする。\n",
    "#\n",
    "# 設計メモ:\n",
    "# - 日本語は英語と違い空白を単語境界に用いないため、多くのモデル/ベクトル化器に入力する前に\n",
    "#   形態素解析で単語境界を明示する必要がある。\n",
    "# - 本コードは Janome を採用（純 Python で導入容易・辞書同梱）。高速性やドメイン適合度が\n",
    "#   重要なら MeCab(+IPA/NEologd) 等の代替も検討する。\n",
    "# - wakati=True は「表層形の列」を返すため、品詞情報は捨てて単純な単語列にする運用。\n",
    "#   品詞に応じたフィルタ（名詞/動詞の原形のみ等）を行いたい場合は Token の属性を使う別関数を用意する。\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Tokenizer の生成は比較的コストがかかるため、関数外で 1 度だけインスタンス化して再利用する。\n",
    "# （大量文書を処理する場合のパフォーマンス観点）\n",
    "t = Tokenizer()\n",
    "\n",
    "\n",
    "def wakati(text: str) -> str:\n",
    "    \"\"\"\n",
    "    文字列 `text` を Janome で分かち書きし、半角スペース区切りで返す。\n",
    "\n",
    "    パラメータ:\n",
    "        text (str): 日本語テキスト（Wikipedia サマリーなど）\n",
    "    戻り値:\n",
    "        str: \"単語1 単語2 単語3 ...\" のようなスペース区切り列\n",
    "    備考:\n",
    "        - wakati=True を指定しているため、Token オブジェクトではなく表層形のイテラブルが得られる。\n",
    "        - 記号・数字・助詞なども出力に含まれる（Janome の標準挙動）。\n",
    "          下流で除去/正規化したい場合は別途前処理（例: 正規化、品詞フィルタ）を追加する。\n",
    "        - 未知語（固有名詞・新語）は辞書次第で分割精度が変わる点に留意。\n",
    "    \"\"\"\n",
    "    # Tokenizer.tokenize(..., wakati=True) は「表層形のイテレータ」を返す\n",
    "    # 例: \"徳川家康 は 江戸 幕府 を 開い た\" → ' ' で join して 1 本の文字列に\n",
    "    words_iter = t.tokenize(text, wakati=True)\n",
    "    return \" \".join(words_iter)\n",
    "\n",
    "\n",
    "# --- 分かち書きの実行 ---------------------------------------------------------\n",
    "# list*_w は直前ステップ（リスト 5.3.2）で取得した Wikipedia サマリー配列を想定\n",
    "# * _w: raw 文（sentence/string）のリスト\n",
    "# * _x: 分かち書き（tokenized）後の文のリスト\n",
    "list1_x = [wakati(w) for w in list1_w]  # 歴史カテゴリ\n",
    "list2_x = [wakati(w) for w in list2_w]  # 地理カテゴリ\n",
    "list3_x = [wakati(w) for w in list3_w]  # テスト用（評価/可視化など）\n",
    "\n",
    "# 学習・評価で一括処理したいケースに備えてマージ\n",
    "list_all_x = list1_x + list2_x + list3_x\n",
    "\n",
    "# --- 参考: 以降の一般的な流れ（ここでは実装しない） --------------------------\n",
    "# 1) 正規化（NFKC）、英数字の統一、記号除去など（必要に応じて）\n",
    "# 2) 語彙化（Vocabulary 構築）→ 単語ID列に変換\n",
    "# 3) 固定長化（MAX_LEN へのパディング/トランケーション）\n",
    "# 4) 埋め込み層 or 事前学習ベクトルの読み込み（Word2Vec/BERT トークナイザ等）\n",
    "# 5) 学習（分類/系列ラベリング/類似度計算など）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4379d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "総単語数:  1465\n",
      "変換前テキスト:  織田   信長 （ おだ   のぶ な が ） は 、 日本 の 戦国 時代 から 安土 桃山 時代 にかけて の 武将 ・ 大名 。 戦国 の 三 英傑 の 一 人 。 \n",
      " 尾張 国 （ 現在 の 愛知 県 ） 出身 。 織田 信秀 の 嫡男 。 家督 争い の 混乱 を 収め た 後 に 、 桶 狭間 の 戦い で 今川 義元 を 討ち取り 、 勢力 を 拡大 し た 。 足利 義昭 を 奉じ て 上洛 し 、 後 に は 義昭 を 追放 する こと で 、 畿内 を 中心 に 独自 の 中央 政権 （ 「 織田 政権 」 ） を 確立 し て 天下 人 と なっ た 。 しかし 、 天正 10 年 6 月 2 日 （ 1582 年 6 月 21 日 ） 、 家臣 ・ 明智 光秀 に 謀反 を 起こさ れ 、 本能寺 で 自害 し た 。 \n",
      " これ まで 信長 の 政権 は 、 豊臣 秀吉 による 豊臣 政権 、 徳川 家康 が 開い た 江戸 幕府 へ の 流れ を つくっ た 画期的 な もの で 、 その 政治 手法 も 革新 的 な もの で ある と みなさ れ て き た 。 しかし 、 近年 の 歴史 学界 で は その 政策 の 前 時代 性 が 指摘 さ れる よう に なり 、 しばしば 「 中世 社会 の 最終 段階 」 と も 評さ れ 、 その 革新 性 を 否定 する 研究 が 主流 と なっ て いる 。\n",
      "変換後:  [62, 103, 8, 459, 333, 30, 14, 7, 4, 2, 23, 1, 51, 10, 33, 54, 32, 10, 553, 1, 76, 13, 56, 3, 51, 1, 105, 644, 1, 161, 95, 3, 356, 36, 8, 351, 1, 205, 15, 7, 1423, 3, 62, 1424, 1, 320, 3, 558, 1425, 1, 1426, 6, 1427, 11, 75, 5, 2, 564, 565, 1, 342, 9, 204, 353, 6, 1428, 2, 359, 6, 254, 19, 11, 3, 72, 645, 6, 1429, 16, 1430, 19, 2, 75, 5, 4, 645, 6, 517, 20, 37, 9, 2, 211, 6, 93, 5, 1431, 1, 164, 68, 8, 18, 62, 68, 17, 7, 6, 284, 19, 16, 398, 95, 12, 74, 11, 3, 646, 2, 1432, 374, 26, 108, 85, 67, 96, 8, 1433, 26, 108, 85, 627, 96, 7, 2, 1434, 13, 1435, 1436, 5, 1437, 6, 1438, 27, 2, 1439, 9, 1440, 19, 11, 3, 225, 98, 103, 1, 68, 4, 2, 73, 137, 152, 73, 68, 2, 140, 535, 14, 1441, 11, 119, 31, 153, 1, 1442, 6, 1443, 11, 1444, 30, 233, 9, 2, 86, 112, 1445, 25, 647, 43, 30, 233, 9, 22, 12, 1446, 27, 16, 273, 11, 3, 646, 2, 426, 1, 44, 305, 9, 4, 86, 1447, 1, 441, 10, 245, 14, 1448, 29, 50, 117, 5, 114, 2, 1449, 18, 113, 648, 1, 246, 1450, 17, 12, 25, 1451, 27, 2, 86, 647, 245, 6, 1452, 20, 212, 14, 1453, 12, 74, 16, 60, 3]\n",
      "パディング後:  [   3  646    2  426    1   44  305    9    4   86 1447    1  441   10\n",
      "  245   14 1448   29   50  117    5  114    2 1449   18  113  648    1\n",
      "  246 1450   17   12   25 1451   27    2   86  647  245    6 1452   20\n",
      "  212   14 1453   12   74   16   60    3]\n",
      "正解データ(学習用):  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1]\n",
      "正解データ(検証用):  [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# リスト 5.3.4 学習データ作成（説明コメント付き）\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenizer:\n",
    "# - テキストを語彙辞書（単語→整数ID）に変換し、整数ID列（シーケンス）へ数値化するユーティリティ。\n",
    "# - 既定では「出現頻度の高い順」に 1, 2, 3, ... の ID が割り当てられる（0 はパディング用に予約しない設計）。\n",
    "# - 実務では oov_token（未知語トークン）を指定しておくと、学習時に未観測な語にも頑健になる。\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 学習・検証で使う全テキストを引数にして辞書を作成する\n",
    "# 注意（データリーク）:\n",
    "# - ここでは list_all_x（学習+検証+テスト相当）をまとめて fit しているため、\n",
    "#   厳密には検証・テスト側の語彙情報が学習側に漏れる（軽微だが情報リーク）。\n",
    "# - 再現実験や教材としては簡潔だが、厳密評価では「学習コーパスのみで fit」→\n",
    "#   「検証/テストは texts_to_sequences のみ」を推奨。\n",
    "tokenizer.fit_on_texts(list_all_x)\n",
    "\n",
    "# 単語一覧（語→ID の辞書）を取得\n",
    "# 例: {'江戸時代': 1, 'に': 2, 'は': 3, ...}\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# 総単語数（ユニーク語彙数）を取得\n",
    "num_words = len(word_index)\n",
    "print(\"総単語数: \", num_words)\n",
    "\n",
    "# 変換前の検証用テキスト確認（分かち書き済みの文字列）\n",
    "# - list3_x はテスト用タイトルの Wikipedia サマリーを分かち書きした配列（前工程参照）。\n",
    "print(\"変換前テキスト: \", list3_x[0])\n",
    "\n",
    "# テキストの数値化:\n",
    "# - texts_to_sequences は、各テキストを語彙辞書に基づいて「整数IDのリスト」へ変換する。\n",
    "# - 未知語は既定では無視され、ID 化されない（→ シーケンスから脱落）。\n",
    "#   未知語を保持したい場合は Tokenizer(oov_token='[UNK]') などを利用。\n",
    "sequence_test = tokenizer.texts_to_sequences(list3_x)\n",
    "\n",
    "# 変換結果確認（例: [12, 345, 78, ...]）\n",
    "print(\"変換後: \", sequence_test[0])\n",
    "\n",
    "# 単語のパディング:\n",
    "# - ニューラルネットに固定長テンソルを与えるため、pad_sequences で\n",
    "#   「短い文は 0 で埋める」「長い文は途中で切り詰める」処理を行う。\n",
    "# - 既定では先頭側にパディング（padding='pre'）、長い場合は先頭側を切る（truncating='pre'）。\n",
    "#   LSTM の方向やモデル設計に応じて 'post' に変えることも多い。\n",
    "sequence_test = pad_sequences(sequence_test, maxlen=MAX_LEN)\n",
    "\n",
    "# パディング後の確認（長さが MAX_LEN の固定長ベクトルに）\n",
    "print(\"パディング後: \", sequence_test[0])\n",
    "\n",
    "# 学習データ（歴史: list1_x、地理: list2_x）に対しても同じ数値化→パディングを適用\n",
    "# - fit はすでに済んでいるため、ここでは texts_to_sequences + pad_sequences のみ。\n",
    "sequence_train = tokenizer.texts_to_sequences(list1_x + list2_x)\n",
    "sequence_train = pad_sequences(sequence_train, maxlen=MAX_LEN)\n",
    "\n",
    "# 正解ラベルの作成:\n",
    "# - 2 クラス分類を想定し、歴史カテゴリを 0、地理カテゴリを 1 として付与。\n",
    "# - 学習用 y は学習データの件数に合わせて 0/1 を連結。\n",
    "# - 検証/テスト用 y は list3_x の内容（例: ['織田信長','豊臣秀吉','青森県','北海道']）に合わせて\n",
    "#   先頭 2 件を歴史=0、後ろ 2 件を地理=1 として作成（デモ簡略化のための固定割当）。\n",
    "Y_train = np.array([0] * len(list1_x) + [1] * len(list2_x))\n",
    "Y_test = np.array([0] * 2 + [1] * 2)\n",
    "\n",
    "# ラベル配列の確認\n",
    "print(\"正解データ(学習用): \", Y_train)\n",
    "print(\"正解データ(検証用): \", Y_test)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 補足（実務上のベストプラクティス）:\n",
    "# - 語彙構築は学習データのみに限定（データリーク回避）。\n",
    "# - トークン頻度に基づく語彙上限 num_words を設定し、希少語をまとめて OOV へ。\n",
    "# - pad_sequences の padding/truncating はモデル特性に合わせて 'post' を検討。\n",
    "# - ラベルは One-Hot ではなく整数ラベルのままでも SparseCategoricalCrossentropy で学習可。\n",
    "# - クラス不均衡がある場合は class_weight やサンプリングで補正。\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e462e25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
