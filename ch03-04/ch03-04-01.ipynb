{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b35f824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.4.1 Wikipediaの日本百名湯記事をTF-IDFで分析（説明コメント付き）\n",
    "# --------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・以降の TF-IDF 分析の入力となる「Wikipedia 上に記事が存在する日本百名湯」の記事タイトル一覧。\n",
    "#   ・この配列 spa_list をもとに、各記事の本文を収集→前処理→ベクトル化（TF-IDF）する。\n",
    "#\n",
    "# データ仕様/前提:\n",
    "#   ・型: List[str]（純粋な記事タイトルの列）\n",
    "#   ・表記: Wikipedia の**ページタイトル**に一致する表記を採用（例: 地名の重複は括弧つきで曖昧性解消）。\n",
    "#       - 例: 「朝日温泉 (北海道)」「玉川温泉 (秋田県)」「二日市温泉 (筑紫野市)」\n",
    "#   ・同義/別表記は**ここでは扱わない**（語彙統制は後段処理で対応）。タイトルは原則、Wikipedia 側の公式見出しを使用。\n",
    "#\n",
    "# 取得時の実務メモ:\n",
    "#   ・Python の wikipedia / wikipediaapi ライブラリで記事本文を取得する際は、\n",
    "#     `auto_suggest=False` を付けて**表記揺れで別ページに飛ばないように**するのが安全。\n",
    "#   ・括弧つきタイトルは**完全一致**させないと別名・曖昧さ回避ページに誘導される可能性がある。\n",
    "#   ・ページが存在しない/移動済みの場合は 404/Redirect/Disambiguation をハンドリングすること。\n",
    "#\n",
    "# 後工程（TF-IDF）での注意:\n",
    "#   ・日本語はスペースで分かち書きされないため、ベクトル化には\n",
    "#       (A) 形態素解析（MeCab/Janome + TfidfVectorizer(tokenizer=...)）\n",
    "#       (B) 文字 n-gram（例: analyzer=\"char_wb\", ngram_range=(2,4)）\n",
    "#     のいずれかを選ぶ。学術用途は (A) 推奨だがセットアップ容易性は (B) が高い。\n",
    "#   ・本文取得後は、注記/脚注/テンプレ/目次の除去、記号正規化（NFKC）、全半角統一などの前処理が精度に寄与。\n",
    "#\n",
    "# 品質チェックのヒント（必要に応じて実行）:\n",
    "#   # 重複検出: assert len(spa_list) == len(set(spa_list))\n",
    "#   # 存在確認: 404/曖昧さ回避を検知し、欠損リストを別途保存しておくと再現性が上がる。\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# 日本百名湯のうち、Wikipedia に記事のある温泉のリスト（記事タイトル完全一致）\n",
    "spa_list = [\n",
    "    \"菅野温泉\",\n",
    "    \"養老牛温泉\",\n",
    "    \"定山渓温泉\",\n",
    "    \"登別温泉\",\n",
    "    \"洞爺湖温泉\",\n",
    "    \"ニセコ温泉郷\",\n",
    "    \"朝日温泉 (北海道)\",\n",
    "    \"酸ヶ湯温泉\",\n",
    "    \"蔦温泉\",\n",
    "    \"花巻南温泉峡\",\n",
    "    \"夏油温泉\",\n",
    "    \"須川高原温泉\",\n",
    "    \"鳴子温泉郷\",\n",
    "    \"遠刈田温泉\",\n",
    "    \"峩々温泉\",\n",
    "    \"乳頭温泉郷\",\n",
    "    \"後生掛温泉\",\n",
    "    \"玉川温泉 (秋田県)\",\n",
    "    \"秋ノ宮温泉郷\",\n",
    "    \"銀山温泉\",\n",
    "    \"瀬見温泉\",\n",
    "    \"赤倉温泉 (山形県)\",\n",
    "    \"東山温泉\",\n",
    "    \"飯坂温泉\",\n",
    "    \"二岐温泉\",\n",
    "    \"那須温泉郷\",\n",
    "    \"塩原温泉郷\",\n",
    "    \"鬼怒川温泉\",\n",
    "    \"奥鬼怒温泉郷\",\n",
    "    \"草津温泉\",\n",
    "    \"伊香保温泉\",\n",
    "    \"四万温泉\",\n",
    "    \"法師温泉\",\n",
    "    \"箱根温泉\",\n",
    "    \"湯河原温泉\",\n",
    "    \"越後湯沢温泉\",\n",
    "    \"松之山温泉\",\n",
    "    \"大牧温泉\",\n",
    "    \"山中温泉\",\n",
    "    \"山代温泉\",\n",
    "    \"粟津温泉\",\n",
    "    \"奈良田温泉\",\n",
    "    \"西山温泉 (山梨県)\",\n",
    "    \"野沢温泉\",\n",
    "    \"湯田中温泉\",\n",
    "    \"別所温泉\",\n",
    "    \"中房温泉\",\n",
    "    \"白骨温泉\",\n",
    "    \"小谷温泉\",\n",
    "    \"下呂温泉\",\n",
    "    \"福地温泉\",\n",
    "    \"熱海温泉\",\n",
    "    \"伊東温泉\",\n",
    "    \"修善寺温泉\",\n",
    "    \"湯谷温泉 (愛知県)\",\n",
    "    \"榊原温泉\",\n",
    "    \"木津温泉\",\n",
    "    \"有馬温泉\",\n",
    "    \"城崎温泉\",\n",
    "    \"湯村温泉 (兵庫県)\",\n",
    "    \"十津川温泉\",\n",
    "    \"南紀白浜温泉\",\n",
    "    \"南紀勝浦温泉\",\n",
    "    \"湯の峰温泉\",\n",
    "    \"龍神温泉\",\n",
    "    \"奥津温泉\",\n",
    "    \"湯原温泉\",\n",
    "    \"三朝温泉\",\n",
    "    \"岩井温泉\",\n",
    "    \"関金温泉\",\n",
    "    \"玉造温泉\",\n",
    "    \"有福温泉\",\n",
    "    \"温泉津温泉\",\n",
    "    \"湯田温泉\",\n",
    "    \"長門湯本温泉\",\n",
    "    \"祖谷温泉\",\n",
    "    \"道後温泉\",\n",
    "    \"二日市温泉 (筑紫野市)\",\n",
    "    \"嬉野温泉\",\n",
    "    \"武雄温泉\",\n",
    "    \"雲仙温泉\",\n",
    "    \"小浜温泉\",\n",
    "    \"黒川温泉\",\n",
    "    \"地獄温泉\",\n",
    "    \"垂玉温泉\",\n",
    "    \"杖立温泉\",\n",
    "    \"日奈久温泉\",\n",
    "    \"鉄輪温泉\",\n",
    "    \"明礬温泉\",\n",
    "    \"由布院温泉\",\n",
    "    \"川底温泉\",\n",
    "    \"長湯温泉\",\n",
    "    \"京町温泉\",\n",
    "    \"指宿温泉\",\n",
    "    \"霧島温泉郷\",\n",
    "    \"新川渓谷温泉郷\",\n",
    "    \"栗野岳温泉\",\n",
    "]\n",
    "\n",
    "# （参考）後続処理の典型フロー（ここでは実行しない・骨子のみ）\n",
    "# from wikipedia import summary, set_lang\n",
    "# set_lang(\"ja\")\n",
    "# texts = []\n",
    "# for title in spa_list:\n",
    "#     try:\n",
    "#         texts.append(summary(title, auto_suggest=False))\n",
    "#     except Exception as e:\n",
    "#         # DisambiguationError/PageError を拾って要確認リストへ回す\n",
    "#         pass\n",
    "# # 形態素解析 or 文字 n-gram 前処理 → TfidfVectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12724fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "菅野温泉\n",
      "養老牛温泉\n",
      "定山渓温泉\n",
      "登別温泉\n",
      "洞爺湖温泉\n",
      "ニセコ温泉郷\n",
      "朝日温泉 (北海道)\n",
      "酸ヶ湯温泉\n",
      "蔦温泉\n",
      "花巻南温泉峡\n",
      "夏油温泉\n",
      "須川高原温泉\n",
      "鳴子温泉郷\n",
      "遠刈田温泉\n",
      "峩々温泉\n",
      "乳頭温泉郷\n",
      "後生掛温泉\n",
      "玉川温泉 (秋田県)\n",
      "秋ノ宮温泉郷\n",
      "銀山温泉\n",
      "瀬見温泉\n",
      "赤倉温泉 (山形県)\n",
      "東山温泉\n",
      "飯坂温泉\n",
      "二岐温泉\n",
      "那須温泉郷\n",
      "塩原温泉郷\n",
      "鬼怒川温泉\n",
      "奥鬼怒温泉郷\n",
      "草津温泉\n",
      "伊香保温泉\n",
      "四万温泉\n",
      "法師温泉\n",
      "箱根温泉\n",
      "湯河原温泉\n",
      "越後湯沢温泉\n",
      "松之山温泉\n",
      "大牧温泉\n",
      "山中温泉\n",
      "山代温泉\n",
      "粟津温泉\n",
      "奈良田温泉\n",
      "西山温泉 (山梨県)\n",
      "野沢温泉\n",
      "湯田中温泉\n",
      "別所温泉\n",
      "中房温泉\n",
      "白骨温泉\n",
      "小谷温泉\n",
      "下呂温泉\n",
      "福地温泉\n",
      "熱海温泉\n",
      "伊東温泉\n",
      "修善寺温泉\n",
      "湯谷温泉 (愛知県)\n",
      "榊原温泉\n",
      "木津温泉\n",
      "有馬温泉\n",
      "城崎温泉\n",
      "湯村温泉 (兵庫県)\n",
      "十津川温泉\n",
      "南紀白浜温泉\n",
      "南紀勝浦温泉\n",
      "湯の峰温泉\n",
      "龍神温泉\n",
      "奥津温泉\n",
      "湯原温泉\n",
      "三朝温泉\n",
      "岩井温泉\n",
      "関金温泉\n",
      "玉造温泉\n",
      "有福温泉\n",
      "温泉津温泉\n",
      "湯田温泉\n",
      "長門湯本温泉\n",
      "祖谷温泉\n",
      "道後温泉\n",
      "二日市温泉 (筑紫野市)\n",
      "嬉野温泉\n",
      "武雄温泉\n",
      "雲仙温泉\n",
      "小浜温泉\n",
      "黒川温泉\n",
      "地獄温泉\n",
      "垂玉温泉\n",
      "杖立温泉\n",
      "日奈久温泉\n",
      "鉄輪温泉\n",
      "明礬温泉\n",
      "由布院温泉\n",
      "川底温泉\n",
      "長湯温泉\n",
      "京町温泉\n",
      "指宿温泉\n",
      "霧島温泉郷\n",
      "新川渓谷温泉郷\n",
      "栗野岳温泉\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Wikipediaの記事の読み取り（日本百名湯）— 説明コメント付き\n",
    "# 目的:\n",
    "#   - spa_list に含まれる各温泉の Wikipedia 記事本文（.content）を取得し、content_list に格納する。\n",
    "# 前提:\n",
    "#   - 2.1節で定義済みの spa_list（ページタイトルの配列）が存在すること。\n",
    "#   - ネットワーク環境があること（wikipedia パッケージはオンラインで Wikipedia にアクセスする）。\n",
    "# 使用ライブラリ:\n",
    "#   - wikipedia（PyPI: wikipedia）：MediaWiki API を叩く薄いラッパ。簡便だが曖昧さ処理の例外が発生しやすい。\n",
    "# 設計メモ（理論）:\n",
    "#   - set_lang(\"ja\") により検索対象を日本語版 Wikipedia に固定する（多言語ページの誤ヒットを避ける）。\n",
    "#   - page(title, auto_suggest=False) は「完全一致」志向。自動補完で別ページに飛ばない利点がある一方、\n",
    "#     タイトルの誤記や表記揺れがあると PageError/DisambiguationError を起こしやすい。\n",
    "#   - .content はページ本文のテキスト（マークアップ除去済みのことが多いが、節タイトル等は含まれる）。\n",
    "#   - 収集後は前処理（正規化・ノイズ除去）→ ベクトル化（例: TF-IDF）へ進む想定。\n",
    "# 例外と実務上の注意:\n",
    "#   - wikipedia.exceptions.DisambiguationError：曖昧さ回避ページに当たった場合に発生（例: 同名温泉が複数）。\n",
    "#   - wikipedia.exceptions.PageError：タイトルに対応するページが存在しない場合に発生。\n",
    "#   - 連続アクセスが多い場合、礼儀として待機を入れる（time.sleep）。API 側レート制限にも配慮。\n",
    "#   - 取得漏れを把握するために、成功/失敗のタイトルを別リストに蓄積すると再現性が上がる。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# wikipedia パッケージの読み込みと日本語版の指定\n",
    "import wikipedia\n",
    "\n",
    "wikipedia.set_lang(\"ja\")  # 以降の page/summary は日本語版 Wikipedia を対象にする\n",
    "\n",
    "# 取得した本文を格納するリスト\n",
    "content_list = []\n",
    "\n",
    "# 各温泉タイトルに対して Wikipedia ページ本文を取得\n",
    "for spa in spa_list:\n",
    "    print(spa)  # 進捗ログ: 現在処理中のページタイトルを表示\n",
    "    # auto_suggest=False:\n",
    "    #   - 自動補完を無効化して、与えたタイトルにできるだけ厳密に一致するページを取得する。\n",
    "    #   - 曖昧さ回避や誤補完による誤取得を避ける狙い。\n",
    "    content = wikipedia.page(spa, auto_suggest=False).content\n",
    "    content_list.append(content)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# （発展: 例外処理・レート制御を入れたい場合の雛形。必要に応じて使用）\n",
    "# import time\n",
    "# from wikipedia.exceptions import DisambiguationError, PageError\n",
    "# content_list, failed = [], []\n",
    "# for spa in spa_list:\n",
    "#     try:\n",
    "#         print(spa)\n",
    "#         txt = wikipedia.page(spa, auto_suggest=False).content\n",
    "#         content_list.append(txt)\n",
    "#         time.sleep(0.5)  # マナーとしてのウェイト（環境に応じて調整）\n",
    "#     except DisambiguationError as e:\n",
    "#         # e.options に候補リストが入る。必要なら手動で最適候補を選ぶ処理を追加。\n",
    "#         failed.append({\"title\": spa, \"reason\": \"disambiguation\", \"options\": e.options[:5]})\n",
    "#     except PageError:\n",
    "#         failed.append({\"title\": spa, \"reason\": \"page_not_found\"})\n",
    "# # 収集漏れ確認:\n",
    "# # print(f\"成功: {len(content_list)} 件 / 失敗: {len(failed)} 件\"); print(failed)\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf8e383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 形態素解析\n",
    "# 2.2節参照\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Tokenizer インスタンスの生成\n",
    "# - Janome は純粋 Python 実装の日本語形態素解析器（追加インストールのみで動作）\n",
    "# - 毎回生成するとコストが高いため、モジュールレベルで 1 度だけ生成して使い回す\n",
    "t = Tokenizer()\n",
    "\n",
    "\n",
    "# 形態素解析関数の定義\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    与えられた日本語テキストを Janome で形態素解析し、\n",
    "    「名詞」および「形容詞」に該当する語の原形（base_form）のみを配列で返す。\n",
    "\n",
    "    返り値:\n",
    "        List[str]: 名詞・形容詞の原形のみからなるトークン列\n",
    "\n",
    "    実装メモ:\n",
    "    - token.part_of_speech は「品詞,品詞細分類1,品詞細分類2,品詞細分類3」のカンマ区切り文字列。\n",
    "      先頭要素（品詞大分類）が '名詞' または '形容詞' のものだけを採用する。\n",
    "    - token.base_form は語の原形。名詞は多くの場合 surface と同一、形容詞は原形化される（例: 「高かった」→「高い」）。\n",
    "    - 未知語では base_form が '*' となる場合がある（この関数ではそのまま返る点に注意）。\n",
    "      必要なら '*' を surface で置き換える処理を追加してもよい（下に例をコメントで記載）。\n",
    "    - 動詞も分析対象に含めたい場合は、条件の配列に '動詞' を追加するだけでよい。\n",
    "    \"\"\"\n",
    "    return [\n",
    "        token.base_form\n",
    "        for token in t.tokenize(text)\n",
    "        # 品詞大分類（先頭ラベル）でフィルタ\n",
    "        if token.part_of_speech.split(\",\")[0] in [\"名詞\", \"形容詞\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "# --- 拡張の例（必要なら使用） -----------------------------------------------\n",
    "# def tokenize(text):\n",
    "#     tokens = []\n",
    "#     for tok in t.tokenize(text):\n",
    "#         pos = tok.part_of_speech.split(',')[0]\n",
    "#         if pos in ['名詞', '形容詞']:\n",
    "#             base = tok.base_form if tok.base_form != '*' else tok.surface  # 未知語を表層形にフォールバック\n",
    "#             tokens.append(base)\n",
    "#     return tokens\n",
    "# ------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49bddb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia記事を名詞と形容詞のみとし、ブランクで分かち書き\n",
    "# 2.2節参照（tokenize: 名詞・形容詞のみを原形で返す関数を使用）\n",
    "\n",
    "# 目的:\n",
    "#   ・各 Wikipedia 記事本文（content）から、解析対象語（名詞・形容詞）のみを抽出し、\n",
    "#     空白区切り（スペース区切り）の 1 行テキストへ変換してコーパス words_list に格納する。\n",
    "#   ・以降の TF-IDF（または CountVectorizer）に、そのまま渡せる形を用意する。\n",
    "#\n",
    "# 理論/設計メモ:\n",
    "#   ・ベクトル化器（TfidfVectorizer など）がデフォルトで「空白区切り」を単語境界とみなす前提で整形。\n",
    "#   ・日本語は本来スペースで分かち書きされないため、先に形態素解析で単語列にしてから ' '.join(...) する。\n",
    "#   ・Wikipedia の heading マーク「== 見出し ==」に含まれる \"==\" は、そもそも品詞が記号のため\n",
    "#     tokenize（名詞/形容詞抽出）では落ちるが、念のため '==' を除去する処理を残している。\n",
    "#     （厳密に見出し行全体を落とすには、事前に正規表現で `==...==` の行を削除するのが堅牢）\n",
    "#\n",
    "# 実務上の注意:\n",
    "#   ・content が None/空文字のケースや、未知語が base_form='*' となるケースに留意。\n",
    "#     必要なら '*' を surface へフォールバックする tokenize 拡張を検討。\n",
    "#   ・後続の TF-IDF で記号・数字を落としたい場合は、ここで正規化/置換を行うと一貫性が出る。\n",
    "#   ・spa_list と content_list のインデックス対応が崩れないように、例外処理を入れる際は\n",
    "#     失敗要素を別途記録する（タイトルと一緒に）など再現性の担保を。\n",
    "#\n",
    "# 例の拡張（必要に応じて使用・下にコメントで雛形あり）:\n",
    "#   - 正規化: NFKC、全/半角統一、改行→スペース、連続空白の縮約\n",
    "#   - heading 行の完全除去: re.sub(r\"^==+.*?==+$\", \"\", text, flags=re.MULTILINE)\n",
    "#   - 記号/数字の除去: re.sub(r\"[0-9０-９]+\", \"0\", text) など\n",
    "\n",
    "words_list = []\n",
    "for content in content_list:\n",
    "    # 1) 形態素解析で名詞・形容詞の原形シーケンスに変換\n",
    "    tokens = tokenize(content)  # 例: [\"温泉\",\"湧出\",\"豊富\",\"名所\", ...]\n",
    "    # 2) 空白区切りの 1 行テキストへ変換（ベクトル化器に素直に渡せる形）\n",
    "    words = \" \".join(tokens)\n",
    "    # 3) 見出しマークの保険的な除去（多くの場合、tokens に \"==\" は含まれないため冗長だが無害）\n",
    "    words = words.replace(\"==\", \"\")\n",
    "    # 4) 必要なら追加の軽量正規化（任意・例）\n",
    "    #    - 連続空白の縮約:\n",
    "    # import re\n",
    "    # words = re.sub(r\"\\s+\", \" \", words).strip()\n",
    "    #    - 数字の正規化（全部 0 へ統一など）:\n",
    "    # words = re.sub(r\"[0-9０-９]+\", \"0\", words)\n",
    "\n",
    "    words_list.append(words)\n",
    "\n",
    "# --- 参考: 見出し行を事前に丸ごと落としたい場合の雛形（tokenize 前に適用） ------------\n",
    "# import re\n",
    "# words_list = []\n",
    "# for content in content_list:\n",
    "#     cleaned = re.sub(r\"^==+.*?==+$\", \"\", content, flags=re.MULTILINE)  # 見出し行の除去\n",
    "#     tokens = tokenize(cleaned)\n",
    "#     words = ' '.join(tokens)\n",
    "#     words_list.append(re.sub(r\"\\s+\", \" \", words).strip())\n",
    "# ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e85dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.4.2\n",
    "# TF-IDF分析の実施（詳細コメント付き）\n",
    "\n",
    "# ライブラリのインポート\n",
    "# - TfidfVectorizer: 文書集合から TF-IDF（用語頻度×逆文書頻度）によるベクトル表現を生成する。\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ベクトライザの初期化\n",
    "# -----------------------------------------------------------------------------\n",
    "# 前処理前提:\n",
    "#   ・すでに words_list は「形態素解析済みのトークンを空白で連結した文字列」の配列である\n",
    "#     （例: \"温泉 豊富 名所 ...\"}）。\n",
    "# 設計ポイント:\n",
    "#   ・token_pattern の既定は r'(?u)\\b\\w\\w+\\b'（= 2 文字以上のトークンのみ有効）。\n",
    "#     日本語では 1 文字名詞（例: 「湯」「滝」など）も扱いたい場合があるため、\n",
    "#     ここでは 1 文字以上を許す r'(?u)\\b\\w+\\b' に変更している。\n",
    "#   ・min_df=1: 1 文書にしか出ない語も特徴に残す（学習用の可観測性重視）。\n",
    "#   ・max_df=50: 51 文書以上に出現する汎用語を落とす（df > 50 を切る上限）。\n",
    "#     ※ 文書数（= len(words_list)）に応じて適宜調整。割合指定（例: max_df=0.5）も可。\n",
    "#   ・norm='l2': ベクトルを L2 正規化（各文書ベクトルの長さを 1 に）→ 文書長の影響を抑える。\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",  # 1 文字語も採用（既定は 2 文字以上）\n",
    "    min_df=1,\n",
    "    max_df=50,\n",
    "    norm=\"l2\",\n",
    "    use_idf=True,  # 逆文書頻度を掛ける\n",
    "    smooth_idf=True,  # idf のスムージング（ゼロ回避）\n",
    "    sublinear_tf=False,  # TF のサブリニア（log(1+tf)）はここでは無効（必要に応じて True）\n",
    ")\n",
    "\n",
    "# フィーチャーベクトルの生成（疎行列; shape = [文書数, 語彙数]）\n",
    "# - fit: 語彙（ボキャブラリ）を学習\n",
    "# - transform: TF-IDF を計算\n",
    "# - fit_transform: 上記を一括実行\n",
    "features = vectorizer.fit_transform(words_list)\n",
    "\n",
    "# 特徴語（語彙リスト）の抽出\n",
    "# - scikit-learn >= 1.0: get_feature_names_out()\n",
    "# - 旧バージョン互換のため try/except で両対応\n",
    "try:\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "except AttributeError:\n",
    "    terms = vectorizer.get_feature_names()\n",
    "\n",
    "# 疎行列を TF-IDF の密行列（numpy.ndarray）へ変換\n",
    "# - 大規模コーパスではメモリ使用量が跳ね上がるため、解析・可視化などの必要時のみ実施する。\n",
    "tfidfs = features.toarray()\n",
    "\n",
    "# 参考: 形状や一部の確認（必要ならコメント解除）\n",
    "# print(f\"docs={features.shape[0]}, vocab={features.shape[1]}\")\n",
    "# print(\"sample terms:\", terms[:20])\n",
    "# print(\"TF-IDF[0, :5]:\", tfidfs[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd4b65e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【菅野温泉】\n",
      "然別 菅野 かん 峡 再開 食塩 重曹 湯舟 鹿追 営業\n",
      "【養老牛温泉】\n",
      "養老牛 坂本 裏 標津 開業 まつ 西村 アイヌ 藤 堀口\n",
      "【定山渓温泉】\n",
      "定山渓 かっぱ 札幌 山 橋 淵 小樽 完成 定 北海道\n",
      "【登別温泉】\n",
      "登別 登別温泉 地獄谷 地獄 北海道 大湯沼 滝本 岡田 大正 日和山\n",
      "【洞爺湖温泉】\n",
      "洞爺湖温泉 洞爺湖 洞爺 虻田 有珠山 壮瞥 北海道 湖 とうや ジオパーク\n",
      "【ニセコ温泉郷】\n",
      "ニセコ パス 名人 温泉郷 スタンプ 蘭越 ニセコアンヌプリ 倶知安 贈呈 北海道\n",
      "【朝日温泉 (北海道)】\n",
      "岩内 朝日 土砂 災害 休業 雷電 ユウ ナイ川 内川 2010\n",
      "【酸ヶ湯温泉】\n",
      "八甲田山 熱 青森 植物 八甲田 千 気候 時 混浴 午前\n",
      "【蔦温泉】\n",
      "蔦 十和田 沼 猪木 野鳥 コース 森 要塞 アントニオ 東北\n",
      "【花巻南温泉峡】\n",
      "花巻 鉛 峡 花巻温泉 豊沢川 はな 戸平 東和 松倉 金矢\n"
     ]
    }
   ],
   "source": [
    "# リスト 3.4.3 温泉毎の特徴語の表示（詳細コメント付き）\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_feature_words(terms, tfidfs, i, n):\n",
    "    \"\"\"\n",
    "    i 番目の文書（温泉記事）について、TF-IDF 値が高い上位 n 語を返す。\n",
    "\n",
    "    引数:\n",
    "        terms : array-like (語彙リスト; shape = [V])\n",
    "            TfidfVectorizer から得た語彙（特徴量名）。インデックスは列次元に対応。\n",
    "        tfidfs: np.ndarray (TF-IDF 行列; shape = [D, V])\n",
    "            fit_transform(...).toarray() 等で得た密行列（D=文書数, V=語彙数）。\n",
    "        i     : int\n",
    "            対象とする文書インデックス（0 <= i < D）。\n",
    "        n     : int\n",
    "            返す上位語数（n > V の場合は V に丸める）。\n",
    "\n",
    "    戻り値:\n",
    "        List[str]: 文書 i における TF-IDF が高い順の上位 n 語（語形は terms の表記）\n",
    "\n",
    "    実装方針 / 理論メモ:\n",
    "        - TF-IDF は「その文書で相対的に重要な語」を浮かび上がらせる指標。\n",
    "          * TF（Term Frequency）: 文書内頻度\n",
    "          * IDF（Inverse Document Frequency）: 文書間の一般性の低さ（希少性）\n",
    "        - 上位抽出は、語彙サイズ V が大きい場合に O(V log V) の完全ソートは重い。\n",
    "          ここでは np.argpartition を用いて O(V) で上位 n の候補を抽出し、\n",
    "          その部分だけ降順に並べ替えることで計算量を削減する。\n",
    "        - TF-IDF が同点の語の順序は未定義（語彙インデックス順になる可能性）。\n",
    "    \"\"\"\n",
    "    # 型/次元の基本チェック（学習コードの可観測性向上のため）\n",
    "    tfidfs = np.asarray(tfidfs)\n",
    "    D, V = tfidfs.shape\n",
    "    if not (0 <= i < D):\n",
    "        raise IndexError(f\"文書インデックス i={i} が範囲外です（0 <= i < {D}）。\")\n",
    "    if V == 0:\n",
    "        return []\n",
    "\n",
    "    # 要求語数を語彙数に丸める\n",
    "    n = int(n)\n",
    "    n = max(0, min(n, V))\n",
    "    if n == 0:\n",
    "        return []\n",
    "\n",
    "    # i 番目の文書ベクトルを取得\n",
    "    tfidf_row = tfidfs[i]\n",
    "\n",
    "    # 上位 n の候補インデックスを argpartition で取得（ここでは n 個の“最大要素”集合）\n",
    "    # 注意: argpartition は順序を保証しないため、後段で降順にソートし直す\n",
    "    candidate_idx = np.argpartition(tfidf_row, -n)[-n:]\n",
    "\n",
    "    # 候補を TF-IDF 降順に整列\n",
    "    sorted_local = candidate_idx[np.argsort(tfidf_row[candidate_idx])[::-1]]\n",
    "\n",
    "    # 語彙へマッピング\n",
    "    return [terms[idx] for idx in sorted_local]\n",
    "\n",
    "\n",
    "# --- 結果の出力 ---------------------------------------------------------------\n",
    "# 先頭 10 件の温泉について、上位 10 語を表示。\n",
    "# ・words_list / terms / tfidfs / spa_list が事前に定義されている前提（前リスト参照）。\n",
    "# ・インデックス対応の安全化のため、available_docs = min(10, len(spa_list), tfidfs.shape[0]) とする。\n",
    "available_docs = min(10, len(spa_list), tfidfs.shape[0])\n",
    "\n",
    "for i in range(available_docs):\n",
    "    print(\"【\" + spa_list[i] + \"】\")\n",
    "    top_words = extract_feature_words(terms, tfidfs, i, 10)\n",
    "    # 語のみを空白区切りで出力（元コード互換）\n",
    "    print(\" \".join(top_words))\n",
    "\n",
    "# --- 参考: スコア付きで見たい場合（必要時のみコメント解除） --------------------\n",
    "# def extract_feature_words_with_scores(terms, tfidfs, i, n):\n",
    "#     tfidfs = np.asarray(tfidfs)\n",
    "#     D, V = tfidfs.shape\n",
    "#     if not (0 <= i < D): raise IndexError(\"i out of range\")\n",
    "#     n = max(0, min(int(n), V))\n",
    "#     if n == 0: return []\n",
    "#     row = tfidfs[i]\n",
    "#     cand = np.argpartition(row, -n)[-n:]\n",
    "#     order = cand[np.argsort(row[cand])[::-1]]\n",
    "#     return [(terms[j], float(row[j])) for j in order]\n",
    "#\n",
    "# for i in range(available_docs):\n",
    "#     print('【' + spa_list[i] + '】')\n",
    "#     for w, s in extract_feature_words_with_scores(terms, tfidfs, i, 10):\n",
    "#         print(f\"{w}:{s:.3f}\", end=' ')\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
