{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticsearchインスタンスの生成\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea352e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.x 日本語用インデックスの登録（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・kuromoji + ICU を用いた日本語向けアナライザを Elasticsearch 側に定義し、\n",
    "#     検索時用（jpn-search）と索引用（jpn-index）の2系統で設定する最小例。\n",
    "#\n",
    "# 前提/注意:\n",
    "#   ・このコードは Python クライアント（elasticsearch-py）から ES に送る設定 JSON を組み立てて実行する。\n",
    "#   ・kuromoji/icu が使用可能な ES（プラグイン内蔵の公式ディストリ or Elastic Cloud 等）を想定。\n",
    "#   ・\"user_dictionary\": \"my_jisho.dic\" は **ESノード側のファイルパス**。ローカルPC上のパスではない点に注意。\n",
    "#     管理サービスではユーザー辞書ファイルの配置に制約がある（不可/代替手段が必要）ことが多い。\n",
    "#   ・同義語（synonyms_filter）は初期は空配列 → 実運用では再インデックス or エイリアス切替で更新する設計が安全。\n",
    "#   ・本スニペットは settings（analysis）のみ定義。**mappings で各 text フィールドへ\n",
    "#     analyzer/search_analyzer を割り当てない限り効果は出ない**（下部コメント参照）。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# インデックス作成用JSONの定義\n",
    "create_index = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            # ---------------------------\n",
    "            # Token Filter 定義セクション\n",
    "            # ---------------------------\n",
    "            \"filter\": {\n",
    "                \"synonyms_filter\": {  # 同義語フィルタ\n",
    "                    \"type\": \"synonym\",\n",
    "                    # \"synonyms\": [...] に「A, B」「C => D」等で定義。\n",
    "                    # 空配列の現状では no-op（効果なし）。運用開始後の更新は再作成/エイリアス切替が基本。\n",
    "                    \"synonyms\": [\n",
    "                        # 例: \"寿司, すし\", \"蕎麦, そば\", \"コンピューター, コンピュータ\"\n",
    "                    ],\n",
    "                }\n",
    "                # 例: 固有名詞保護が必要なら keyword_marker を追加し、stemmer より前段に置く\n",
    "                # \"ja_protected\": {\n",
    "                #     \"type\": \"keyword_marker\",\n",
    "                #     \"protected_words\": [\"メーラー\",\"プレイヤー\"]  # 長音削減の対象から除外したい語\n",
    "                # }\n",
    "            },\n",
    "            # ----------------------------\n",
    "            # Tokenizer 定義セクション\n",
    "            # ----------------------------\n",
    "            \"tokenizer\": {\n",
    "                \"kuromoji_w_dic\": {  # ユーザー辞書付き kuromoji トークナイザ\n",
    "                    \"type\": \"kuromoji_tokenizer\",  # ← ベースは kuromoji_tokenizer（綴り注意）\n",
    "                    # ユーザー辞書（*.dic = ビルド済み辞書）を追加。パスは ES ノード側の参照範囲。\n",
    "                    \"user_dictionary\": \"my_jisho.dic\",\n",
    "                }\n",
    "            },\n",
    "            # --------------------------\n",
    "            # Analyzer 定義セクション\n",
    "            # --------------------------\n",
    "            \"analyzer\": {\n",
    "                # 検索用（想起重視: synonyms を含める）\n",
    "                \"jpn-search\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\n",
    "                        \"icu_normalizer\",  # 文字単位の正規化（NFKC等：全角/半角・互換文字の揺れ吸収）\n",
    "                        \"kuromoji_iteration_mark\",  # 繰り返し記号（々/ゝ/ヽ 等）の展開\n",
    "                    ],\n",
    "                    \"tokenizer\": \"kuromoji_w_dic\",  # ユーザー辞書込みの形態素解析\n",
    "                    \"filter\": [\n",
    "                        \"synonyms_filter\",  # 同義語展開（想起向上）。定義が空なら実質無効\n",
    "                        \"kuromoji_baseform\",  # 活用語の原形化（食べた→食べる）\n",
    "                        \"kuromoji_part_of_speech\",  # 不要品詞の除去（助詞/助動詞など）※ stoptags 既定に依存\n",
    "                        \"ja_stop\",  # 日本語ストップワードの除去\n",
    "                        \"kuromoji_number\",  # 数表現の正規化（漢数字→算用数字等）\n",
    "                        \"kuromoji_stemmer\",  # カタカナ語の長音正規化（コンピューター→コンピュータ）\n",
    "                    ],\n",
    "                },\n",
    "                # 索引用（精度重視: 通常は synonyms を含めない）\n",
    "                \"jpn-index\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\"icu_normalizer\", \"kuromoji_iteration_mark\"],\n",
    "                    \"tokenizer\": \"kuromoji_w_dic\",\n",
    "                    \"filter\": [\n",
    "                        \"kuromoji_baseform\",\n",
    "                        \"kuromoji_part_of_speech\",\n",
    "                        \"ja_stop\",\n",
    "                        \"kuromoji_number\",\n",
    "                        \"kuromoji_stemmer\",\n",
    "                        # ※ 必要に応じてここでは synonyms を外すことで、インデックス語彙を素直に保ち、\n",
    "                        #    検索時だけ同義語で想起を伸ばす設計が一般的（観測性/再構成容易性の観点）。\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 日本語用インデックス名の定義\n",
    "jp_index = \"jp_index\"\n",
    "\n",
    "# 既存インデックスの削除（検証用途）\n",
    "# 本番運用では危険操作。誤削除防止のため、対象インデックスの限定やスナップショット取得を徹底すること。\n",
    "if es.indices.exists(index=jp_index):\n",
    "    es.indices.delete(index=jp_index)\n",
    "\n",
    "# インデックス jp_index の生成（settings のみ）\n",
    "# 実運用では mappings でフィールドへ analyzer/search_analyzer を割り当てること。\n",
    "# 例）マッピングの一例（参考: 実際には create に body で併せて渡す）:\n",
    "# {\n",
    "#   \"mappings\": {\n",
    "#     \"properties\": {\n",
    "#       \"title\":   { \"type\": \"text\", \"analyzer\": \"jpn-index\", \"search_analyzer\": \"jpn-search\" },\n",
    "#       \"content\": { \"type\": \"text\", \"analyzer\": \"jpn-index\", \"search_analyzer\": \"jpn-search\" },\n",
    "#       \"brand\":   { \"type\": \"keyword\" }  # 正確一致が欲しいフィールドは keyword を併用\n",
    "#     }\n",
    "#   }\n",
    "# }\n",
    "es.indices.create(index=jp_index, body=create_index)\n",
    "\n",
    "# （動作確認のヒント）\n",
    "# - _analyze API でトークンを確認:\n",
    "#   es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-index\",\"text\":\"コンピューターを操作する\"})\n",
    "# - synonyms を更新したい場合は、新インデックスを作って reindex → エイリアス切替が安全（ゼロダウンタイム）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25169c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析結果表示用関数（Elasticsearch _analyze を用いた日本語トークン列の取得）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・インデックス jp_index に定義済みの検索用アナライザ \"jpn-search\" を使って、入力文字列を\n",
    "#     形態素解析＋各種フィルタ（原形化・品詞/ストップ語除去・数値/長音正規化 等）に通し、\n",
    "#     生成されたトークンの「語彙（token 文字列）」だけを配列として返す。\n",
    "#\n",
    "# 理論メモ:\n",
    "#   ・_analyze は「char_filter → tokenizer → filter」の順に適用される検索前処理のシミュレーション。\n",
    "#   ・ここでは検索時アナライザ（jpn-search）を明示するため、**index と analyzer を必ず指定**している。\n",
    "#     これによりインデックス固有のユーザー辞書・同義語・stoptags などが反映される。\n",
    "#   ・戻り値 ret[\"tokens\"] は各トークンの詳細辞書（token, position, start_offset, end_offset, type 等）。\n",
    "#     学習用に token 文字列のみを抽出して返すが、デバッグ時は position/offset を見ると挙動が把握しやすい。\n",
    "#\n",
    "# 前提:\n",
    "#   ・es: Elasticsearch クライアント（例: es = Elasticsearch(...)）\n",
    "#   ・jp_index: インデックス名（例: 'jp_index'）\n",
    "#   ・\"jpn-search\": jp_index の settings.analysis.analyzer に定義済みであること\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def analyse_jp_text(text):\n",
    "    # _analyze API に渡すリクエストボディを構築\n",
    "    # - analyzer: 検索用アナライザ（\"jpn-search\"）。同義語展開や原形化など検索時の処理を再現\n",
    "    # - text    : 解析対象テキスト\n",
    "    body = {\"analyzer\": \"jpn-search\", \"text\": text}\n",
    "\n",
    "    # 指定インデックスのアナライザで解析を実行\n",
    "    # 例外例:\n",
    "    #  - RequestError(400): 指定アナライザが存在しない / ユーザー辞書の読込失敗 など\n",
    "    #  - ConnectionError: ES ノード未起動・接続情報不備\n",
    "    ret = es.indices.analyze(index=jp_index, body=body)\n",
    "\n",
    "    # 返却ペイロードから tokens 配列を取得\n",
    "    tokens = ret[\"tokens\"]\n",
    "\n",
    "    # 各トークン辞書の \"token\" フィールド（語彙）だけを抽出して返す\n",
    "    tokens2 = [token[\"token\"] for token in tokens]\n",
    "    return tokens2\n",
    "\n",
    "\n",
    "# 関数のテスト\n",
    "# 期待例（設定依存）: 「関数」「テスト」などの内容語が残り、助詞/記号は除去されやすい\n",
    "print(analyse_jp_text(\"関数のテスト\"))\n",
    "\n",
    "# （デバッグ補助: 位置情報も見たい場合の例。必要時にコメントアウトを外す）\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-search\",\"text\":\"関数のテスト\"})[\"tokens\"]\n",
    "# for t in toks:\n",
    "#     print(t[\"token\"], t.get(\"position\"), t.get(\"start_offset\"), t.get(\"end_offset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.12 マッピングの設定（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・日本語用インデックス（jp_index）に対し、テキストフィールド \"content\" のマッピングを設定する。\n",
    "#   ・索引用アナライザ（\"jpn-index\"）と検索用アナライザ（\"jpn-search\"）をフィールドに紐付ける。\n",
    "#\n",
    "# 理論メモ（重要ポイント）:\n",
    "#   1) text と keyword の住み分け\n",
    "#      - Elasticsearch v6 以降は、全文検索= text、完全一致/集計= keyword が基本。\n",
    "#      - 本例の \"content\" は全文検索を想定し、type=\"text\" としている。\n",
    "#        ※ 完全一致や集計が必要なら multi-fields（例: \"content.raw\": keyword）を別途追加する設計が定石。\n",
    "#   2) analyzer / search_analyzer\n",
    "#      - index 時（投入時）: analyzer（ここでは \"jpn-index\"）でトークナイズ・正規化されて保存される。\n",
    "#      - search 時（問い合わせ）: search_analyzer（\"jpn-search\"）でクエリ側のみ前処理（同義語展開など）を行う。\n",
    "#        → 「インデックスはプレーン、検索で想起を伸ばす」設計により観測性と運用性を確保。\n",
    "#   3) put_mapping の制約\n",
    "#      - 既存フィールドの analyzer を後から変更することはできない（エラーになる/無視される）。\n",
    "#        → 変更が必要な場合は「新インデックス作成 → reindex → エイリアス切替」が基本。\n",
    "#      - 既存インデックスへの put_mapping は「新規フィールド追加」など限定的な変更のみ安全。\n",
    "#   4) 依存関係\n",
    "#      - \"jpn-index\" / \"jpn-search\" はインデックス settings.analysis に既に存在している必要がある\n",
    "#        （前のステップで create の settings に定義済みであること）。\n",
    "#   5) タイプレス・マッピング\n",
    "#      - 7.x 以降は mapping type が廃止され、現行の書き方（type-less）で良い。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"content\": {\n",
    "            # v6+ では全文検索対象は \"text\"、完全一致型は \"keyword\"\n",
    "            # （v5 までの \"string\" は廃止済み）\n",
    "            \"type\": \"text\",\n",
    "            # インデックス生成時（文書投入時）のアナライザ\n",
    "            # - \"jpn-index\": kuromoji + 各種フィルタ（同義語なし運用が一般的）\n",
    "            \"analyzer\": \"jpn-index\",\n",
    "            # 検索時（クエリ解析時）に用いるアナライザ\n",
    "            # - \"jpn-search\": 同義語展開などで想起を高める検索専用チェーン\n",
    "            \"search_analyzer\": \"jpn-search\",\n",
    "            # 【発展（必要なら追加）】\n",
    "            # \"fields\": {\n",
    "            #   \"raw\": {\n",
    "            #     \"type\": \"keyword\",\n",
    "            #     \"ignore_above\": 256\n",
    "            #     # 正規化が必要なら normalizer を定義して付与（例: lowercase）\n",
    "            #   }\n",
    "            # }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# マッピングの適用\n",
    "# 注意:\n",
    "# - 既に \"content\" が存在していて analyzer を変える等は不可。必要なら新インデックスを作成して reindex する。\n",
    "# - Python クライアント 7/8 系では body=... での指定が一般的（8.x では query=... 等の新APIもあり）。\n",
    "es.indices.put_mapping(index=jp_index, body=mapping)\n",
    "\n",
    "# 【確認のヒント（必要時・参考）】\n",
    "# cur = es.indices.get_mapping(index=jp_index)\n",
    "# from pprint import pprint; pprint(cur[jp_index][\"mappings\"][\"properties\"].get(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a505b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.13 日本語文書の投入（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・前段で定義した日本語用インデックス（jp_index）に対して、サンプル文書を投入（indexing）する。\n",
    "#   ・\"content\" フィールドには、マッピングで analyzer=\"jpn-index\" / search_analyzer=\"jpn-search\" が\n",
    "#     紐付いている前提（リスト 3.3.12）。投入時に索引用アナライザでトークナイズ・正規化される。\n",
    "#\n",
    "# 理論メモ（設計の含意）:\n",
    "#   ・全文検索の主対象は \"content\"。ここでは kuromoji＋ICU により、\n",
    "#       - 文字正規化（全/半角・互換文字）\n",
    "#       - 反復記号の展開（々/ゝ/ヽ）\n",
    "#       - 原形化・品詞/ストップ語除去\n",
    "#       - 数値正規化・長音正規化\n",
    "#     を経て、**インデックス側の語彙**が安定化する。\n",
    "#   ・検索時には \"jpn-search\" が適用され、（定義があれば）同義語展開で想起を伸ばす。\n",
    "#   ・\"title\" や \"name\" は本スニペットではマッピング未定義のため **動的マッピング**に従う\n",
    "#     （クラスター設定に依存して text / keyword サブフィールド等が自動付与される場合がある）。\n",
    "#   ・`es.index(id=...)` は **同一IDへ再投入で上書き**（バージョン更新）となる。\n",
    "#     既存IDがある場合にエラーとしたいなら `op_type=\"create\"` を用いる。\n",
    "#   ・直後に検索する検証では `refresh=\"wait_for\"` を付けるか、投入後に `es.indices.refresh()` を呼ぶ。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "bodys = [\n",
    "    {\n",
    "        \"title\": \"山田太郎の紹介\",\n",
    "        \"name\": {\"last\": \"山田\", \"first\": \"太郎\"},\n",
    "        # 例: 「スシ」表記は icu_normalizer（互換/全半角）と kuromoji_stemmer 等で揺れ吸収\n",
    "        #     同義語で「寿司, すし」を束ねたい場合は synonyms_filter に登録して検索側で展開するのが定石\n",
    "        \"content\": \"スシが好物です。犬も好きです。\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"田中次郎の紹介\",\n",
    "        \"name\": {\"last\": \"田中\", \"first\": \"次郎\"},\n",
    "        # 例: 「だいすき/大好き」揺れは icu_normalizer と原形化+ストップ語除去である程度吸収\n",
    "        \"content\": \"そばがだいすきです。ねこも大好きです。\",\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"渡辺三郎の紹介\",\n",
    "        \"name\": {\"last\": \"渡辺\", \"first\": \"三郎\"},\n",
    "        # 固有名詞（「はやぶさ」など）は NEologd やユーザー辞書を用意すると分割・品詞付与が安定\n",
    "        \"content\": \"天ぷらが好きです。新幹線はやぶさのファンです。\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 文書投入\n",
    "# - enumerate の開始は 0。既に同IDが存在する環境では上書きになる点に注意（学習用としては妥当）。\n",
    "# - ES 8.x の Python クライアントでは `document=` 引数が推奨（`body=` は後方互換）。\n",
    "for i, body in enumerate(bodys):\n",
    "    es.index(index=jp_index, id=i, body=body)\n",
    "    # 例: 衝突回避（既存IDがあれば失敗させたい）場合\n",
    "    # es.index(index=jp_index, id=i, document=body, op_type=\"create\")\n",
    "\n",
    "# 検証直後に検索する場合は refresh を明示（投入→即検索の不可視を防ぐ）\n",
    "# es.indices.refresh(index=jp_index)\n",
    "\n",
    "# （発展）\n",
    "# - 大量投入時は helpers.bulk を利用して高速化し、明示 refresh（またはエイリアス切替）で可視化を制御する。\n",
    "# - \"title\" や \"name\" にも検索要件があるなら、専用マッピング（text/keyword や search_as_you_type 等）を付与する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9050939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.14 日本語文書の検索（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・jp_index に投入済みの日本語文書から、全文検索（match）で「ｽｼ」を検索する。\n",
    "#   ・検索クエリ側にはフィールドの search_analyzer（= \"jpn-search\"）が適用されるため、\n",
    "#     文字正規化（icu_normalizer）→ 反復記号展開 → 形態素解析（kuromoji）→ 原形化/品詞・stop除去\n",
    "#     → 数値/長音正規化 →（同義語：未定義なら no-op）という流れで前処理される。\n",
    "#\n",
    "# 理論メモ:\n",
    "#   ・match は **全文検索**。`content` が text 型であればトークン化・正規化された語彙に対して BM25 で照合。\n",
    "#   ・本例のクエリ文字列 \"ｽｼ\"（半角カナ）は、search_analyzer の **icu_normalizer** により\n",
    "#     全角カタカナ「スシ」相当に正規化されるため、インデックス側（\"jpn-index\" で正規化済み）の\n",
    "#     「スシ」等と一致しやすくなる（= 表記ゆれ吸収）。\n",
    "#   ・「寿司/すし/スシ」を横断して想起を伸ばしたい場合は、synonyms_filter に\n",
    "#     \"寿司, すし, スシ\" などを登録し、**検索側**（jpn-search）で展開する設計が定石。\n",
    "#\n",
    "# 実務上の注意:\n",
    "#   ・直前に index したばかりの文書は、refresh 前は検索に出ないことがある。\n",
    "#     検証では `es.index(..., refresh=\"wait_for\")` や `es.indices.refresh(index=jp_index)` を利用。\n",
    "#   ・返却件数は既定 size=10。厳密件数が必要なら search に `track_total_hits=True` を付与。\n",
    "#   ・8.x クライアントでは `body=` より `query=` 引数が推奨（後方互換で body も動作）。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# 検索条件の設定\n",
    "# - \"ｽｼ\" は icu_normalizer で「スシ」に正規化され、kuromoji で形態素解析された上でマッチングされる。\n",
    "query = {\"query\": {\"match\": {\"content\": \"ｽｼ\"}}}\n",
    "\n",
    "# （参考：8.x 推奨形。必要ならこちらを利用）\n",
    "# res = es.search(index=jp_index, query={\"match\": {\"content\": \"ｽｼ\"}}, size=10, track_total_hits=True)\n",
    "\n",
    "# 検索実行\n",
    "res = es.search(index=jp_index, body=query)\n",
    "\n",
    "# 結果表示\n",
    "# - ensure_ascii=False: 日本語などを \\u エスケープせずに可読表示\n",
    "# - 実務では res[\"hits\"][\"hits\"] を走査して _id/_score/_source を整形してログに残すと追跡しやすい\n",
    "import json\n",
    "\n",
    "print(json.dumps(res, indent=2, ensure_ascii=False))\n",
    "\n",
    "# （発展：_source 抽出の最小例）\n",
    "# hits = [{\"_id\": h[\"_id\"], \"_score\": h[\"_score\"], \"_source\": h[\"_source\"]} for h in res[\"hits\"][\"hits\"]]\n",
    "# print(json.dumps(hits, indent=2, ensure_ascii=False))\n",
    "\n",
    "# （発展：同義語の利用例）\n",
    "# - synonyms_filter に \"寿司, すし, スシ\" を登録し、インデックスは jpn-index（同義語なし）、\n",
    "#   検索は jpn-search（同義語あり）とすることで、表記を跨いでヒットさせつつ観測性を保つ。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
