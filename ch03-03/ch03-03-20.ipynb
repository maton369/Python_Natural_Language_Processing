{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56dd055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticsearchインスタンスの生成\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.x 日本語向けアナライザ設定とインデックス作成（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・Elasticsearch に日本語検索用の analysis 設定を与える。\n",
    "#   ・検索時は同義語展開（すし/スシ/鮨/寿司）を適用し、索引時は素直な語彙を保持する設計。\n",
    "#   ・kuromoji + ICU +（任意のユーザー辞書）で日本語の表記ゆれ/活用/長音などを吸収する。\n",
    "#\n",
    "# 設計指針（理論）:\n",
    "#   ・アナライザは「char_filter → tokenizer → filter」の順で適用される。\n",
    "#   ・index 時（analyzer=\"jpn-index\"）: 文書をトークン化・正規化して**格納語彙**を作る（通常は同義語なし）。\n",
    "#   ・search 時（search_analyzer=\"jpn-search\"）: クエリを同様に処理し、**検索側のみ**同義語で想起を拡張する。\n",
    "#     → 観測性/保守性のため、同義語は検索側に寄せるのが定石（索引側に混ぜない）。\n",
    "#   ・synonym の \"A, B, C\" 形式は相互等価展開（expand=true 相当）。片方向正規化は \"A, B => C\" や synonyms_graph を検討。\n",
    "#   ・\"user_dictionary\" は **ESノード上** のパスである点に注意（ローカルPCのパスではない）。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# インデックス作成用JSONの定義\n",
    "create_index = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            # ---------------------------\n",
    "            # Token Filter（語彙変換系）の定義\n",
    "            # ---------------------------\n",
    "            \"filter\": {\n",
    "                \"synonyms_filter\": {  # 同義語フィルタ（検索側でのみ使用する想定）\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [  # 等価展開: どれで検索しても他の表記を含めて照合する\n",
    "                        \"すし,スシ,鮨,寿司\"\n",
    "                        # 片方向に正規化したい場合の例:\n",
    "                        # \"スシ, すし, 鮨 => 寿司\"\n",
    "                    ],\n",
    "                }\n",
    "                # 過正規化（ブランド名等）の保護が必要なら keyword_marker を stemmer 前段に追加する案もある\n",
    "                # \"ja_protected\": {\n",
    "                #   \"type\": \"keyword_marker\",\n",
    "                #   \"protected_words\": [\"メーラー\",\"プレイヤー\"]\n",
    "                # }\n",
    "            },\n",
    "            # ---------------------------\n",
    "            # Tokenizer（形態素分割）の定義\n",
    "            # ---------------------------\n",
    "            \"tokenizer\": {\n",
    "                \"kuromoji_w_dic\": {  # ユーザー辞書付き kuromoji トークナイザ\n",
    "                    \"type\": \"kuromoji_tokenizer\",  # ← 綴りは \"kuromoji\"（コメントの誤記に注意）\n",
    "                    \"user_dictionary\": \"my_jisho.dic\",  # ESノード側の配置パス（*.dic = ビルド済み辞書）\n",
    "                }\n",
    "            },\n",
    "            # ---------------------------\n",
    "            # Analyzer（前処理パイプライン）の定義\n",
    "            # ---------------------------\n",
    "            \"analyzer\": {\n",
    "                # 検索用（想起重視: 同義語を含める）\n",
    "                \"jpn-search\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\n",
    "                        \"icu_normalizer\",  # 全/半角・互換文字の正規化（NFKC 等）\n",
    "                        \"kuromoji_iteration_mark\",  # 々/ゝ/ヽ など反復記号の展開\n",
    "                    ],\n",
    "                    \"tokenizer\": \"kuromoji_w_dic\",  # ユーザー辞書込みの形態素解析\n",
    "                    \"filter\": [\n",
    "                        \"synonyms_filter\",  # ← 同義語展開（検索側のみ）\n",
    "                        \"kuromoji_baseform\",  # 活用の原形化（食べた→食べる 等）\n",
    "                        \"kuromoji_part_of_speech\",  # 不要品詞除去（助詞・助動詞・記号 等）\n",
    "                        \"ja_stop\",  # 日本語ストップワード除去\n",
    "                        \"kuromoji_number\",  # 数表現の正規化（漢数字→算用数字 等）\n",
    "                        \"kuromoji_stemmer\",  # カタカナ長音の正規化（コンピューター→コンピュータ）\n",
    "                    ],\n",
    "                },\n",
    "                # 索引用（精度/観測性重視: 同義語は含めない）\n",
    "                \"jpn-index\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\"icu_normalizer\", \"kuromoji_iteration_mark\"],\n",
    "                    \"tokenizer\": \"kuromoji_w_dic\",\n",
    "                    \"filter\": [\n",
    "                        \"kuromoji_baseform\",\n",
    "                        \"kuromoji_part_of_speech\",\n",
    "                        \"ja_stop\",\n",
    "                        \"kuromoji_number\",\n",
    "                        \"kuromoji_stemmer\",\n",
    "                        # ※ 索引側は素直に保持。検索側で synonyms を適用する方針。\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 日本語用インデックス名の定義\n",
    "jp_index = \"jp_index\"\n",
    "\n",
    "# 既存インデックスの削除（学習/検証用途）\n",
    "# 本番では必ずエイリアス切替 + スナップショット等の安全策を取ること。\n",
    "if es.indices.exists(index=jp_index):\n",
    "    es.indices.delete(index=jp_index)\n",
    "\n",
    "# インデックスの作成（settings のみ）\n",
    "# この後、マッピングで対象フィールドへ\n",
    "#   \"analyzer\": \"jpn-index\", \"search_analyzer\": \"jpn-search\"\n",
    "# を割り当てないと効果が出ない点に注意（content 等の text フィールドに明示する）。\n",
    "es.indices.create(index=jp_index, body=create_index)\n",
    "\n",
    "# 【動作確認のヒント（必要時にコメント解除）】\n",
    "# 1) クエリ側トークン（同義語展開を含む）を確認\n",
    "# print(es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-search\",\"text\":\"寿司\"}))\n",
    "# 2) マッピングの例（参考）\n",
    "# mapping = {\"properties\":{\"content\":{\"type\":\"text\",\"analyzer\":\"jpn-index\",\"search_analyzer\":\"jpn-search\"}}}\n",
    "# es.indices.put_mapping(index=jp_index, body=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175cf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析結果表示用関数（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・Elasticsearch の _analyze API を用いて、指定インデックス（jp_index）に定義済みの\n",
    "#     検索用アナライザ \"jpn-search\" で日本語テキストを解析し、生成トークン（語彙）だけを配列で返す。\n",
    "#\n",
    "# 前提:\n",
    "#   ・変数 es: Elasticsearch クライアントが初期化済み（例: es = Elasticsearch(...)）。\n",
    "#   ・変数 jp_index: 解析対象インデックス名の文字列（例: 'jp_index'）。\n",
    "#   ・インデックス jp_index 側の settings.analysis に \"jpn-search\" アナライザが定義されていること。\n",
    "#     （char_filter → tokenizer(kuromoji) → filter(synonyms/baseform/POS/stop/number/stemmer) のような構成）\n",
    "#\n",
    "# 出力:\n",
    "#   ・list[str]: 各トークンの \"token\" 文字列のみを抽出した配列。\n",
    "#\n",
    "# 備考（理論）:\n",
    "#   ・_analyze は「char_filter → tokenizer → filter」の順に適用された“検索前処理”の結果を返す。\n",
    "#   ・返却 JSON の tokens は、各要素に token / position / start_offset / end_offset / type などを含む。\n",
    "#     本関数は学習用途のため token 文字列のみを抽出する最小実装としている。\n",
    "#   ・検索時は通常、インデックス側は \"jpn-index\"（同義語なし）、クエリ側は \"jpn-search\"（同義語あり）\n",
    "#     と分離する設計が観測性・保守性の観点で推奨される。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def analyse_jp_text(text):\n",
    "    \"\"\"\n",
    "    与えられた文字列を \"jpn-search\" アナライザで解析し、得られた語彙（token）配列を返す。\n",
    "    例外は上位へ伝播（RequestError/ConnectionError など）する。\n",
    "    \"\"\"\n",
    "    # _analyze API に渡すリクエストボディを構築\n",
    "    # - analyzer: 検索用アナライザ名を明示（\"jpn-search\"）\n",
    "    # - text    : 解析対象の生テキスト\n",
    "    body = {\"analyzer\": \"jpn-search\", \"text\": text}\n",
    "\n",
    "    # 解析実行:\n",
    "    # - index を指定することで、そのインデックスに紐づく analysis（ユーザー辞書・同義語など）が反映される\n",
    "    # - 代表的な失敗例:\n",
    "    #   * 400 Bad Request: \"jpn-search\" 未定義 / analysis 設定の不整合 / ユーザー辞書読み込み失敗\n",
    "    #   * 接続例外       : ES ノード未起動・接続設定不備\n",
    "    ret = es.indices.analyze(index=jp_index, body=body)\n",
    "\n",
    "    # レスポンス構造の例:\n",
    "    # {\n",
    "    #   \"tokens\": [\n",
    "    #     {\"token\": \"寿司\", \"start_offset\": 0, \"end_offset\": 2, \"type\": \"...\", \"position\": 0},\n",
    "    #     ...\n",
    "    #   ]\n",
    "    # }\n",
    "    tokens = ret[\"tokens\"]\n",
    "\n",
    "    # 各トークン辞書から \"token\" フィールド（語彙文字列）のみを抽出\n",
    "    tokens2 = [token[\"token\"] for token in tokens]\n",
    "\n",
    "    # 学習用途として単純な配列で返す（詳細が必要なら position/offset を返す拡張版を別関数で用意すると良い）\n",
    "    return tokens2\n",
    "\n",
    "\n",
    "# 関数のテスト:\n",
    "# ・設定に依存するが、助詞/記号は POS/ja_stop で落ち、内容語が中心に残ることが多い。\n",
    "# ・同義語（例: すし/スシ/鮨/寿司）を \"jpn-search\" に定義している場合は、クエリ側の表記ゆれ吸収を確認できる。\n",
    "print(analyse_jp_text(\"関数のテスト\"))\n",
    "\n",
    "# （デバッグ補助: クエリ側の実トークンを詳細に確認したい場合の例。必要時のみコメント解除）\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\": \"jpn-search\", \"text\": \"関数のテスト\"})[\"tokens\"]\n",
    "# for t in toks:\n",
    "#     print(t[\"token\"], t.get(\"position\"), t.get(\"start_offset\"), t.get(\"end_offset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddced1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  リスト3.3.20 辞書登録後の「新幹線はやぶさ」と「はやぶさ」の分析結果\n",
    "\n",
    "print(analyse_jp_text(\"新幹線はやぶさ\"))\n",
    "print(analyse_jp_text(\"はやぶさ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dcdffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.12（改）マッピング定義（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・インデックス jp_index に対し、全文検索対象フィールド \"content\" のマッピングを設定する。\n",
    "#   ・投入時は analyzer=\"jpn-index\"（索引用チェーン）、検索時は search_analyzer=\"jpn-search\"\n",
    "#     （検索用チェーン: 同義語などで想起拡張）を適用して、表記ゆれに強く観測性の高い設計にする。\n",
    "#\n",
    "# 理論ポイント:\n",
    "#   ・Elasticsearch v6+ では全文検索用は \"text\"、完全一致や集計用は \"keyword\" を使う。\n",
    "#   ・\"analyzer\" は **索引時**（ドキュメント投入時）に適用、\"search_analyzer\" は **検索時**（クエリ解析）\n",
    "#     に適用されるため、両者を分離すると「索引は素直に、検索で拡張」という運用が可能になる。\n",
    "#   ・既存フィールドの analyzer を **後から変更することはできない**。変更が必要なら\n",
    "#     新インデックス作成 → _reindex → エイリアス切替、が基本手順。\n",
    "#   ・title や name など他フィールドに検索/集計要件があるなら、それぞれの型・アナライザも明示定義する。\n",
    "#   ・完全一致/ソート/集計のために \"content.raw\" のような keyword のマルチフィールドを併設するのが定石。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"content\": {\n",
    "            # 全文検索対象として \"text\" 型を使用（v5 までの \"string\" は廃止済み）\n",
    "            \"type\": \"text\",\n",
    "            # ドキュメント投入（index）時のアナライザ\n",
    "            # - \"jpn-index\": kuromoji + 各種フィルタ（通常は同義語なし）\n",
    "            # - 索引側を“素直”に保つことで、語彙の観測性と再構築容易性を確保\n",
    "            \"analyzer\": \"jpn-index\",\n",
    "            # クエリ解析（search）時のアナライザ\n",
    "            # - \"jpn-search\": 同義語展開（例: すし/スシ/鮨/寿司）などで想起を拡張し、表記ゆれを吸収\n",
    "            \"search_analyzer\": \"jpn-search\",\n",
    "            # （必要に応じて追加する例：完全一致や集計用の keyword サブフィールド）\n",
    "            # \"fields\": {\n",
    "            #   \"raw\": {\n",
    "            #     \"type\": \"keyword\",\n",
    "            #     \"ignore_above\": 256\n",
    "            #     # 必要に応じて normalizer を定義（lowercase など）\n",
    "            #   }\n",
    "            # }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# マッピングの適用\n",
    "# - 既存インデックスに対しては「新規フィールド追加」等は可能だが、\n",
    "#   既存フィールドの analyzer 変更は不可。必要なら新インデックス移行を行うこと。\n",
    "es.indices.put_mapping(index=jp_index, body=mapping)\n",
    "\n",
    "# （確認ヒント：必要時にコメント解除）\n",
    "# cur = es.indices.get_mapping(index=jp_index)\n",
    "# from pprint import pprint\n",
    "# pprint(cur[jp_index][\"mappings\"][\"properties\"].get(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c6ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.13（再掲・詳細コメント付き）日本語文書の投入\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・前段で作成済みの日本語向けインデックス jp_index に、サンプル文書を投入（index）する。\n",
    "#\n",
    "# 前提/設計:\n",
    "#   ・\"content\" フィールドはマッピングで\n",
    "#       analyzer=\"jpn-index\"（索引時） / search_analyzer=\"jpn-search\"（検索時）\n",
    "#     を設定済み（= 投入時は文字正規化→kuromoji→原形化/stop/長音などが適用され、語彙が安定化）。\n",
    "#   ・検索時は \"jpn-search\" 側で同義語（例: すし/スシ/鮨/寿司）等を適用し、想起を拡張する方針。\n",
    "#\n",
    "# 実務上の注意:\n",
    "#   1) 動的マッピング:\n",
    "#      - 本スニペットでは title, name の明示マッピングは未指定 → 動的マッピングに従う。\n",
    "#        これらを検索/集計に使うなら text/keyword 等を明示定義するのが望ましい。\n",
    "#   2) 上書き挙動:\n",
    "#      - es.index(index=..., id=...) は同一IDが存在すれば上書き（バージョン増分）となる。\n",
    "#        既存IDがあれば失敗させたいときは op_type=\"create\" を使う。\n",
    "#   3) 即時検索の可視化:\n",
    "#      - 直後に検索するなら refresh=\"wait_for\" を付けるか、事後に es.indices.refresh(index=...) を呼ぶ。\n",
    "#   4) パフォーマンス:\n",
    "#      - 大量投入では elasticsearch.helpers.bulk の利用を推奨。\n",
    "#   5) 互換性:\n",
    "#      - elasticsearch-py 8.x では body より document 引数が推奨（後方互換のため body でも動作）。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "bodys = [\n",
    "    {  # ドキュメントID: 0\n",
    "        \"title\": \"山田太郎の紹介\",\n",
    "        \"name\": {\"last\": \"山田\", \"first\": \"太郎\"},\n",
    "        # 例: 「スシ」は icu_normalizer の互換正規化や kuromoji の形態素解析、\n",
    "        #     さらに stemmer/stop 等により、索引側の語彙として安定化して格納される。\n",
    "        \"content\": \"スシが好物です。犬も好きです。\",\n",
    "    },\n",
    "    {  # ドキュメントID: 1\n",
    "        \"title\": \"田中次郎の紹介\",\n",
    "        \"name\": {\"last\": \"田中\", \"first\": \"次郎\"},\n",
    "        # 例: 「だいすき/大好き」の表記ゆれは正規化＋原形化である程度吸収。\n",
    "        #     固有表現（新幹線名など）はユーザー辞書の整備で分割の安定性が向上する。\n",
    "        \"content\": \"そばがだいすきです。ねこも大好きです。\",\n",
    "    },\n",
    "    {  # ドキュメントID: 2\n",
    "        \"title\": \"渡辺三郎の紹介\",\n",
    "        \"name\": {\"last\": \"渡辺\", \"first\": \"三郎\"},\n",
    "        # 例: 「はやぶさ」は固有名詞として 1 トークン化されやすいが辞書依存。\n",
    "        \"content\": \"天ぷらが好きです。新幹線はやぶさのファンです。\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 文書投入ループ\n",
    "# - enumerate により _id=0,1,2 を割り当て。\n",
    "# - 直後に検索する検証を行うなら refresh=\"wait_for\" を付けると可視化の遅延を避けられる。\n",
    "for i, body in enumerate(bodys):\n",
    "    es.index(index=jp_index, id=i, body=body)\n",
    "    # 例（8.x 推奨の引数スタイル & 衝突回避・即時可視化の例）:\n",
    "    # es.index(index=jp_index, id=i, document=body, op_type=\"create\", refresh=\"wait_for\")\n",
    "\n",
    "# 直後に検索する場合の明示リフレッシュ（上の refresh を使わない場合の代替）\n",
    "# es.indices.refresh(index=jp_index)\n",
    "\n",
    "# （参考）大量投入の雛形:\n",
    "# from elasticsearch import helpers\n",
    "# actions = ({\"_index\": jp_index, \"_id\": i, \"_source\": doc} for i, doc in enumerate(bodys))\n",
    "# helpers.bulk(es, actions, refresh=\"wait_for\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64971c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.21 辞書登録後にキーワード「はやぶさ」で検索（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・ユーザー辞書（my_jisho.dic）を組み込んだ kuromoji トークナイザ設定（\"kuromoji_w_dic\"）を\n",
    "#     インデックス jp_index に反映させた後（＝「辞書登録後」）に、\n",
    "#     検索用アナライザ \"jpn-search\" でクエリ語「はやぶさ」を全文検索（match）し、ヒット結果を確認する。\n",
    "#\n",
    "# 重要ポイント（理論）:\n",
    "#   1) アナライザの適用:\n",
    "#      - 検索時は search_analyzer=\"jpn-search\" がクエリ文字列に適用される。\n",
    "#        処理順: char_filter（icu_normalizer 等）→ tokenizer（kuromoji_w_dic）→ filter\n",
    "#        （synonyms/baseform/POS/stop/number/stemmer）\n",
    "#   2) ユーザー辞書の効果:\n",
    "#      - 「はやぶさ」単体は既定辞書でも 1 トークンになりやすいが、\n",
    "#        連接語（例: 「新幹線はやぶさ」）の分割安定化・品詞付与の改善にユーザー辞書が効く。\n",
    "#      - **辞書や analysis 設定の変更は、既に格納済みのトークンには後追いで反映されない。**\n",
    "#        → 変更反映には「新インデックス作成 → _reindex → エイリアス切替」等が必要。\n",
    "#   3) 検索意味論:\n",
    "#      - `match` は token-level の全文検索。BM25（TF・IDF・文書長補正）でスコアリング。\n",
    "#      - 語順・近接を重視するなら `match_phrase`、複数フィールド重み付けなら `multi_match` を検討。\n",
    "#   4) 可視化の安定化:\n",
    "#      - 直前に index した文書を即検索で確認する場合、index 側で `refresh=\"wait_for\"` を使うか、\n",
    "#        検索前に `es.indices.refresh(index=jp_index)` を行う。\n",
    "#   5) 同義語について:\n",
    "#      - 本系の同義語定義は「寿司/すし/スシ/鮨」であり、「はやぶさ」には同義語展開は掛からない想定（no-op）。\n",
    "#        固有名詞の別表記を束ねたい場合は synonyms を別途設計する。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# 検索条件の設定\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content\": \"はやぶさ\"  # ← 検索側で jpn-search が適用され、kuromoji_w_dic に基づきトークナイズされる\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# （参考: 8.x 推奨の引数スタイルと厳密件数の明示。必要に応じて置き換え）\n",
    "# res = es.search(\n",
    "#     index=jp_index,\n",
    "#     query={\"match\": {\"content\": \"はやぶさ\"}},\n",
    "#     size=20,\n",
    "#     track_total_hits=True,\n",
    "#     _source=[\"title\",\"name.last\",\"content\"]\n",
    "# )\n",
    "\n",
    "# 検索実行（後方互換のため body=... でも可）\n",
    "res = es.search(index=jp_index, body=query)\n",
    "\n",
    "# 結果表示\n",
    "import json\n",
    "\n",
    "print(json.dumps(res, indent=2, ensure_ascii=False))\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# デバッグ補助（必要時のみコメント解除）:\n",
    "# 1) クエリ側トークンの確認（辞書・stop・原形化の影響を把握）\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-search\",\"text\":\"はやぶさ\"})[\"tokens\"]\n",
    "# for t in toks:\n",
    "#     print(t[\"token\"], t.get(\"position\"), t.get(\"start_offset\"), t.get(\"end_offset\"))\n",
    "#\n",
    "# 2) 直前投入文書の可視化（refresh）\n",
    "# es.indices.refresh(index=jp_index)\n",
    "#\n",
    "# 3) フレーズ一致（語順・近接を重視：連接語を狙い撃ち）\n",
    "# res = es.search(index=jp_index, query={\"match_phrase\": {\"content\": \"新幹線 はやぶさ\"}}, size=20, track_total_hits=True)\n",
    "# print(json.dumps(res, indent=2, ensure_ascii=False))\n",
    "#\n",
    "# 4) 再インデックスの注意:\n",
    "# - ユーザー辞書の更新や analysis の変更は既存トークンに遡及しないため、\n",
    "#   新インデックス（更新済み settings + mappings）を作り、_reindex で移行するのが基本。\n",
    "# -----------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
