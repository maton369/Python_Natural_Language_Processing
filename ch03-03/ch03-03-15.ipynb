{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elasticsearchインスタンスの生成\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebae271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト3.3.15 同義語の定義（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・日本語検索向けに「検索時のみ」同義語展開を行うアナライザ \"jpn-search\" を定義し、\n",
    "#     インデックス jp_index を作成する。\n",
    "#   ・同義語は「すし」「スシ」「鮨」「寿司」を相互同値として扱い、表記ゆれを吸収する。\n",
    "#\n",
    "# 理論メモ:\n",
    "#   ・synonym フィルタは既定で **等価展開（expand=true 相当）** を行う。\n",
    "#       \"A, B, C\" 形式は A↔B↔C を相互に展開（どれで検索しても他が候補に加わる）。\n",
    "#     片方向の正規化（例: 略語→正規形）を行いたい場合は \"A, B => C\" 形式や synonyms_graph を検討する。\n",
    "#   ・**検索側のみで同義語**を適用し、**索引側は素直に保持**するのが一般的な設計。\n",
    "#     - 理由: 観測性（ログで元の語が見える）、運用性（辞書更新時に再インデックスを避けやすい）。\n",
    "#   ・同義語の更新はしばしば **インデックス再作成＋reindex→エイリアス切替** が最も安全。\n",
    "#     ホットリロードに制約があるため、変更計画は運用手順に組み込むこと。\n",
    "#   ・トークナイズ順序: char_filter → tokenizer（kuromoji）→ filter（synonyms→原形化→POS/stop→数値→長音）。\n",
    "#     先に正規化・形態素解析を済ませた語彙に対して同義語展開が掛かる想定。\n",
    "#   ・注意: \"user_dictionary\" は **Elasticsearch ノード側のパス**。ローカルファイルではない。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "# インデックス作成用JSONの定義\n",
    "create_index = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            # ---------------------------\n",
    "            # Token Filter 定義\n",
    "            # ---------------------------\n",
    "            \"filter\": {\n",
    "                \"synonyms_filter\": {  # 同義語フィルタの定義（検索側でのみ使用）\n",
    "                    \"type\": \"synonym\",\n",
    "                    \"synonyms\": [  # 等価展開の同義語リスト\n",
    "                        \"すし, スシ, 鮨, 寿司\"\n",
    "                        # 片方向に正規化したい場合の例:\n",
    "                        # \"スシ, 鮨, すし => 寿司\"\n",
    "                    ],\n",
    "                    # 必要に応じて:\n",
    "                    # \"lenient\": true   # フォーマット厳格性を緩める（検討用）\n",
    "                    # \"expand\":  true   # 既定で true（相互展開）。false で片方向のみ\n",
    "                }\n",
    "                # 固有名詞を過正規化から保護したい場合の例（stemmer などの前段に置く）:\n",
    "                # ,\"ja_protected\": {\n",
    "                #     \"type\": \"keyword_marker\",\n",
    "                #     \"protected_words\": [\"メーラー\",\"プレイヤー\"]\n",
    "                # }\n",
    "            },\n",
    "            # ---------------------------\n",
    "            # Tokenizer 定義\n",
    "            # ---------------------------\n",
    "            \"tokenizer\": {\n",
    "                \"kuromoji_w_dic\": {  # カスタム形態素解析の定義\n",
    "                    \"type\": \"kuromoji_tokenizer\",  # ← 綴りは \"kuromoji\"\n",
    "                    # ユーザー辞書（*.dic＝ビルド済み）を ES ノード側の参照可能パスで指定\n",
    "                    \"user_dictionary\": \"my_jisho.dic\",\n",
    "                }\n",
    "            },\n",
    "            # ---------------------------\n",
    "            # Analyzer 定義\n",
    "            # ---------------------------\n",
    "            \"analyzer\": {\n",
    "                # 検索用アナライザ（想起重視: 同義語を含める）\n",
    "                \"jpn-search\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\n",
    "                        \"icu_normalizer\",  # 文字正規化（NFKC 等：全半角・互換文字）\n",
    "                        \"kuromoji_iteration_mark\",  # 々/ゝ/ヽ 等の展開\n",
    "                    ],\n",
    "                    \"tokenizer\": \"kuromoji_w_dic\",  # ユーザー辞書付き kuromoji\n",
    "                    \"filter\": [\n",
    "                        \"synonyms_filter\",  # ← 同義語展開（検索側のみ）\n",
    "                        \"kuromoji_baseform\",  # 活用語の原形化\n",
    "                        \"kuromoji_part_of_speech\",  # 不要品詞除去（助詞/助動詞/記号 等）\n",
    "                        \"ja_stop\",  # 日本語ストップワード除去\n",
    "                        \"kuromoji_number\",  # 数表現の正規化（漢数字→算用数字 等）\n",
    "                        \"kuromoji_stemmer\",  # カタカナ長音の正規化（コンピューター→コンピュータ）\n",
    "                    ],\n",
    "                },\n",
    "                # 索引用アナライザ（精度重視: 通常は同義語を含めない）\n",
    "                \"jpn-index\": {\n",
    "                    \"type\": \"custom\",\n",
    "                    \"char_filter\": [\"icu_normalizer\", \"kuromoji_iteration_mark\"],\n",
    "                    \"tokenizer\": \"kuromoji_w_dic\",\n",
    "                    \"filter\": [\n",
    "                        \"kuromoji_baseform\",\n",
    "                        \"kuromoji_part_of_speech\",\n",
    "                        \"ja_stop\",\n",
    "                        \"kuromoji_number\",\n",
    "                        \"kuromoji_stemmer\",\n",
    "                        # ※ インデックス側は素直に保持。検索側で拡張（synonyms）する方針。\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 日本語用インデックス名の定義\n",
    "jp_index = \"jp_index\"\n",
    "\n",
    "# 既存インデックスがあれば削除（検証用。運用ではエイリアス切替＋安全な移行手順が推奨）\n",
    "if es.indices.exists(index=jp_index):\n",
    "    es.indices.delete(index=jp_index)\n",
    "\n",
    "# インデックス jp_index の生成（ここでは settings のみ）\n",
    "# 実運用では mappings で対象フィールドに\n",
    "#   \"analyzer\": \"jpn-index\", \"search_analyzer\": \"jpn-search\"\n",
    "# を紐付けること（例: content フィールド）。\n",
    "es.indices.create(index=jp_index, body=create_index)\n",
    "\n",
    "# 【動作確認のヒント（コメント）】\n",
    "# - _analyze で同義語展開を確認:\n",
    "#   es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-search\",\"text\":\"寿司\"})\n",
    "#   → tokens に \"寿司\" と等価語（\"すし\",\"スシ\",\"鮨\" 等）が含まれることを確認（構成に依存）\n",
    "# - 同義語更新フロー例:\n",
    "#   新インデックス作成（新 synonyms）→ _reindex → エイリアス切替 → 旧を削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015aa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析結果表示用関数（Elasticsearch の _analyze を用いて日本語トークン列を取得）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・インデックス jp_index に定義済みの検索用アナライザ \"jpn-search\" を使って、\n",
    "#     入力テキストを形態素解析＋各種フィルタ処理（原形化・品詞/ストップ語除去・数値/長音正規化・同義語展開 等）\n",
    "#     に通し、得られた「語彙（token 文字列）」の配列を返す。\n",
    "#\n",
    "# 前提:\n",
    "#   ・Elasticsearch 側で jp_index が作成済みで、settings.analysis.analyzer に \"jpn-search\" が存在すること。\n",
    "#   ・Python 側では es（Elasticsearch クライアント）と jp_index 変数が有効であること。\n",
    "#\n",
    "# 理論メモ（何が起きるか）:\n",
    "#   ・_analyze は「char_filter → tokenizer → filter」の順に処理を適用する。\n",
    "#   ・\"jpn-search\" の想定チェーン（例）:\n",
    "#       1) char_filter: icu_normalizer（全角/半角・互換文字の正規化）, kuromoji_iteration_mark（々/ゝ/ヽ の展開）\n",
    "#       2) tokenizer : kuromoji（ユーザー辞書付き）\n",
    "#       3) filter    : synonyms（同義語展開）→ baseform（原形化）→ POS/ja_stop（機能語除去）\n",
    "#                      → number（数値正規化）→ stemmer（カタカナ長音の正規化）\n",
    "#     これにより検索クエリ側の語彙が安定化し、インデックス側（通常は \"jpn-index\"）で格納された語彙と\n",
    "#     BM25 等で比較される前提を検証できる。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def analyse_jp_text(text):\n",
    "    # _analyze API に渡すリクエストボディを構築\n",
    "    # - analyzer: 使用するアナライザ（検索時用 \"jpn-search\"）\n",
    "    # - text    : 解析対象の文字列\n",
    "    body = {\"analyzer\": \"jpn-search\", \"text\": text}\n",
    "\n",
    "    # 指定インデックスのアナライザで解析を実行\n",
    "    # 例外例:\n",
    "    #   - RequestError(400): 指定アナライザが存在しない / 辞書読込失敗 / 設定不整合\n",
    "    #   - ConnectionError  : ES ノード未起動・接続情報不備\n",
    "    # 本関数では例外を上位へ伝播（学習用のため最小実装）。\n",
    "    ret = es.indices.analyze(index=jp_index, body=body)\n",
    "\n",
    "    # レスポンスは {\"tokens\": [ { \"token\": \"...\", \"start_offset\": ..., \"end_offset\": ..., \"position\": ..., \"type\": \"...\" }, ... ]}\n",
    "    # 学習用として語彙文字列 \"token\" だけを抽出して返す\n",
    "    tokens = ret[\"tokens\"]\n",
    "    tokens2 = [token[\"token\"] for token in tokens]\n",
    "    return tokens2\n",
    "\n",
    "\n",
    "# 関数のテスト\n",
    "# 期待例（設定依存）:\n",
    "#   ・助詞や句読点は POS/ja_stop で落ちやすく、内容語が残る\n",
    "#   ・「スシ/寿司/すし」のような表記揺れは icu_normalizer＋synonyms で吸収可能（設定していれば）\n",
    "print(analyse_jp_text(\"関数のテスト\"))\n",
    "\n",
    "# （デバッグ補助：必要に応じて有効化）\n",
    "# - 実際のトークン列（位置やオフセット）を見て解析チェーンの挙動を確認できる\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\": \"jpn-search\", \"text\": \"関数のテスト\"})[\"tokens\"]\n",
    "# for t in toks:\n",
    "#     print(t[\"token\"], t.get(\"position\"), t.get(\"start_offset\"), t.get(\"end_offset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.16 同義語のテスト（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・検索用アナライザ \"jpn-search\" に定義した同義語（すし/スシ/鮨/寿司）が\n",
    "#     クエリ側で展開され、表記ゆれを吸収してマッチ範囲を広げることを確認する。\n",
    "#\n",
    "# 前提:\n",
    "#   ・インデックス jp_index が存在し、settings.analysis.analyzer に \"jpn-search\" が定義済み。\n",
    "#   ・\"jpn-search\" の filter 先頭に `synonyms_filter`（\"すし, スシ, 鮨, 寿司\"）がある前提。\n",
    "#   ・analyse_jp_text(text) は _analyze を呼び、\"jpn-search\" で解析した token 文字列配列を返す関数。\n",
    "#\n",
    "# 解析チェーン（想定）:\n",
    "#   char_filter（icu_normalizer, kuromoji_iteration_mark）\n",
    "#     → tokenizer（kuromoji + user_dictionary）\n",
    "#       → filter（synonyms → baseform → POS/ja_stop → number → stemmer）\n",
    "#   - 同義語は tokenizer の後に **同位置の代替語として展開**（等価展開; expand=true 相当）。\n",
    "#   - その後、原形化・不要品詞/ストップ語除去・数値/長音正規化が順次適用される。\n",
    "#\n",
    "# 期待される挙動（出力は環境依存の一例。辞書・stoptags により差異あり）:\n",
    "#   1) '寿司を食べたい'\n",
    "#      - 「寿司」 → 同義語展開で {寿司, すし, スシ, 鮨} が同じ position に並ぶ\n",
    "#      - 「を」   → 助詞のため POS/ja_stop で除去されやすい\n",
    "#      - 「食べたい」→ 原形化で「食べる」\n",
    "#      → 例: ['寿司','すし','スシ','鮨','食べる']\n",
    "#\n",
    "#   2) '私はスシが好きだ'\n",
    "#      - 「スシ」 → 同義語展開で {スシ, すし, 寿司, 鮨}\n",
    "#      - 「が」「だ」→ 助詞/助動詞のため除去されやすい\n",
    "#      - 「好きだ」 → 原形化で「好き」\n",
    "#      - 「私」     → 名詞として残る（stop 語に含めない限り）\n",
    "#      → 例: ['私','スシ','すし','寿司','鮨','好き']\n",
    "#\n",
    "# メモ:\n",
    "#   ・同義語は「検索側（jpn-search）」にのみ適用し、インデックス側（jpn-index）は素直に保持するのが定石。\n",
    "#     これにより、変更時は新インデックス作成＋reindex＋エイリアス切替で安全に反映できる。\n",
    "#   ・実運用で不要語（例: 一人称「私」）を落としたい場合は、`ja_stop` の拡張や `kuromoji_part_of_speech`\n",
    "#     の `stoptags` 明示で制御するとよい。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# テスト1: 「寿司」を含む文。助詞は除去されやすく、「食べたい」は原形「食べる」に。\n",
    "print(analyse_jp_text(\"寿司を食べたい\"))\n",
    "\n",
    "# テスト2: 半角/全角/漢字/かなを横断できるか確認（「スシ」が同義語展開で統合される想定）。\n",
    "print(analyse_jp_text(\"私はスシが好きだ\"))\n",
    "\n",
    "# （デバッグ補助：必要ならコメント解除）\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-search\",\"text\":\"私はスシが好きだ\"})[\"tokens\"]\n",
    "# for t in toks:\n",
    "#     # token と position/offset を見ると、同位置展開（synonyms）の挙動が把握しやすい\n",
    "#     print(t[\"token\"], t.get(\"position\"), t.get(\"start_offset\"), t.get(\"end_offset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f821b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.12（改） マッピング定義（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・インデックス jp_index に対し、全文検索対象フィールド \"content\" のマッピングを設定する。\n",
    "#   ・投入時は \"jpn-index\"（索引用アナライザ）、検索時は \"jpn-search\"（検索用アナライザ）を適用する。\n",
    "#\n",
    "# 背景/理論:\n",
    "#   ・Elasticsearch v6 以降、全文検索用は \"text\"、完全一致/集計用は \"keyword\" が基本。\n",
    "#   ・\"analyzer\" は「ドキュメント投入（indexing）」時に使われ、\"search_analyzer\" は\n",
    "#     「クエリ解析」時に使われる（両者で処理を分離可能）。\n",
    "#     - 本設計では、インデックス側は“素直に保持”（同義語なし）、\n",
    "#       検索側で“想起を拡張”（同義語あり）することで観測性・運用性を確保する。\n",
    "#\n",
    "# 注意（重要）:\n",
    "#   ・既に存在するフィールドに対して analyzer を変更することは不可（仕様）。必要なら\n",
    "#     新インデックスを作成 → _reindex → エイリアス切替、の手順で移行する。\n",
    "#   ・本マッピングは \"content\" のみを定義。他のフィールド（title, name 等）にも\n",
    "#     解析要件がある場合は、それぞれに適切な analyzer / search_analyzer を明示する。\n",
    "#   ・完全一致や並べ替え/集計が必要なら multi-fields を追加し、\"content.raw\": keyword\n",
    "#     のように併用するのが定石。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "mapping = {\n",
    "    \"properties\": {\n",
    "        \"content\": {\n",
    "            # 全文検索対象のため \"text\" 型を指定（v5 の \"string\" は廃止済み）\n",
    "            \"type\": \"text\",\n",
    "            # ドキュメント投入時に使うアナライザ\n",
    "            # - \"jpn-index\": kuromoji + 各種フィルタ（通常は同義語なし）\n",
    "            # - 索引側をプレーンに保つことで、語彙の観測性・再構成容易性が高まる\n",
    "            \"analyzer\": \"jpn-index\",\n",
    "            # クエリ解析時に使うアナライザ\n",
    "            # - \"jpn-search\": 同義語（すし/スシ/鮨/寿司）等で想起を拡張し、表記ゆれを吸収\n",
    "            \"search_analyzer\": \"jpn-search\",\n",
    "            # （必要に応じて追加例）\n",
    "            # \"fields\": {\n",
    "            #   \"raw\": {                 # 完全一致・ソート・集計用のサブフィールド\n",
    "            #     \"type\": \"keyword\",\n",
    "            #     \"ignore_above\": 256\n",
    "            #     # normalizer を定義して大文字小文字統一なども可\n",
    "            #   }\n",
    "            # }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# マッピングの適用\n",
    "# - 既存フィールドの analyzer 変更は不可。適用エラーや無視の原因になるため、移行は新インデックスで。\n",
    "es.indices.put_mapping(index=jp_index, body=mapping)\n",
    "\n",
    "# （確認のヒント：必要時にコメント解除）\n",
    "# cur = es.indices.get_mapping(index=jp_index)\n",
    "# from pprint import pprint\n",
    "# pprint(cur[jp_index][\"mappings\"][\"properties\"].get(\"content\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb92c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.13（改）日本語文書の投入（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・前段で作成した日本語向けインデックス jp_index に、学習用のサンプル文書を投入する。\n",
    "#   ・\"content\" フィールドにはマッピングで analyzer=\"jpn-index\" / search_analyzer=\"jpn-search\" を\n",
    "#     付与済み（リスト 3.3.12）。投入時は jpn-index に従ってトークン化・正規化される。\n",
    "#\n",
    "# 理論メモ（索引時に何が起きるか）:\n",
    "#   ・インデックス時（analyzer=\"jpn-index\"）:\n",
    "#       char_filter（icu_normalizer, iteration_mark）\n",
    "#         → tokenizer（kuromoji + user_dictionary）\n",
    "#           → filter（baseform, POS/ja_stop, number, stemmer）で**格納語彙**を生成。\n",
    "#     これにより「全角/半角」「々/ゝ」「活用」「長音」などの表記ゆれが索引側で安定化する。\n",
    "#   ・検索時（search_analyzer=\"jpn-search\"）は上記に加えて synonyms を適用（表記ゆれの想起拡張）。\n",
    "#     ⇒ 「スシ/すし/寿司/鮨」の横断が可能（同義語を jpn-search のみに置くのが定石）。\n",
    "#\n",
    "# 実務上の注意:\n",
    "#   ・このスニペットでは \"title\" や \"name\" の明示マッピングは未設定 → 動的マッピングに依存する。\n",
    "#     厳密な検索/集計要件があるなら、それぞれに text/keyword を設計しておくこと。\n",
    "#   ・`es.index(id=...)` は同一 ID への再投入で**上書き**になる。衝突を避けたい場合は `op_type=\"create\"`。\n",
    "#   ・直後に検索する検証では `refresh=\"wait_for\"` を付けるか、ループ後に `es.indices.refresh()` を呼ぶ。\n",
    "#   ・大量投入は `elasticsearch.helpers.bulk` の利用で高速化・最適化できる。\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "bodys = [\n",
    "    {  # ドキュメント 0\n",
    "        \"title\": \"山田太郎の紹介\",\n",
    "        \"name\": {\"last\": \"山田\", \"first\": \"太郎\"},\n",
    "        # 例: 「スシ」は icu_normalizer（互換/全半角）や stemmer と kuromoji により安定化して格納される\n",
    "        \"content\": \"スシが好物です。犬も好きです。\",\n",
    "    },\n",
    "    {  # ドキュメント 1\n",
    "        \"title\": \"田中次郎の紹介\",\n",
    "        \"name\": {\"last\": \"田中\", \"first\": \"次郎\"},\n",
    "        # 例: 「だいすき/大好き」の揺れは正規化＋原形化/stop である程度吸収\n",
    "        #     固有名詞（はやぶさ）などは NEologd/ユーザー辞書の整備で分割の安定性が向上\n",
    "        \"content\": \"そばがだいすきです。新幹線はやぶさも好きです。\",\n",
    "    },\n",
    "    {  # ドキュメント 2\n",
    "        \"title\": \"渡辺三郎の紹介\",\n",
    "        \"name\": {\"last\": \"渡辺\", \"first\": \"三郎\"},\n",
    "        \"content\": \"天ぷらが好きです。はやぶさのファンです。\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# 文書投入:\n",
    "# - enumerate の開始は 0。ここでは _id = 0, 1, 2 で投入される。\n",
    "# - 既存 ID があると**上書き**される点に注意（検証用としては妥当）。\n",
    "# - ES 8.x のクライアントでは `document=` 引数が推奨（`body=` は後方互換）。\n",
    "for i, body in enumerate(bodys):\n",
    "    es.index(index=jp_index, id=i, body=body)\n",
    "    # 既存 ID があれば失敗させたい場合（衝突検出）:\n",
    "    # es.index(index=jp_index, id=i, document=body, op_type=\"create\")\n",
    "    # 直後に検索する検証では各呼び出しに refresh を付ける:\n",
    "    # es.index(index=jp_index, id=i, document=body, refresh=\"wait_for\")\n",
    "\n",
    "# バルク投入の雛形（大量データ時の推奨; ここでは実行しないため参考のみ）\n",
    "# from elasticsearch import helpers\n",
    "# actions = ({\"_index\": jp_index, \"_id\": i, \"_source\": doc} for i, doc in enumerate(bodys))\n",
    "# helpers.bulk(es, actions, refresh=\"wait_for\")\n",
    "\n",
    "# 直後に検索する場合の明示リフレッシュ（wait_for を使わない場合の代替）\n",
    "# es.indices.refresh(index=jp_index)\n",
    "\n",
    "# 参考: 同義語テスト（検索側のみ synonyms を適用）\n",
    "# - jpn-search に \"すし, スシ, 鮨, 寿司\" を定義済みなら、いずれの表記でもヒットが期待できる\n",
    "# res = es.search(index=jp_index, query={\"match\": {\"content\": \"寿司\"}})\n",
    "# print(res[\"hits\"][\"hits\"][0][\"_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7f68f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.17 同義語による検索（詳細コメント付き）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・\"content\" フィールドに対し、検索用アナライザ \"jpn-search\" を用いた全文検索（match）を行う。\n",
    "#   ・\"jpn-search\" には同義語フィルタ（例: 「すし, スシ, 鮨, 寿司」）が含まれている前提。\n",
    "#     → クエリ語「寿司」を投入すると、同位置に等価語が展開され、表記ゆれを横断してヒットが期待できる。\n",
    "#\n",
    "# 重要な理論ポイント（何が起きるか）:\n",
    "#   1) match クエリは **全文検索**であり、クエリ文字列は search_analyzer（= \"jpn-search\"）で前処理される。\n",
    "#      処理順: char_filter（icu_normalizer/iteration_mark）→ tokenizer（kuromoji）→\n",
    "#               filter（synonyms → baseform → POS/ja_stop → number → stemmer）\n",
    "#   2) `synonyms_filter` は等価展開（expand=true 相当）:\n",
    "#      - 「寿司」→ 同位置に {寿司, すし, スシ, 鮨} を追加（同一 position）。\n",
    "#      - これにより、インデックス側（通常 \"jpn-index\" で正規化済み）に格納されたどの表記ともマッチしやすくなる。\n",
    "#   3) スコアリングは BM25 が既定:\n",
    "#      - 展開後のトークンが文書側の語彙と一致するほどスコアが上がる。\n",
    "#      - 語順一致が重要なら match_phrase、複数フィールドをまたぐなら multi_match を検討。\n",
    "#\n",
    "# 実務上の注意:\n",
    "#   ・直前に index した文書がヒットしない場合は refresh 未反映の可能性があるため、\n",
    "#     インデクシング側で refresh=\"wait_for\" を使うか、ここで es.indices.refresh(index=jp_index) を行う。\n",
    "#   ・返却件数は既定 size=10、厳密件数が必要な場合は track_total_hits=True を付ける（8.x では query=... 形式推奨）。\n",
    "#   ・観測性のため、_analyze を併用してクエリ側トークンを確認すると原因追跡が速い（下に参考コード）。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 検索条件の設定\n",
    "# - クエリ語「寿司」は jpn-search の synonyms_filter により {寿司, すし, スシ, 鮨} へ等価展開される。\n",
    "query = {\"query\": {\"match\": {\"content\": \"寿司\"}}}\n",
    "\n",
    "# （参考・代替: 8.x 推奨の引数スタイル）\n",
    "# res = es.search(index=jp_index, query={\"match\": {\"content\": \"寿司\"}}, size=20, track_total_hits=True)\n",
    "\n",
    "# 検索実行\n",
    "# - body=... は後方互換のため残している。8.x では query=... を推奨。\n",
    "res = es.search(index=jp_index, body=query)\n",
    "\n",
    "# 結果表示\n",
    "# - ensure_ascii=False: 日本語を \\u エスケープせず可読出力。\n",
    "# - 実務では hits.hits から _id/_score/_source を抜粋してログ整形するのが見やすい。\n",
    "import json\n",
    "\n",
    "print(json.dumps(res, indent=2, ensure_ascii=False))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# （デバッグ支援：必要なときだけ使う参考コード。実行時はコメントを外す）\n",
    "# 1) クエリ側の実トークンを確認（同義語展開・原形化・stop 適用の結果を見る）\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\": \"jpn-search\", \"text\": \"寿司\"})[\"tokens\"]\n",
    "# for t in toks:\n",
    "#     print(t[\"token\"], t.get(\"position\"), t.get(\"start_offset\"), t.get(\"end_offset\"))\n",
    "#\n",
    "# 2) 直前インデクシングの可視化（refresh）\n",
    "# es.indices.refresh(index=jp_index)\n",
    "#\n",
    "# 3) 返戻の要約表示（閲覧性向上）\n",
    "# hits = [{\"_id\": h[\"_id\"], \"_score\": h[\"_score\"], \"_source\": h[\"_source\"]} for h in res[\"hits\"][\"hits\"]]\n",
    "# print(json.dumps({\"total\": res[\"hits\"][\"total\"], \"hits\": hits}, indent=2, ensure_ascii=False))\n",
    "# -----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f9f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.18 「新幹線はやぶさ」と「はやぶさ」の分析結果（詳細コメント付き）\n",
    "# ----------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・検索用アナライザ \"jpn-search\" を通したときのトークン化結果を比較し、\n",
    "#     固有表現（はやぶさ）の扱いと、複合語/連接の分割挙動を確認する。\n",
    "#\n",
    "# 理論メモ:\n",
    "#   ・処理順は char_filter（icu_normalizer, iteration_mark）→ tokenizer（kuromoji+userdict）\n",
    "#       → filter（synonyms → baseform → POS/ja_stop → number → stemmer）。\n",
    "#   ・本例の語は記号・長音・反復記号を含まないため、主に **tokenizer（kuromoji）** と\n",
    "#     **ユーザー辞書（my_jisho.dic）** の有無が結果を左右する。\n",
    "#     - 「新幹線はやぶさ」: 通常は「新幹線」「はやぶさ」の2トークンに分割されやすい。\n",
    "#       （ユーザー辞書で「新幹線はやぶさ」を一語登録していれば、1トークン化も可能）\n",
    "#     - 「はやぶさ」: 固有名詞（列車名/探査機名/一般名詞「隼」）として1トークン化される想定。\n",
    "#   ・同義語や原形化は本例にほぼ影響しない（名詞で活用無し、同義語も未定義なら no-op）。\n",
    "#   ・固有表現の検索精度を上げたい場合:\n",
    "#       1) ユーザー辞書で連接（「新幹線はやぶさ」）を1語として登録 → 索引側の分割安定化\n",
    "#       2) フィールドを multi-fields 化（stemmer/stop 無しの厳密フィールドも併設）\n",
    "#       3) クエリ側で match_phrase を併用（語順と近接を重視）\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "# 比較1: 連接形（通常は「新幹線」「はやぶさ」の2トークン想定）\n",
    "print(analyse_jp_text(\"新幹線はやぶさ\"))\n",
    "\n",
    "# 比較2: 単独形（通常は1トークン「はやぶさ」想定）\n",
    "print(analyse_jp_text(\"はやぶさ\"))\n",
    "\n",
    "# （デバッグ補助：必要に応じてコメント解除）\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-search\",\"text\":\"新幹線はやぶさ\"})[\"tokens\"]\n",
    "# for t in toks:\n",
    "#     print(t[\"token\"], t.get(\"position\"), t.get(\"start_offset\"), t.get(\"end_offset\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77b71d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リスト 3.3.19 キーワード「はやぶさ」で検索（理論コメント付き）\n",
    "# -----------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・全文検索（match）で \"content\" フィールドから「はやぶさ」を検索する。\n",
    "#   ・検索側には search_analyzer=\"jpn-search\" が適用される前提（同義語・原形化・品詞/stop 除去・\n",
    "#     数値/長音正規化等がクエリ文字列に対して実行される）。\n",
    "#\n",
    "# 理論メモ（何が起こるか）:\n",
    "#   ・match は **token-level** の全文検索。クエリ文字列は jpn-search の処理\n",
    "#     [char_filter → kuromoji → filter] を通ってトークン化され、インデックス側（通常は jpn-index）\n",
    "#     で格納済みの語彙と BM25（TF・IDF・文書長補正）でスコアリングされる。\n",
    "#   ・「はやぶさ」は一般名詞/固有名詞（列車名・探査機名等）として 1 トークン化されることが多いが、\n",
    "#     kuromoji の辞書/ユーザー辞書に依存する。連接語（例: 「新幹線はやぶさ」）を 1 語として扱いたい\n",
    "#     場合はユーザー辞書に登録するか、クエリ側で match_phrase を用いて語順・近接を強調する。\n",
    "#   ・同義語は jpn-search に定義していればクエリ側で展開される（等価展開）。未定義なら no-op。\n",
    "#\n",
    "# 実務的な注意:\n",
    "#   ・直前に index した文書がヒットしない場合は refresh の問題。検証時は index 時に\n",
    "#     refresh=\"wait_for\" を付けるか、検索前に es.indices.refresh(index=jp_index) を実行。\n",
    "#   ・返戻件数は既定 size=10、厳密件数が必要なら track_total_hits=True を指定（下に代替例のコメント）。\n",
    "#   ・完全一致（非解析）を行いたい用途は keyword フィールド + term クエリを使用する。\n",
    "#   ・語順/フレーズ一致を重視する場合は match_phrase（必要に応じて slop を調整）。\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# 検索条件の設定\n",
    "query = {\n",
    "    \"query\": {\"match\": {\"content\": \"はやぶさ\"}}  # ← クエリ側で jpn-search が適用される\n",
    "}\n",
    "\n",
    "# 検索実行\n",
    "# ※ 8.x クライアントでは body=... より query=... の使用が推奨（後方互換で body も可）。\n",
    "#    厳密件数や返戻件数をコントロールしたい場合は下の代替例を参照。\n",
    "res = es.search(index=jp_index, body=query)\n",
    "\n",
    "# 結果表示\n",
    "# - ensure_ascii=False: 日本語を \\u エスケープせずに出力\n",
    "import json\n",
    "\n",
    "print(json.dumps(res, indent=2, ensure_ascii=False))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# （代替/拡張例: 必要に応じてコメントアウトを外して使用）\n",
    "# 1) 厳密件数と返戻サイズを明示（推奨）\n",
    "# res = es.search(\n",
    "#     index=jp_index,\n",
    "#     query={\"match\": {\"content\": \"はやぶさ\"}},\n",
    "#     size=20,\n",
    "#     track_total_hits=True,\n",
    "#     _source=[\"title\",\"name.last\",\"content\"]\n",
    "# )\n",
    "# hits = [\n",
    "#     {\"_id\": h[\"_id\"], \"_score\": h[\"_score\"], \"_source\": h[\"_source\"]}\n",
    "#     for h in res[\"hits\"][\"hits\"]\n",
    "# ]\n",
    "# print(json.dumps({\"total\": res[\"hits\"][\"total\"], \"hits\": hits}, indent=2, ensure_ascii=False))\n",
    "#\n",
    "# 2) フレーズ一致（語順・近接を重視）\n",
    "# res = es.search(\n",
    "#     index=jp_index,\n",
    "#     query={\"match_phrase\": {\"content\": \"新幹線 はやぶさ\"}},\n",
    "#     size=20,\n",
    "#     track_total_hits=True\n",
    "# )\n",
    "#\n",
    "# 3) クエリ側トークンの可視化（_analyze）\n",
    "# toks = es.indices.analyze(index=jp_index, body={\"analyzer\":\"jpn-search\",\"text\":\"はやぶさ\"})[\"tokens\"]\n",
    "# for t in toks:\n",
    "#     print(t[\"token\"], t.get(\"position\"), t.get(\"start_offset\"), t.get(\"end_offset\"))\n",
    "#\n",
    "# 4) 直前投入の可視化\n",
    "# es.indices.refresh(index=jp_index)\n",
    "# -----------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
