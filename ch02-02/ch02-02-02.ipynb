{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb6f270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Tokenizer.__tokenize_stream at 0x110ea2dc0>\n",
      "\n",
      "これ\t名詞,代名詞,一般,*,*,*,これ,コレ,コレ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "日本語\t名詞,一般,*,*,*,*,日本語,ニホンゴ,ニホンゴ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "文章\t名詞,一般,*,*,*,*,文章,ブンショウ,ブンショー\n",
      "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n"
     ]
    }
   ],
   "source": [
    "# リスト 2.2.2 Janome サンプルコード\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "# 目的:\n",
    "#   ・純Python形態素解析器「Janome」を用いて、（1）分かち書き（wakati）と\n",
    "#     （2）形態素トークン列（品詞等の素性を含む）の 2 パターンを確認する。\n",
    "#\n",
    "# 理論メモ（実装は変えず、背景のみ説明）:\n",
    "#   ・Janome は MeCab と異なり C バインディング不要の純Python実装 → 導入容易だが速度は相対的に遅い。\n",
    "#   ・辞書は IPADIC 互換フォーマット相当（Janome同梱）。そのため feature 列（品詞体系や列順）は\n",
    "#     IPADIC の慣習（品詞, 品詞細分類1-3, 活用型, 活用形, 原形, 読み, 発音）に準じる。\n",
    "#   ・Tokenizer(wakati=True) は「分かち書きモード」。`tokenize(text)` は表層形（str）の列を返す。\n",
    "#     可視化としては文字列の join で `\" \".join(...)` とするのが一般だが、ここでは元コードどおり list をそのまま出力。\n",
    "#   ・Tokenizer()（通常モード）の `tokenize(text)` は Token オブジェクト列を返す。\n",
    "#     Token の `__str__` は「表層形[TAB]feature列」を返すので、print(token) で人間可読な1行出力になる。\n",
    "#   ・句読点「。」などの記号も1トークンとして出現（例：`記号,句点,*,*,*,*,*,*,*`）。\n",
    "#     記号を残す/除くは下流処理（学習・検索・要約）に影響するため、仕様として方針を固定するのが望ましい。\n",
    "#   ・Unicode 正規化（NFC/NFKC）は分割境界に影響しうる。再現性を重視するなら入力正規化の有無を運用規約に明記。\n",
    "#   ・ユーザー辞書（janome.dic.userdic.UserDictionary）を使えば固有名詞等の OOV を低減できる。\n",
    "#   ・スレッド/並列実行では、各スレッドごとに Tokenizer インスタンスを持つと安全（共有よりも衝突リスクが低い）。\n",
    "#   ・速度最適化が必要なら、対象文を事前に文単位へ分割（バッチ化）し、イテレータで処理するなどでオーバーヘッドを抑える。\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 解析対象文\n",
    "text = \"これは日本語の文章です。\"  # 短い日本語文。末尾の句点「。」も1トークンとして扱われる\n",
    "\n",
    "# 利用パターン1 対象文書を分かち書きにする\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "t1 = Tokenizer(\n",
    "    wakati=True\n",
    ")  # 分かち書きモード。表層形のみのトークン列（list[str]）を返す\n",
    "\n",
    "print(\n",
    "    t1.tokenize(text)\n",
    ")  # [ 'これ', 'は', '日本', '語', 'の', '文章', 'です', '。' ] のような配列（辞書/文により変動）\n",
    "print()\n",
    "\n",
    "# 利用パターン2 単語毎に分析結果を全部表示する\n",
    "t2 = Tokenizer()  # 通常モード。Token オブジェクト列を返す\n",
    "\n",
    "for token in t2.tokenize(text):\n",
    "    # Token.__str__() は「表層形 + タブ + feature（カンマ区切り）」を返す。\n",
    "    # 例: これ  代名詞,*,*,*,*,*,これ,コレ,コレ\n",
    "    #     は    助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
    "    #     日本  名詞,固有名詞,地域,国,*,*,日本,ニホン,ニッポン\n",
    "    #     語    名詞,一般,*,*,*,*,語,ゴ,ゴ\n",
    "    #     の    助詞,連体化,*,*,*,*,の,ノ,ノ\n",
    "    #     文章  名詞,一般,*,*,*,*,文章,ブンショウ,ブンショー\n",
    "    #     です  助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
    "    #     。    記号,句点,*,*,*,*,*,*,*\n",
    "    print(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
